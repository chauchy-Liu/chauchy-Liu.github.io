{"meta":{"version":1,"warehouse":"4.0.2"},"models":{"Asset":[{"_id":"themes/butterfly/source/img/404.jpg","path":"img/404.jpg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/favicon.png","path":"img/favicon.png","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/friend_404.gif","path":"img/friend_404.gif","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/haizei.jpg","path":"img/haizei.jpg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/haizei2.jpg","path":"img/haizei2.jpg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/kenan.jpg","path":"img/kenan.jpg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/lanqiu.jpg","path":"img/lanqiu.jpg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/touxiang.jpg","path":"img/touxiang.jpg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/img/yang.jpg","path":"img/yang.jpg","modified":0,"renderable":1},{"_id":"themes/butterfly/source/js/main.js","path":"js/main.js","modified":0,"renderable":1},{"_id":"themes/butterfly/source/js/tw_cn.js","path":"js/tw_cn.js","modified":0,"renderable":1},{"_id":"themes/butterfly/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"themes/butterfly/source/css/index.styl","path":"css/index.styl","modified":0,"renderable":1},{"_id":"themes/butterfly/source/css/var.styl","path":"css/var.styl","modified":0,"renderable":1},{"_id":"themes/butterfly/source/js/search/algolia.js","path":"js/search/algolia.js","modified":0,"renderable":1},{"_id":"themes/butterfly/source/js/search/local-search.js","path":"js/search/local-search.js","modified":0,"renderable":1},{"_id":"themes/butterfly/source/js/code-unfold.js","path":"js/code-unfold.js","modified":0,"renderable":1},{"_id":"themes/butterfly/source/js/search.js","path":"js/search.js","modified":0,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"d0f48ad27328df10fab4be1d80c87b9a6bce2a52","modified":1669086230399},{"_id":"source/manifest.json","hash":"1b8385031172b68bf23d0ef358625e4867e2058c","modified":1666331931011},{"_id":"source/categories/index.md","hash":"c16b99fc49a3099c32bf7adb02c81b163e0febf4","modified":1666336558510},{"_id":"source/_posts/.DS_Store","hash":"028e6eba7cad67bf3744e6d22816be86e7031cf2","modified":1669255427114},{"_id":"source/tags/index.md","hash":"10c7de8aa5e060d77823f3fbd1ea551adad9d4d5","modified":1666336682201},{"_id":"source/_posts/c-知识汇总.md","hash":"6547c29090dd1731dc5134fd10e8add1f75c4859","modified":1668821789207},{"_id":"source/_posts/hello-world.md","hash":"7d98d6592de80fdcd2949bd7401cec12afd98cdf","modified":1660901647987},{"_id":"source/_posts/myFirstBlog.md","hash":"c584d935aa9fc0aa1f22e36e3cee404b41e4d94b","modified":1660902460633},{"_id":"source/_posts/robotic-operating-system.md","hash":"8d12c38e6ff0641526e7e142e4e7b309c1bbac9b","modified":1666336915073},{"_id":"source/_posts/test.md","hash":"6edd1c8ca11da3c665bf4cbe967cfff8d95027e9","modified":1666852052673},{"_id":"source/_posts/Xnect/.DS_Store","hash":"89e3c29d115506b024ea0962608b7e62f106a04e","modified":1666403435399},{"_id":"source/_posts/Xnect/Untitled 1.png","hash":"a63c475e4eb97ea259326542f0050c058a1033e7","modified":1660873356000},{"_id":"source/_posts/Xnect/Untitled 4.png","hash":"924700c262ee6cd01c3bb2aa8d18b1b936b5d6a7","modified":1660873356000},{"_id":"source/_posts/Xnect/姿态检测公式.md","hash":"4907e4f1240f95f0c64e7c933bb9bba590bed430","modified":1666365386339},{"_id":"source/_posts/c-知识汇总/内存副本.jpg","hash":"76f470b7eb07e0e3186cfbf363bb698ad78d09a9","modified":1666840636258},{"_id":"source/_posts/c-知识汇总/内存.jpg","hash":"9ebd28040681223f38ad60e79ec8aa155d165330","modified":1666836352201},{"_id":"source/_posts/robotic-operating-system/image-20221020164052499.png","hash":"75031cad7e1671f5a7baeba44bb18b9a70657fea","modified":1666255252500},{"_id":"source/_posts/c-知识汇总/内存.psd","hash":"2eb0381e76570ab3971f855b63486ca7dfec367d","modified":1666840613415},{"_id":"themes/butterfly/.DS_Store","hash":"6fa4ac95d2cc1035c19f5e7348f09beef4722def","modified":1667547296722},{"_id":"themes/butterfly/README.md","hash":"249abda2d59e738d8e5416c9b0e2727fc73493dd","modified":1666318501187},{"_id":"themes/butterfly/LICENSE","hash":"1128f8f91104ba9ef98d37eea6523a888dcfa5de","modified":1666318501187},{"_id":"themes/butterfly/_config.yml","hash":"29dfd3f5d06e18afd9c2f994ebf31507feff3860","modified":1667547921438},{"_id":"themes/butterfly/README_CN.md","hash":"ec3df032fa5e197eef3822c1af517e29a91da2df","modified":1666318501187},{"_id":"themes/butterfly/_config.yml.bak","hash":"76f9883745d6d28f751f76328aa04b20865d09fb","modified":1666320255388},{"_id":"themes/butterfly/package.json","hash":"a937d4aa286401430c672bce28d93fe417d87a1e","modified":1666318501215},{"_id":"themes/butterfly/layout/.DS_Store","hash":"674109fd272908486f9dce4c51852e1e64a5d1df","modified":1666945105411},{"_id":"themes/butterfly/layout/category.pug","hash":"710708cfdb436bc875602abf096c919ccdf544db","modified":1666318501190},{"_id":"themes/butterfly/plugins.yml","hash":"b96cbef0a8af718b1b7bbe3317bb7a15484e7985","modified":1666318501215},{"_id":"themes/butterfly/layout/archive.pug","hash":"a0c034c2d319320a54046805e80b58dc48b7e233","modified":1666318501190},{"_id":"themes/butterfly/layout/page.pug","hash":"baf469784aef227e4cc840550888554588e87a13","modified":1666318501215},{"_id":"themes/butterfly/layout/post.pug","hash":"fc9f45252d78fcd15e4a82bfd144401cba5b169a","modified":1666318501215},{"_id":"themes/butterfly/layout/tag.pug","hash":"0440f42569df2676273c026a92384fa7729bc4e9","modified":1666318501215},{"_id":"themes/butterfly/layout/index.pug","hash":"e1c3146834c16e6077406180858add0a8183875a","modified":1666318501215},{"_id":"themes/butterfly/languages/default.yml","hash":"1e37a3695d50e3e61d7c36e58a6dac872a4a56cd","modified":1666318501188},{"_id":"themes/butterfly/languages/en.yml","hash":"d1bb560698eb8b0079495b7b18b44facb610f9fd","modified":1666318501189},{"_id":"themes/butterfly/languages/zh-TW.yml","hash":"947f794e862bb2813e36887f777bdb760f70a322","modified":1666318501190},{"_id":"themes/butterfly/languages/zh-CN.yml","hash":"28b6f0c39155651d747eb595e0a283bc97be2e09","modified":1666318501189},{"_id":"themes/butterfly/source/.DS_Store","hash":"e8b89c3ac3e0ffa62a2ca3b9c942c1aa1e7999d4","modified":1666945141129},{"_id":"themes/butterfly/.github/ISSUE_TEMPLATE/bug_report.yml","hash":"67e4f5a66d4b8cabadbaad0410628364ee75e0ae","modified":1666318501174},{"_id":"themes/butterfly/.github/ISSUE_TEMPLATE/feature_request.yml","hash":"996640605ed1e8e35182f0fd9a60a88783b24b03","modified":1666318501175},{"_id":"themes/butterfly/.github/ISSUE_TEMPLATE/config.yml","hash":"7dfe7189ffeaebb6db13842237f8e124649bea3d","modified":1666318501174},{"_id":"themes/butterfly/.github/workflows/publish.yml","hash":"05857c2f265246d8de00e31037f2720709540c09","modified":1666318501175},{"_id":"themes/butterfly/.github/workflows/stale.yml","hash":"ac62b989b5550c756e1986fcc68f243170705383","modified":1666318501186},{"_id":"themes/butterfly/layout/includes/404.pug","hash":"cb49f737aca272ccfeb62880bd651eccee72a129","modified":1666318501191},{"_id":"themes/butterfly/layout/includes/additional-js.pug","hash":"594a977ebe8d97e60fa3d7cb40fc260ded4d8a58","modified":1666318501191},{"_id":"themes/butterfly/layout/includes/head.pug","hash":"de8e618ec03295561c667a49e1c383213b856f6f","modified":1666318501192},{"_id":"themes/butterfly/layout/includes/footer.pug","hash":"02390a5b6ae1f57497b22ba2e6be9f13cfb7acac","modified":1666318501191},{"_id":"themes/butterfly/layout/includes/layout.pug","hash":"a557280a25f63f4312afad63fc3303ec74165557","modified":1666318501197},{"_id":"themes/butterfly/layout/includes/rightside.pug","hash":"699d0d2cff233628752956c4434125c8203f7d63","modified":1666318501201},{"_id":"themes/butterfly/layout/includes/pagination.pug","hash":"0b80f04950bd0fe5e6c4e7b7559adf4d0ce28436","modified":1666318501200},{"_id":"themes/butterfly/layout/includes/sidebar.pug","hash":"8d39473ed112d113674a0f689f63fae06c72abd2","modified":1666318501201},{"_id":"themes/butterfly/scripts/filters/post_lazyload.js","hash":"932df912976261929f809b7dbd4eb473e7787345","modified":1666318501216},{"_id":"themes/butterfly/scripts/filters/random_cover.js","hash":"21379ed2dccb69c43b893895c9d56238c11e5f43","modified":1666318501217},{"_id":"themes/butterfly/scripts/events/404.js","hash":"83cd7f73225ccad123afbd526ce1834eb1eb6a6d","modified":1666318501216},{"_id":"themes/butterfly/scripts/events/cdn.js","hash":"acfe765fb2c607bff7198630dbfa53a888c36357","modified":1666318501216},{"_id":"themes/butterfly/scripts/events/comment.js","hash":"a3d1f417965ca20253c36f9e93429f3df6268856","modified":1666318501216},{"_id":"themes/butterfly/scripts/events/init.js","hash":"b4940a5c73d3a5cd8bb5883e3041ecdd905a74e0","modified":1666318501216},{"_id":"themes/butterfly/scripts/events/stylus.js","hash":"9819f0996234fbd80d6c50a9e526c56ebf22588d","modified":1666318501216},{"_id":"themes/butterfly/scripts/events/welcome.js","hash":"3cfc46c749e2fd7ae9c2a17206238ed0e0e17e7d","modified":1666318501216},{"_id":"themes/butterfly/scripts/tag/button.js","hash":"91d954f6e9fe6e571eb8ec9f8996294b2dc3688e","modified":1666318501218},{"_id":"themes/butterfly/scripts/tag/flink.js","hash":"ab62919fa567b95fbe14889517abda649991b1ee","modified":1666318501218},{"_id":"themes/butterfly/scripts/tag/inlineImg.js","hash":"a43ee2c7871bdd93cb6beb804429e404570f7929","modified":1666318501218},{"_id":"themes/butterfly/scripts/tag/label.js","hash":"03b2afef41d02bd1045c89578a02402c28356006","modified":1666318501219},{"_id":"themes/butterfly/scripts/tag/hide.js","hash":"396c3ab1bcf1c7693ad7e506eadd13016c6769b6","modified":1666318501218},{"_id":"themes/butterfly/scripts/tag/gallery.js","hash":"f79c99f6c5b626c272dc2bed2b0250d6b91bb28a","modified":1666318501218},{"_id":"themes/butterfly/scripts/tag/mermaid.js","hash":"531808a290b8bdd66bac2faab211ada8e9646a37","modified":1666318501219},{"_id":"themes/butterfly/scripts/tag/note.js","hash":"d51812b43924f1bbf413c67499510dd125022005","modified":1666318501219},{"_id":"themes/butterfly/scripts/tag/tabs.js","hash":"6c6e415623d0fd39da016d9e353bb4f5cca444f5","modified":1666318501219},{"_id":"themes/butterfly/scripts/tag/timeline.js","hash":"300eb779588bf35a1b687d9f829d866074b707e3","modified":1666318501219},{"_id":"themes/butterfly/scripts/helpers/aside_archives.js","hash":"2ec66513d5322f185d2071acc052978ba9415a8e","modified":1666318501217},{"_id":"themes/butterfly/scripts/helpers/aside_categories.js","hash":"e00efdb5d02bc5c6eb4159e498af69fa61a7dbb9","modified":1666318501217},{"_id":"themes/butterfly/scripts/helpers/inject_head_js.js","hash":"b4cd617c619d1a0df93603721a6fa1317526174b","modified":1666318501217},{"_id":"themes/butterfly/scripts/helpers/page.js","hash":"763dab5c83f50c1c62fffc9a9dfedea29bb4e629","modified":1666318501217},{"_id":"themes/butterfly/scripts/helpers/related_post.js","hash":"d368a8830e506c8b5eb6512b709ec8db354d5ea1","modified":1666318501218},{"_id":"themes/butterfly/scripts/helpers/findArchiveLength.js","hash":"ee3e70098eb0849497d50b75e18cf4a27c397d52","modified":1666318501217},{"_id":"themes/butterfly/source/img/.DS_Store","hash":"af053f9f5bfb87806cdb996eddb8364d7613a75d","modified":1666360986373},{"_id":"themes/butterfly/source/img/404.jpg","hash":"fb4489bc1d30c93d28f7332158c1c6c1416148de","modified":1666318501229},{"_id":"themes/butterfly/source/img/favicon.png","hash":"3cf89864b4f6c9b532522a4d260a2e887971c92d","modified":1666318501230},{"_id":"themes/butterfly/source/img/friend_404.gif","hash":"8d2d0ebef70a8eb07329f57e645889b0e420fa48","modified":1666318501230},{"_id":"themes/butterfly/source/img/touxiang.jpg","hash":"b4343fb441da58bd2a56a81e17d9e43763df30c7","modified":1656577277452},{"_id":"themes/butterfly/source/js/main.js","hash":"73d2624ed465e4cfb1ebb00b2c8a24f5fc29bb21","modified":1666318501231},{"_id":"themes/butterfly/source/js/tw_cn.js","hash":"00053ce73210274b3679f42607edef1206eebc68","modified":1666318501231},{"_id":"themes/butterfly/source/js/utils.js","hash":"0b95daada72abb5d64a1e3236049a60120e47cca","modified":1666318501232},{"_id":"themes/butterfly/source/css/index.styl","hash":"861998e4ac67a59529a8245a9130d68f826c9c12","modified":1666318501229},{"_id":"themes/butterfly/source/css/var.styl","hash":"4890a40366d6443f8b8942a4e9a6dce9fe3494f5","modified":1666318501229},{"_id":"themes/butterfly/layout/includes/head/analytics.pug","hash":"15530d9ac59c576d79af75dd687efe71e8d261b0","modified":1666318501193},{"_id":"themes/butterfly/layout/includes/head/Open_Graph.pug","hash":"6c41f49a3e682067533dd9384e6e4511fc3a1349","modified":1666318501192},{"_id":"themes/butterfly/layout/includes/head/config.pug","hash":"8f41fa9732ea654a10f6e666d9c782c7e27e5ea6","modified":1666318501193},{"_id":"themes/butterfly/layout/includes/head/config_site.pug","hash":"7df90c8e432e33716517ab918b0a125bc284041b","modified":1666318501193},{"_id":"themes/butterfly/layout/includes/head/google_adsense.pug","hash":"95a37e92b39c44bcbea4be7e29ddb3921c5b8220","modified":1666318501194},{"_id":"themes/butterfly/layout/includes/head/noscript.pug","hash":"d16ad2ee0ff5751fd7f8a5ce1b83935518674977","modified":1666318501194},{"_id":"themes/butterfly/layout/includes/head/pwa.pug","hash":"3d492cfe645d37c94d30512e0b230b0a09913148","modified":1666318501195},{"_id":"themes/butterfly/layout/includes/head/preconnect.pug","hash":"65a23b5170204e55b813ce13a79d799b66b7382c","modified":1666318501194},{"_id":"themes/butterfly/layout/includes/head/site_verification.pug","hash":"e2e8d681f183f00ce5ee239c42d2e36b3744daad","modified":1666318501195},{"_id":"themes/butterfly/layout/includes/header/index.pug","hash":"aa175e2254704335f4da09175e59ef2375ca7d03","modified":1666318501196},{"_id":"themes/butterfly/layout/includes/header/nav.pug","hash":"78a3abd90bb3c18cd773d3d5abac3541e7f415e5","modified":1666318501196},{"_id":"themes/butterfly/layout/includes/header/menu_item.pug","hash":"31346a210f4f9912c5b29f51d8f659913492f388","modified":1666318501196},{"_id":"themes/butterfly/layout/includes/header/post-info.pug","hash":"9698f22751778dde063cbfbd01c59ca4462ccd85","modified":1666318501197},{"_id":"themes/butterfly/layout/includes/header/social.pug","hash":"0d953e51d04a9294a64153c89c20f491a9ec42d4","modified":1666318501197},{"_id":"themes/butterfly/layout/includes/loading/loading-js.pug","hash":"4cfcf0100e37ce91864703cd44f1cb99cb5493ea","modified":1666318501198},{"_id":"themes/butterfly/layout/includes/loading/loading.pug","hash":"5276937fbcceb9d62879dc47be880cd469a27349","modified":1666318501198},{"_id":"themes/butterfly/layout/includes/mixins/article-sort.pug","hash":"2fb74d0b0e4b98749427c5a1a1b0acb6c85fadc4","modified":1666318501198},{"_id":"themes/butterfly/layout/includes/mixins/post-ui.pug","hash":"6ae047d566289294cc2f6192cacbff297cec9f1a","modified":1666318501199},{"_id":"themes/butterfly/layout/includes/page/categories.pug","hash":"5276a8d2835e05bd535fedc9f593a0ce8c3e8437","modified":1666318501199},{"_id":"themes/butterfly/layout/includes/page/default-page.pug","hash":"12c65c174d26a41821df9bad26cdf1087ec5b0ca","modified":1666318501199},{"_id":"themes/butterfly/layout/includes/page/flink.pug","hash":"fed069baa9b383f57db32bb631115071d29bdc60","modified":1666318501199},{"_id":"themes/butterfly/layout/includes/page/tags.pug","hash":"6311eda08e4515281c51bd49f43902a51832383c","modified":1666318501200},{"_id":"themes/butterfly/layout/includes/post/reward.pug","hash":"864869c43fe5b5bb6f4ac6b13dd4bfb16ea47550","modified":1666318501200},{"_id":"themes/butterfly/layout/includes/third-party/aplayer.pug","hash":"c7cfade2b160380432c47eef4cd62273b6508c58","modified":1666318501201},{"_id":"themes/butterfly/layout/includes/post/post-copyright.pug","hash":"ebecba46a5f4efe1c98a386df06c56e26fbd07b9","modified":1666318501200},{"_id":"themes/butterfly/layout/includes/third-party/effect.pug","hash":"6528e86656906117a1af6b90e0349c2c4651d5e1","modified":1666318501207},{"_id":"themes/butterfly/layout/includes/third-party/pangu.pug","hash":"0f024e36b8116118233e10118714bde304e01e12","modified":1666318501210},{"_id":"themes/butterfly/layout/includes/third-party/pjax.pug","hash":"6d6474ef186c18d9b4f334e1f735eadd6699effa","modified":1666318501210},{"_id":"themes/butterfly/layout/includes/third-party/prismjs.pug","hash":"ffb9ea15a2b54423cd4cd441e2d061b8233e9b58","modified":1666318501210},{"_id":"themes/butterfly/layout/includes/widget/card_ad.pug","hash":"60dc48a7b5d89c2a49123c3fc5893ab9c57dd225","modified":1666318501212},{"_id":"themes/butterfly/layout/includes/third-party/subtitle.pug","hash":"142621f70bedcb5033ee99a988f7bb6c5eea3493","modified":1666318501212},{"_id":"themes/butterfly/layout/includes/widget/card_archives.pug","hash":"86897010fe71503e239887fd8f6a4f5851737be9","modified":1666318501213},{"_id":"themes/butterfly/layout/includes/widget/card_announcement.pug","hash":"ae392459ad401a083ca51ee0b27526b3c1e1faed","modified":1666318501213},{"_id":"themes/butterfly/layout/includes/widget/card_author.pug","hash":"e37468e63db2a0ac09b65d21b7de3e62425bb455","modified":1666318501213},{"_id":"themes/butterfly/layout/includes/widget/card_bottom_self.pug","hash":"13dc8ce922e2e2332fe6ad5856ebb5dbf9ea4444","modified":1666318501213},{"_id":"themes/butterfly/layout/includes/widget/card_categories.pug","hash":"d1a416d0a8a7916d0b1a41d73adc66f8c811e493","modified":1666318501213},{"_id":"themes/butterfly/layout/includes/widget/card_post_toc.pug","hash":"3057a2f6f051355e35d3b205121af8735100eacf","modified":1666318501214},{"_id":"themes/butterfly/layout/includes/widget/card_recent_post.pug","hash":"9c1229af6ab48961021886882c473514101fba21","modified":1666318501214},{"_id":"themes/butterfly/layout/includes/widget/card_newest_comment.pug","hash":"6d93564a8bd13cb9b52ee5e178db3bcbf18b1bc6","modified":1666318501213},{"_id":"themes/butterfly/layout/includes/widget/card_tags.pug","hash":"438aea3e713ed16b7559b9a80a9c5ec0221263df","modified":1666318501214},{"_id":"themes/butterfly/layout/includes/widget/card_webinfo.pug","hash":"35ce167c5a275211bfc1fa3d49adfde5b404d98f","modified":1666318501214},{"_id":"themes/butterfly/layout/includes/widget/card_top_self.pug","hash":"ae67c6d4130a6c075058a9c1faea1648bcc6f83e","modified":1666318501214},{"_id":"themes/butterfly/layout/includes/widget/index.pug","hash":"7fb096656c8a6c21a4b6a5100885b1081d6021ed","modified":1666318501214},{"_id":"themes/butterfly/source/js/search/algolia.js","hash":"9feb248552667c53ce1b19bc7a295215f8c77008","modified":1666318501231},{"_id":"themes/butterfly/source/js/search/local-search.js","hash":"3071a4208fdf89ad7e0031536dd6ffa7bc951e4d","modified":1666318501231},{"_id":"themes/butterfly/source/css/_global/function.styl","hash":"644d520fe80cc82058467708ab82ccad313b0c27","modified":1666318501219},{"_id":"themes/butterfly/source/css/_global/index.styl","hash":"714f19e7d66df84938bd1b82b33d5667abe1f147","modified":1666318501220},{"_id":"themes/butterfly/source/css/_highlight/highlight.styl","hash":"2f95e99b8351fbecd9037a1bbdc3fee9d6ea8a77","modified":1666318501220},{"_id":"themes/butterfly/source/css/_highlight/theme.styl","hash":"bcd384c8b2aa0390c9eb69ac1abbfd1240ce1da4","modified":1666318501221},{"_id":"themes/butterfly/source/css/_layout/aside.styl","hash":"a0010d833ed30211601c1e0bbbc68e85b77428c6","modified":1666318501221},{"_id":"themes/butterfly/source/css/_layout/chat.styl","hash":"f9a5d3f1fc5ed0ed2ee4c1eaa58ed650d11ddebd","modified":1666318501221},{"_id":"themes/butterfly/source/css/_layout/comments.styl","hash":"c61dccca690d486c3d9c29cf028d87b777385141","modified":1666318501222},{"_id":"themes/butterfly/source/css/_layout/head.styl","hash":"a457a0dbe23af21a7b6b8bfc5ee103c9fe92e89c","modified":1666318501222},{"_id":"themes/butterfly/source/css/_layout/footer.styl","hash":"26be2afa9d4e7016cf3c42a6cd166f01e8e4ad5c","modified":1666318501222},{"_id":"themes/butterfly/source/css/_layout/pagination.styl","hash":"fb9f78bfbb79579f1d752cb73fb6d25c8418e0fd","modified":1666318501222},{"_id":"themes/butterfly/source/css/_layout/loading.styl","hash":"ef21990de28bd75dcd0f88b8d616e1a7a137502f","modified":1666318501222},{"_id":"themes/butterfly/source/css/_layout/post.styl","hash":"15056fba0bd5a45ea8dc97eb557f6929ff16797a","modified":1666318501222},{"_id":"themes/butterfly/source/css/_layout/relatedposts.styl","hash":"d53de408cb27a2e704aba7f7402b7caebe0410d8","modified":1666318501223},{"_id":"themes/butterfly/source/css/_layout/rightside.styl","hash":"bd88ee30ebf8ca2e7b4d3a034c317fd61733921f","modified":1666318501223},{"_id":"themes/butterfly/source/css/_layout/reward.styl","hash":"c5cfed620708807a48076b5ee59b0ba84e29aa80","modified":1666318501223},{"_id":"themes/butterfly/source/css/_layout/sidebar.styl","hash":"631ca35a38bc4ac052e9caf47508ff1f99842fc7","modified":1666318501223},{"_id":"themes/butterfly/source/css/_mode/darkmode.styl","hash":"a92984f566c97bb4179f34be79240af1552c6f17","modified":1666318501224},{"_id":"themes/butterfly/source/css/_layout/third-party.styl","hash":"8314e9749eb1ae40c4bae9735b7a6638b2d6876a","modified":1666318501223},{"_id":"themes/butterfly/source/css/_mode/readmode.styl","hash":"69f8e9414526dfda3af9a71c8e528fdd0ecbbfe5","modified":1666318501224},{"_id":"themes/butterfly/source/css/_page/404.styl","hash":"50dbb9e6d98c71ffe16741b8c1b0c1b9771efd2b","modified":1666318501224},{"_id":"themes/butterfly/source/css/_page/archives.styl","hash":"6f4b4ede52305bce9b22c8c897dcbde8af6e2ce4","modified":1666318501224},{"_id":"themes/butterfly/source/css/_page/categories.styl","hash":"f01ee74948cedb44e53cd3bb1ef36b7d2778ede7","modified":1666318501225},{"_id":"themes/butterfly/source/css/_page/common.styl","hash":"a58d35d698885f1034dedbe99f7dbc1a801412c6","modified":1666318501225},{"_id":"themes/butterfly/source/css/_page/flink.styl","hash":"98d755b686ee833e9da10afaa40c4ec2bd66c19a","modified":1666318501225},{"_id":"themes/butterfly/source/css/_page/homepage.styl","hash":"8c90483d461e09cb06e91b16d8bb7b3205b0a40c","modified":1666318501225},{"_id":"themes/butterfly/source/css/_page/tags.styl","hash":"580feb7e8b0822a1be48ac380f8c5c53b1523321","modified":1666318501225},{"_id":"themes/butterfly/source/css/_search/algolia.styl","hash":"51e45625929d57c9df3ba9090af99b9b7bb9a15b","modified":1666318501225},{"_id":"themes/butterfly/source/css/_search/local-search.styl","hash":"25e58a7a8bda4b73d0a0e551643ca01b09ccd7e5","modified":1666318501226},{"_id":"themes/butterfly/source/css/_tags/button.styl","hash":"45f0c32bdea117540f6b14ebac6450d7142bd710","modified":1666318501226},{"_id":"themes/butterfly/source/css/_search/index.styl","hash":"39d61cbe0c1e937f83ba3b147afaa29b4de2f87d","modified":1666318501226},{"_id":"themes/butterfly/source/css/_tags/gallery.styl","hash":"a310e48f826a4cacc55d8e68f43806e5085554f6","modified":1666318501227},{"_id":"themes/butterfly/source/css/_tags/hexo.styl","hash":"d76c38adf1d9c1279ef4241835667789f5b736e0","modified":1666318501227},{"_id":"themes/butterfly/source/css/_tags/hide.styl","hash":"ce489ca2e249e2a3cf71584e20d84bdb022e3475","modified":1666318501227},{"_id":"themes/butterfly/source/css/_tags/inlineImg.styl","hash":"df9d405c33a9a68946b530410f64096bcb72560c","modified":1666318501227},{"_id":"themes/butterfly/source/css/_tags/note.styl","hash":"85ae91c83691ea4511f4277da1194a185251cc78","modified":1666318501228},{"_id":"themes/butterfly/source/css/_tags/tabs.styl","hash":"bf9568444dd54e39dc59b461323dcd38942f27d9","modified":1666318501228},{"_id":"themes/butterfly/source/css/_tags/label.styl","hash":"66c59e193d794cdb02cca7bd1dc4aea5a19d7e84","modified":1666318501228},{"_id":"themes/butterfly/source/css/_tags/timeline.styl","hash":"f071156d439556e7463ed4bc61ceee87170d5d08","modified":1666318501228},{"_id":"themes/butterfly/source/css/_third-party/normalize.min.css","hash":"2c18a1c9604af475b4749def8f1959df88d8b276","modified":1666318501229},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/artalk.pug","hash":"99d9b17668260b242749c16851d9ec1024d31899","modified":1666318501202},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/index.pug","hash":"b2d274db84ef22fbd6d5ea8f4404821898934209","modified":1666318501202},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/disqus.pug","hash":"d85c3737b5c9548553a78b757a7698df126a52cf","modified":1666318501202},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/remark42.pug","hash":"001e8be47854b891efe04013c240c38fed4185eb","modified":1666318501202},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/fb.pug","hash":"f0ddc0f1ffee143d86c81bee1136ae6ee01e8ea5","modified":1666318501202},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/twikoo.pug","hash":"58406a7a3bf45815769f652bf3ef81e57dcd07eb","modified":1666318501203},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/valine.pug","hash":"39427e107230a10790972349c9dd4c4f31d55eb7","modified":1666318501203},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/waline.pug","hash":"5f648086a33a32d169a2f8d8c549c08aa02f67db","modified":1666318501203},{"_id":"themes/butterfly/layout/includes/third-party/chat/chatra.pug","hash":"481cd5053bafb1a19f623554a27d3aa077ea59c3","modified":1666318501203},{"_id":"themes/butterfly/layout/includes/third-party/chat/gitter.pug","hash":"d1d2474420bf4edc2e43ccdff6f92b8b082143df","modified":1666318501204},{"_id":"themes/butterfly/layout/includes/third-party/chat/crisp.pug","hash":"76634112c64023177260d1317ae39cef2a68e35f","modified":1666318501203},{"_id":"themes/butterfly/layout/includes/third-party/chat/daovoice.pug","hash":"cfe63e7d26a6665df6aa32ca90868ad48e05ec04","modified":1666318501204},{"_id":"themes/butterfly/layout/includes/third-party/chat/index.pug","hash":"0611d9216f4c128ebdea4cc21454c3dc4a6398d5","modified":1666318501204},{"_id":"themes/butterfly/layout/includes/third-party/chat/messenger.pug","hash":"bfe7b5a8fad041b63a10c6c6ed02e7c87afb94cd","modified":1666318501204},{"_id":"themes/butterfly/layout/includes/third-party/chat/tidio.pug","hash":"24a926756c2300b9c561aaab6bd3a71fdd16e16d","modified":1666318501204},{"_id":"themes/butterfly/layout/includes/third-party/comments/artalk.pug","hash":"49dcc23815e1d5264bb65c91e7dc67b1a723d1ea","modified":1666318501205},{"_id":"themes/butterfly/layout/includes/third-party/comments/disqus.pug","hash":"8ec24c1939895ac0db2b2e8700bc9307b4ceb53c","modified":1666318501205},{"_id":"themes/butterfly/layout/includes/third-party/comments/facebook_comments.pug","hash":"2c095186033b6b273634ce97a75873da1f3646db","modified":1666318501205},{"_id":"themes/butterfly/layout/includes/third-party/comments/disqusjs.pug","hash":"98ef20f8a3b10c1692f9b2b3c06033d2da8a8eae","modified":1666318501205},{"_id":"themes/butterfly/layout/includes/third-party/comments/giscus.pug","hash":"591ef23c583690bd74af0cafb09af64ba5bd8151","modified":1666318501205},{"_id":"themes/butterfly/layout/includes/third-party/comments/index.pug","hash":"351fe25fbf02635b1f9e86e5e244c7d61f69baa7","modified":1666318501206},{"_id":"themes/butterfly/layout/includes/third-party/comments/gitalk.pug","hash":"22e2ef30fe5eb1db7566e89943c74ece029b2a8e","modified":1666318501206},{"_id":"themes/butterfly/layout/includes/third-party/comments/js.pug","hash":"00ed91c52939b9675b316137f854d13684c895a6","modified":1666318501206},{"_id":"themes/butterfly/layout/includes/third-party/comments/livere.pug","hash":"52ea8aa26b84d3ad38ae28cdf0f163e9ca8dced7","modified":1666318501206},{"_id":"themes/butterfly/layout/includes/third-party/comments/remark42.pug","hash":"e9bdf80d6796afc04eb809dbbe780d97f22c7fcd","modified":1666318501206},{"_id":"themes/butterfly/layout/includes/third-party/comments/twikoo.pug","hash":"e18fbd88d8942e53e771f29b26209ab735c5c567","modified":1666318501206},{"_id":"themes/butterfly/layout/includes/third-party/comments/utterances.pug","hash":"a737046e730eb7264606ba0536218964044492f9","modified":1666318501207},{"_id":"themes/butterfly/layout/includes/third-party/comments/valine.pug","hash":"e55b9c0f8ced231f47eb88bd7f4ec99f29c5c29d","modified":1666318501207},{"_id":"themes/butterfly/layout/includes/third-party/comments/waline.pug","hash":"15462d1ed04651ad3b430c682842ac400f6f9b47","modified":1666318501207},{"_id":"themes/butterfly/layout/includes/third-party/math/index.pug","hash":"b8ae5fd7d74e1edcef21f5004fc96147e064d219","modified":1666318501207},{"_id":"themes/butterfly/layout/includes/third-party/math/katex.pug","hash":"dfcbd9881be569ea420eff1a6b00e4f4dbe2138e","modified":1666318501208},{"_id":"themes/butterfly/layout/includes/third-party/math/mathjax.pug","hash":"f4dc7d02c8192979404ae9e134c5048d3d0a76e2","modified":1666318501208},{"_id":"themes/butterfly/layout/includes/third-party/math/mermaid.pug","hash":"8e33aca36a4d3ae9e041ba05ced8eff56ae38f77","modified":1666318501208},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/artalk.pug","hash":"c12c898b35dd014780c1f3220ddbe12e7270983c","modified":1666318501208},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/disqus-comment.pug","hash":"04b2a5882e789a988e41d45abe606f0617b08e38","modified":1666318501209},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/index.pug","hash":"4ec0642f2d5444acfab570a6f8c7868e7ff43fde","modified":1666318501209},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/github-issues.pug","hash":"e846ddfe4a63b15d1416f6055f5756af5e3da7c6","modified":1666318501209},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/remark42.pug","hash":"ab167c00da4506f591b96f0591bf5bd214a26d4b","modified":1666318501209},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/twikoo-comment.pug","hash":"233907dd7f5b5f33412701d2ccffbc0bbae8707b","modified":1666318501209},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/valine.pug","hash":"d19e1c2c0a50f0e4547d71a17b9be88e8152f17c","modified":1666318501210},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/waline.pug","hash":"dd0bc119029b62dce5dc965d5de7377e438fa29a","modified":1666318501210},{"_id":"themes/butterfly/layout/includes/third-party/search/algolia.pug","hash":"e8245d0b4933129bb1c485d8de11a9e52e676348","modified":1666318501211},{"_id":"themes/butterfly/layout/includes/third-party/share/add-this.pug","hash":"2980f1889226ca981aa23b8eb1853fde26dcf89a","modified":1666318501211},{"_id":"themes/butterfly/layout/includes/third-party/search/index.pug","hash":"da3b9437d061ee68dbc383057db5c73034c49605","modified":1666318501211},{"_id":"themes/butterfly/layout/includes/third-party/search/local-search.pug","hash":"178c9cdcc4ce5a006885b24ce4a3d624e4734899","modified":1666318501211},{"_id":"themes/butterfly/layout/includes/third-party/share/addtoany.pug","hash":"85c92f8a7e44d7cd1c86f089a05be438535e5362","modified":1666318501212},{"_id":"themes/butterfly/layout/includes/third-party/share/index.pug","hash":"4c4a9c15215ae8ac5eadb0e086b278f76db9ee92","modified":1666318501212},{"_id":"themes/butterfly/layout/includes/third-party/share/share-js.pug","hash":"f61d63724ea5c5f352568b3a16bde023affefbe5","modified":1666318501212},{"_id":"themes/butterfly/source/css/_highlight/highlight/index.styl","hash":"18804c58239d95798fa86d0597f32d7f7dd30051","modified":1666318501220},{"_id":"themes/butterfly/source/css/_highlight/highlight/diff.styl","hash":"cf1fae641c927621a4df1be5ca4a853b9b526e23","modified":1666318501220},{"_id":"themes/butterfly/source/css/_highlight/prismjs/diff.styl","hash":"5972c61f5125068cbe0af279a0c93a54847fdc3b","modified":1666318501221},{"_id":"themes/butterfly/source/css/_highlight/prismjs/index.styl","hash":"5dc2e0bcae9a54bfb9bdcc82d02ae5a3cf1ca97d","modified":1666318501221},{"_id":"themes/butterfly/source/css/_highlight/prismjs/line-number.styl","hash":"8970cc1916c982b64a1478792b2822d1d31e276d","modified":1666318501221},{"_id":"source/_posts/Xnect/Untitled.png","hash":"e7dbf5b7372765d28762ee23e98ed5d3d68c8bd4","modified":1660873356000},{"_id":"themes/butterfly/source/img/lanqiu.jpg","hash":"c591a31cf35f20358f342b5293d9baa35a284383","modified":1549898414991},{"_id":"themes/butterfly/source/img/kenan.jpg","hash":"b4cedb8d6f1945d22435875c516b1cedb73ddb5e","modified":1549897959072},{"_id":"themes/butterfly/source/img/haizei2.jpg","hash":"fc148ad7897a630d2d91e17296eeed534e394968","modified":1449397390000},{"_id":"themes/butterfly/source/img/yang.jpg","hash":"05a1520d1bf5778a8379c9ff76ea9e11c0551736","modified":1464013380000},{"_id":"source/_posts/Xnect/Untitled 3.png","hash":"7815c21906e642e7fadf46d393257a3abc76e00d","modified":1660873356000},{"_id":"themes/butterfly/source/img/haizei.jpg","hash":"6a9d5c484d632b304eca2e40457f4db4b10741ad","modified":1449397267000},{"_id":"source/_posts/Xnect/Untitled 2.png","hash":"852e620f1b867f0a342c2cadc102936af6dd85d4","modified":1660873356000},{"_id":"source/_posts/Xnect.zip","hash":"8a51efc45a34e69afb660185f3b095732d7f7b51","modified":1660902163501},{"_id":"public/manifest.json","hash":"e5653018c633c03092c691caf2b17921c60fe633","modified":1666946474794},{"_id":"public/categories/index.html","hash":"431e18e9f1841ffe5761c7f71e1e34e96c3ff34d","modified":1670054140941},{"_id":"public/tags/index.html","hash":"c3d208cd83e026160cb041288b2cff4030395732","modified":1670054140941},{"_id":"public/2022/10/27/test/index.html","hash":"d1f71b151fbbc4e91aaaf6b935a5ee1fa73625d3","modified":1670054140941},{"_id":"public/2022/10/26/c-知识汇总/index.html","hash":"497c2fb6f967a8759f1f48d0593b5fae1589ef08","modified":1670054140941},{"_id":"public/2022/10/20/robotic-operating-system/index.html","hash":"9fc5f084b075726ea70497757356681f3668da65","modified":1670054140941},{"_id":"public/2022/08/19/myFirstBlog/index.html","hash":"7d7db48cd1df121d29d128d00ef5abb8104a72f1","modified":1670054140941},{"_id":"public/2022/08/19/hello-world/index.html","hash":"31d14e6e885c6e190f9b97468cf60721540fe0c8","modified":1670054140941},{"_id":"public/2022/08/19/Xnect/姿态检测公式/index.html","hash":"fff90728494fdedbf7b98b31c55948ce7e2abf7b","modified":1670054140941},{"_id":"public/archives/index.html","hash":"37cdcdb4590d1a9dc50934d4d1e6135a479aa4e0","modified":1670054140941},{"_id":"public/archives/2022/index.html","hash":"8ba7653842c0260bb3ce64838afba0d6fda50cfc","modified":1670054140941},{"_id":"public/archives/2022/08/index.html","hash":"f0a1cda1924f224aa752c729f6ebcc9afbbb72a5","modified":1670054140941},{"_id":"public/archives/2022/10/index.html","hash":"653933281c476e95d22287a6f6f7f450679dfb1c","modified":1670054140941},{"_id":"public/categories/编程语言/index.html","hash":"7470fddb640f82641c52ca647bae855c29f1dc92","modified":1670054140941},{"_id":"public/categories/书籍/index.html","hash":"24e6e629b6041de0998abbe0d729fcac1fa7565e","modified":1670054140941},{"_id":"public/categories/编程语言/c/index.html","hash":"d89597a9d7cc66cc577fb5290f4b38e0351525c8","modified":1670054140941},{"_id":"public/categories/书籍/机器人/index.html","hash":"c58fbf5745fc0cbe244a6c464a8f96f9db4d80c7","modified":1670054140941},{"_id":"public/categories/编程语言/c/碎片化知识补丁/index.html","hash":"4704824cb14f129fd18dbdbb72d27332414cafa0","modified":1670054140941},{"_id":"public/categories/文献/index.html","hash":"137dbd62cc27e4b748cc20a967a5331dc878c961","modified":1670054140941},{"_id":"public/categories/文献/人体姿态/index.html","hash":"66414dbab0e9dbeacda887d2fcd3f347c2b5322b","modified":1670054140941},{"_id":"public/index.html","hash":"0f70eb4b3bd5a79ec5b4dee065ff89d6fa764278","modified":1670054140941},{"_id":"public/tags/博客/index.html","hash":"7f43bed6ef6cf96ae672f3e893f2341e223c036d","modified":1670054140941},{"_id":"public/tags/c/index.html","hash":"793fa9eef2f3e688e5ecbfcaca2fd562d9dcb8c0","modified":1670054140941},{"_id":"public/tags/机器人/index.html","hash":"1cf435c9a141ad48b9f175b873a0b162f5d3d362","modified":1670054140941},{"_id":"public/tags/系统/index.html","hash":"a5747187dc353a34ebc650ea6ac849f1885c128e","modified":1670054140941},{"_id":"public/tags/神经网络/index.html","hash":"1100f3eb1ae1d0719f539ebb2e1f0776847a1b7f","modified":1670054140941},{"_id":"public/tags/机器学习/index.html","hash":"b6a3676bd2e9c125fdb9739950ac8f6eb6cc676e","modified":1670054140941},{"_id":"public/img/404.jpg","hash":"fb4489bc1d30c93d28f7332158c1c6c1416148de","modified":1666946474794},{"_id":"public/img/favicon.png","hash":"3cf89864b4f6c9b532522a4d260a2e887971c92d","modified":1666946474794},{"_id":"public/img/friend_404.gif","hash":"8d2d0ebef70a8eb07329f57e645889b0e420fa48","modified":1666946474794},{"_id":"public/img/touxiang.jpg","hash":"b4343fb441da58bd2a56a81e17d9e43763df30c7","modified":1666946474794},{"_id":"public/live2dw/assets/hijiki.model.json","hash":"feff43bf7498d213982c3736c2c029664e4bcbd2","modified":1666946474794},{"_id":"public/live2dw/assets/hijiki.pose.json","hash":"81438bf69b32c7c11e311b4fe043730cdc7b7ec2","modified":1666946474794},{"_id":"public/live2dw/assets/mtn/00_idle.mtn","hash":"b224c60e463b9f71ddbfc0c720e430496c175f4f","modified":1666946474794},{"_id":"public/live2dw/assets/mtn/01.mtn","hash":"fb550833ae22c9954c3e01df37ed29b2d61700f2","modified":1666946474794},{"_id":"public/live2dw/assets/mtn/04.mtn","hash":"c7a25d3c5d783639bae18db2f3cd284b819c3c85","modified":1666946474794},{"_id":"public/live2dw/assets/mtn/02.mtn","hash":"7eafc52edc73b7cb80ae70d34b43c6ac778fa47b","modified":1666946474794},{"_id":"public/live2dw/assets/mtn/03.mtn","hash":"f900737c7a98441cbb2e05255427e6260e19ae68","modified":1666946474794},{"_id":"public/live2dw/assets/mtn/05.mtn","hash":"dd20ad24b5d1830a5d44b9bccb28f922eea5e0e5","modified":1666946474794},{"_id":"public/live2dw/assets/mtn/06.mtn","hash":"ad404bd852d276cdd3d054c953e23f90e4e45ae1","modified":1666946474794},{"_id":"public/live2dw/assets/mtn/07.mtn","hash":"b7f2e3a9fa4f3ffbb6e64a08f8d9f45ca1868ffb","modified":1666946474794},{"_id":"public/live2dw/assets/mtn/08.mtn","hash":"4411c7651ff65195b113d95e7d5ebef8a59a37d9","modified":1666946474794},{"_id":"public/live2dw/lib/L2Dwidget.min.js","hash":"5f1a807437cc723bcadc3791d37add5ceed566a2","modified":1666946474794},{"_id":"public/assets/algolia/algoliasearchLite.min.js","hash":"284416885e4e80e27fa4eae6fc305f4de15b914c","modified":1666946474794},{"_id":"public/2022/10/26/c-知识汇总/内存.jpg","hash":"9ebd28040681223f38ad60e79ec8aa155d165330","modified":1666946474794},{"_id":"public/2022/10/26/c-知识汇总/内存副本.jpg","hash":"76f470b7eb07e0e3186cfbf363bb698ad78d09a9","modified":1666946474794},{"_id":"public/2022/10/20/robotic-operating-system/image-20221020164052499.png","hash":"75031cad7e1671f5a7baeba44bb18b9a70657fea","modified":1666946474794},{"_id":"public/live2dw/lib/L2Dwidget.min.js.map","hash":"3290fe2df45f065b51a1cd7b24ec325cbf9bb5ce","modified":1666946474794},{"_id":"public/assets/algolia/algoliasearchLite.js","hash":"e56ad6b82caf69066de545201014291fc961635e","modified":1666946474794},{"_id":"public/assets/algolia/algoliasearch.min.js","hash":"a3b131a9a47ccc16f4dd8988fabb6d306548db2f","modified":1666946474794},{"_id":"public/js/utils.js","hash":"0b95daada72abb5d64a1e3236049a60120e47cca","modified":1666946474794},{"_id":"public/css/var.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1666946474794},{"_id":"public/js/search/algolia.js","hash":"9feb248552667c53ce1b19bc7a295215f8c77008","modified":1666946474794},{"_id":"public/js/search/local-search.js","hash":"3071a4208fdf89ad7e0031536dd6ffa7bc951e4d","modified":1666946474794},{"_id":"public/js/main.js","hash":"73d2624ed465e4cfb1ebb00b2c8a24f5fc29bb21","modified":1666946474794},{"_id":"public/js/tw_cn.js","hash":"00053ce73210274b3679f42607edef1206eebc68","modified":1666946474794},{"_id":"public/css/index.css","hash":"cf75afba50b27d754bd86a1ad8789b908e2493b0","modified":1666946474794},{"_id":"public/img/lanqiu.jpg","hash":"c591a31cf35f20358f342b5293d9baa35a284383","modified":1666946474794},{"_id":"public/live2dw/assets/moc/hijiki.moc","hash":"44289e62545a7046e0f5231103a851750b78524e","modified":1666946474794},{"_id":"public/live2dw/lib/L2Dwidget.0.min.js","hash":"35bb5b588b6de25c9be2dd51d3fd331feafac02d","modified":1666946474794},{"_id":"public/img/kenan.jpg","hash":"b4cedb8d6f1945d22435875c516b1cedb73ddb5e","modified":1666946474794},{"_id":"public/live2dw/assets/moc/hijiki.2048/texture_00.png","hash":"66464e0d96439695b5542c5e2f5be60739c29999","modified":1666946474794},{"_id":"public/assets/algolia/algoliasearch.js","hash":"6948fcdf071e4983e784e8c458cf201536f77792","modified":1666946474794},{"_id":"public/2022/10/26/c-知识汇总/内存.psd","hash":"2eb0381e76570ab3971f855b63486ca7dfec367d","modified":1666946474794},{"_id":"public/img/haizei2.jpg","hash":"fc148ad7897a630d2d91e17296eeed534e394968","modified":1666946474794},{"_id":"public/live2dw/lib/L2Dwidget.0.min.js.map","hash":"35e71cc2a130199efb167b9a06939576602f0d75","modified":1666946474794},{"_id":"public/img/yang.jpg","hash":"05a1520d1bf5778a8379c9ff76ea9e11c0551736","modified":1666946474794},{"_id":"public/img/haizei.jpg","hash":"6a9d5c484d632b304eca2e40457f4db4b10741ad","modified":1666946474794},{"_id":"source/_posts/slam.md","hash":"b896d5c51d1f4938fa04271d689918395d2f5f50","modified":1668157794497},{"_id":"source/_posts/数学知识.md","hash":"9f77c402f13431c61c8011e462884829ab2a2d5c","modified":1667532826280},{"_id":"themes/butterfly/source/js/code-unfold.js","hash":"bc7dff5982d218c5b3fa27acea43fae987ece11c","modified":1666946798105},{"_id":"themes/butterfly/source/css/_highlight/highlight.styl.bak","hash":"2f95e99b8351fbecd9037a1bbdc3fee9d6ea8a77","modified":1666318501220},{"_id":"themes/butterfly/source/js/search.js","hash":"c2ccbb292f2325bc4eca3d07d49d38e1d0930ead","modified":1666947203152},{"_id":"themes/butterfly/source/css/_partial/highlight.styl","hash":"7a4c2f705c4fb150562da64f2ab2157b0005c3e8","modified":1666947489837},{"_id":"public/categories/数学/index.html","hash":"82700ab5562ee8b155ff35fd17f0aeaeefdf142e","modified":1670054140941},{"_id":"public/2022/11/04/数学知识/index.html","hash":"bf9bbde9510e973f2496470d4b92f439061a6a3e","modified":1670054140941},{"_id":"public/2022/11/04/slam/index.html","hash":"004a6f8c2e061ee524837a986b960c071a997ebb","modified":1670054140941},{"_id":"public/archives/2022/11/index.html","hash":"6ff3fdfac260a7c45bf9075329fc8bbf28c80a6b","modified":1670054140941},{"_id":"public/categories/建图和定位/index.html","hash":"556d4edbc7a1678b0b078ffadf900814f7c275ca","modified":1670054140941},{"_id":"public/tags/数学/index.html","hash":"6e601da84fb9219bb6ba46e32bb06c1f35720089","modified":1670054140941},{"_id":"public/tags/slam14讲/index.html","hash":"067041bbf5424898e5d1ec934c5eb167917226da","modified":1670054140941},{"_id":"public/js/search.js","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1667541084295},{"_id":"public/js/code-unfold.js","hash":"54a7cb38ac46f3e081c7940aac20e7f62b3d9f56","modified":1667541084295},{"_id":"source/_posts/slam/image-20221107155224157.png","hash":"f26d5286560f2cd77ca9d58a18af50f04e59d4fd","modified":1667807544157},{"_id":"public/2022/11/04/slam/image-20221107155224157.png","hash":"f26d5286560f2cd77ca9d58a18af50f04e59d4fd","modified":1667808028416},{"_id":"source/_posts/slam/image-20221107162453662.png","hash":"ef7675ce7959eeadfcdbb3ef6aa67b8802aae5f9","modified":1667809493662},{"_id":"source/_posts/slam/image-20221107164511662.png","hash":"d432579c6705f041954139efefe9e519ff07a8fc","modified":1667810711662},{"_id":"source/_posts/slam/image-20221107161443842.png","hash":"69e2b36f28da35c0f7f2a026f153c259e1e98f9a","modified":1667808883842},{"_id":"source/_posts/slam/image-20221107163948504.png","hash":"f58089ffda09e8647fbe0282eea568ca0226bec8","modified":1667810388505},{"_id":"source/_posts/slam/image-20221107162722048.png","hash":"93a32c5795ffd491d6cce21cc06fbf4783f45fe1","modified":1667809642048},{"_id":"source/_posts/slam/image-20221107165342106.png","hash":"91e5f83ccc3b256e3b6c210438e7339e2bed6671","modified":1667811222107},{"_id":"public/2022/11/04/slam/image-20221107162453662.png","hash":"ef7675ce7959eeadfcdbb3ef6aa67b8802aae5f9","modified":1667812855312},{"_id":"public/2022/11/04/slam/image-20221107164511662.png","hash":"d432579c6705f041954139efefe9e519ff07a8fc","modified":1667812855312},{"_id":"public/2022/11/04/slam/image-20221107161443842.png","hash":"69e2b36f28da35c0f7f2a026f153c259e1e98f9a","modified":1667812855312},{"_id":"public/2022/11/04/slam/image-20221107162722048.png","hash":"93a32c5795ffd491d6cce21cc06fbf4783f45fe1","modified":1667812855312},{"_id":"public/2022/11/04/slam/image-20221107163948504.png","hash":"f58089ffda09e8647fbe0282eea568ca0226bec8","modified":1667812855312},{"_id":"public/2022/11/04/slam/image-20221107165342106.png","hash":"91e5f83ccc3b256e3b6c210438e7339e2bed6671","modified":1667812855312},{"_id":"source/_posts/slam/image-20221108141628283.png","hash":"b556466003c2433d9d54fd4b0287548079603239","modified":1667888188285},{"_id":"source/_posts/slam/image-20221108112428467.png","hash":"37df06613f116d2dfb261b8d3d4dcdd6d4372b2b","modified":1667877868468},{"_id":"public/2022/11/04/slam/image-20221108141628283.png","hash":"b556466003c2433d9d54fd4b0287548079603239","modified":1667888451848},{"_id":"public/2022/11/04/slam/image-20221108112428467.png","hash":"37df06613f116d2dfb261b8d3d4dcdd6d4372b2b","modified":1667888451848},{"_id":"source/_posts/目标识别.md","hash":"7d3e773eaf867749f404d50071034d0ff9d3d53c","modified":1670053149241},{"_id":"source/_posts/cmake.md","hash":"f294ae5c88ca2d65fc1ee4ddfd8eaef3602ef973","modified":1668492910648},{"_id":"source/_posts/g2o.md","hash":"60ba04cc84f8a51bb701e9737346538c6b517409","modified":1668137090095},{"_id":"source/_posts/c-知识汇总/.DS_Store","hash":"b970c65fb57a7dcb8c68f88aec7d90fbda934c48","modified":1668147157491},{"_id":"source/_posts/c-知识汇总/基类派生类.png","hash":"80560a236d431e89f9a36c165841840a1d4e0f84","modified":1668147080516},{"_id":"source/_posts/g2o/20210103152500545.png","hash":"92e8f13abbf07f6fb373293aec7a5155c79585c2","modified":1668136125129},{"_id":"source/_posts/c-知识汇总/基类派生类.ai","hash":"218a099df4b8f439ea4c2891a7deaf34ad45f367","modified":1668147036839},{"_id":"public/2022/11/22/目标识别/index.html","hash":"a4cd5487d0baad92112faadfcb9b76a2a99a3281","modified":1670054140941},{"_id":"public/2022/11/15/cmake/index.html","hash":"7ec946681736c82b864896587d01cbe693bd0a5e","modified":1670054140941},{"_id":"public/2022/11/11/g2o/index.html","hash":"930a4a39ccbca555db5770892267b85ce85c4a6b","modified":1670054140941},{"_id":"public/categories/文献/目标识别/index.html","hash":"d906f7fd253d87bd4411de5988008af5cd3eed26","modified":1670054140941},{"_id":"public/categories/编程语言/c/cmake/index.html","hash":"1185fc378b68c664049f6b0c98f589a7cf610307","modified":1670054140941},{"_id":"public/archives/page/2/index.html","hash":"114c71d63fdb33c7546fcf5ca67429a308a0ed50","modified":1670054140941},{"_id":"public/archives/2022/page/2/index.html","hash":"a6f17c9f299dc51c0893cd72226be33655ecad48","modified":1670054140941},{"_id":"public/tags/cmake/index.html","hash":"a50c479ae5332781ed427a63c53a657bf5185539","modified":1670054140941},{"_id":"public/tags/目标识别/index.html","hash":"67ed42f6c7f02a694719766db569b9f8af5c589c","modified":1670054140941},{"_id":"public/page/2/index.html","hash":"3b66269d7030ee5c1ae8d7ac510e04632c110f87","modified":1670054140941},{"_id":"public/2022/10/26/c-知识汇总/基类派生类.png","hash":"80560a236d431e89f9a36c165841840a1d4e0f84","modified":1669257269421},{"_id":"public/2022/11/11/g2o/20210103152500545.png","hash":"92e8f13abbf07f6fb373293aec7a5155c79585c2","modified":1669257269421},{"_id":"public/2022/10/26/c-知识汇总/基类派生类.ai","hash":"218a099df4b8f439ea4c2891a7deaf34ad45f367","modified":1669257269421},{"_id":"source/_posts/目标识别/image-20221127153059574.png","hash":"f46d96014cf92ce6b34a75528dd2a96647f697d2","modified":1669534259575},{"_id":"source/_posts/目标识别/image-20221125100901770.png","hash":"c2dfb0d2557702677a2302287cd04cf69188dafd","modified":1669342141786},{"_id":"source/_posts/目标识别/image-20221127151329769.png","hash":"9b3c0afd29fb6829546d68b48d69687bfb9a526c","modified":1669533209770},{"_id":"public/2022/11/22/目标识别/image-20221127153059574.png","hash":"f46d96014cf92ce6b34a75528dd2a96647f697d2","modified":1669534295800},{"_id":"public/2022/11/22/目标识别/image-20221125100901770.png","hash":"c2dfb0d2557702677a2302287cd04cf69188dafd","modified":1669534295800},{"_id":"public/2022/11/22/目标识别/image-20221127151329769.png","hash":"9b3c0afd29fb6829546d68b48d69687bfb9a526c","modified":1669534295800},{"_id":"source/_posts/目标识别/image-20221130151059959.png","hash":"2a87e80973329e1634e80b544d14c641827a9378","modified":1669792259966},{"_id":"source/_posts/目标识别/image-20221130160631440.png","hash":"fe43de909710bf888be1bf4a83139e7c38fe2b7e","modified":1669795591441},{"_id":"public/2022/11/22/目标识别/image-20221130151059959.png","hash":"2a87e80973329e1634e80b544d14c641827a9378","modified":1669859445811},{"_id":"public/2022/11/22/目标识别/image-20221130160631440.png","hash":"fe43de909710bf888be1bf4a83139e7c38fe2b7e","modified":1669859445811},{"_id":"source/_posts/目标识别/image-20221203145853415.png","hash":"3cee7e276b47e01e2b3d1f3b8f22b3f4a3c6c82e","modified":1670050733420},{"_id":"source/_posts/目标识别/image-20221201110746341.png","hash":"a65af8828c1550e5267ce342fb8897962f7d76cc","modified":1669864066344},{"_id":"source/_posts/目标识别/image-20221202090850052.png","hash":"e4b016db387f7cbf942261d66883dd33c5cd0c0d","modified":1669943330053},{"_id":"public/2022/11/22/目标识别/image-20221203145853415.png","hash":"3cee7e276b47e01e2b3d1f3b8f22b3f4a3c6c82e","modified":1670054140941},{"_id":"public/2022/11/22/目标识别/image-20221201110746341.png","hash":"a65af8828c1550e5267ce342fb8897962f7d76cc","modified":1670054140941},{"_id":"public/2022/11/22/目标识别/image-20221202090850052.png","hash":"e4b016db387f7cbf942261d66883dd33c5cd0c0d","modified":1670054140941}],"Category":[{"name":"编程语言","_id":"cl9s8r0vf0004203k3cwx1coh"},{"name":"书籍","_id":"cl9s8r0vm000b203kdq677r54"},{"name":"c++","parent":"cl9s8r0vf0004203k3cwx1coh","_id":"cl9s8r0vn000d203kf6qw03ei"},{"name":"机器人","parent":"cl9s8r0vm000b203kdq677r54","_id":"cl9s8r0vo000h203k7s5gfwog"},{"name":"碎片化知识补丁","parent":"cl9s8r0vn000d203kf6qw03ei","_id":"cl9s8r0vo000i203k8sa7auph"},{"name":"文献","_id":"cl9s8r0vq000r203k2yeg0o14"},{"name":"人体姿态","parent":"cl9s8r0vq000r203k2yeg0o14","_id":"cl9s8r0vr000u203kh2t9a0ff"},{"name":"数学","_id":"cla22wp830002qt3k5phfec7a"},{"name":"建图和定位","_id":"cla22wp870004qt3k9awob7qa"},{"name":"cmake","parent":"cl9s8r0vn000d203kf6qw03ei","_id":"claugokra0002ccfycjb4dx9h"},{"name":"目标识别","parent":"cl9s8r0vq000r203k2yeg0o14","_id":"claugokrf000bccfy4j171pfg"}],"Data":[],"Page":[{"_content":"{\n    \"name\": \"string\",\n    \"short_name\": \"Junzhou\",\n    \"theme_color\": \"#49b1f5\",\n    \"background_color\": \"#49b1f5\",\n    \"display\": \"standalone\",\n    \"scope\": \"/\",\n    \"start_url\": \"/\",\n    \"icons\": [\n        {\n          \"src\": \"images/pwaicons/36.png\",\n          \"sizes\": \"36x36\",\n          \"type\": \"image/png\"\n        },\n        {\n            \"src\": \"images/pwaicons/48.png\",\n          \"sizes\": \"48x48\",\n          \"type\": \"image/png\"\n        },\n        {\n          \"src\": \"images/pwaicons/72.png\",\n          \"sizes\": \"72x72\",\n          \"type\": \"image/png\"\n        },\n        {\n          \"src\": \"images/pwaicons/96.png\",\n          \"sizes\": \"96x96\",\n          \"type\": \"image/png\"\n        },\n        {\n          \"src\": \"images/pwaicons/144.png\",\n          \"sizes\": \"144x144\",\n          \"type\": \"image/png\"\n        },\n        {\n          \"src\": \"images/pwaicons/192.png\",\n          \"sizes\": \"192x192\",\n          \"type\": \"image/png\"\n        },\n        {\n            \"src\": \"images/pwaicons/512.png\",\n            \"sizes\": \"512x512\",\n            \"type\": \"image/png\"\n          }\n      ],\n      \"splash_pages\": null\n  }\n\n\n","source":"manifest.json","raw":"{\n    \"name\": \"string\",\n    \"short_name\": \"Junzhou\",\n    \"theme_color\": \"#49b1f5\",\n    \"background_color\": \"#49b1f5\",\n    \"display\": \"standalone\",\n    \"scope\": \"/\",\n    \"start_url\": \"/\",\n    \"icons\": [\n        {\n          \"src\": \"images/pwaicons/36.png\",\n          \"sizes\": \"36x36\",\n          \"type\": \"image/png\"\n        },\n        {\n            \"src\": \"images/pwaicons/48.png\",\n          \"sizes\": \"48x48\",\n          \"type\": \"image/png\"\n        },\n        {\n          \"src\": \"images/pwaicons/72.png\",\n          \"sizes\": \"72x72\",\n          \"type\": \"image/png\"\n        },\n        {\n          \"src\": \"images/pwaicons/96.png\",\n          \"sizes\": \"96x96\",\n          \"type\": \"image/png\"\n        },\n        {\n          \"src\": \"images/pwaicons/144.png\",\n          \"sizes\": \"144x144\",\n          \"type\": \"image/png\"\n        },\n        {\n          \"src\": \"images/pwaicons/192.png\",\n          \"sizes\": \"192x192\",\n          \"type\": \"image/png\"\n        },\n        {\n            \"src\": \"images/pwaicons/512.png\",\n            \"sizes\": \"512x512\",\n            \"type\": \"image/png\"\n          }\n      ],\n      \"splash_pages\": null\n  }\n\n\n","date":"2022-10-21T05:58:51.016Z","updated":"2022-10-21T05:58:51.011Z","path":"manifest.json","layout":"false","title":"","comments":1,"_id":"cl9s8r0v80000203kcdpi3w43","content":"{\"name\":\"string\",\"short_name\":\"Junzhou\",\"theme_color\":\"#49b1f5\",\"background_color\":\"#49b1f5\",\"display\":\"standalone\",\"scope\":\"/\",\"start_url\":\"/\",\"icons\":[{\"src\":\"images/pwaicons/36.png\",\"sizes\":\"36x36\",\"type\":\"image/png\"},{\"src\":\"images/pwaicons/48.png\",\"sizes\":\"48x48\",\"type\":\"image/png\"},{\"src\":\"images/pwaicons/72.png\",\"sizes\":\"72x72\",\"type\":\"image/png\"},{\"src\":\"images/pwaicons/96.png\",\"sizes\":\"96x96\",\"type\":\"image/png\"},{\"src\":\"images/pwaicons/144.png\",\"sizes\":\"144x144\",\"type\":\"image/png\"},{\"src\":\"images/pwaicons/192.png\",\"sizes\":\"192x192\",\"type\":\"image/png\"},{\"src\":\"images/pwaicons/512.png\",\"sizes\":\"512x512\",\"type\":\"image/png\"}],\"splash_pages\":null}","site":{"data":{}},"cover":"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/cover7.JPG","excerpt":"","more":"{\"name\":\"string\",\"short_name\":\"Junzhou\",\"theme_color\":\"#49b1f5\",\"background_color\":\"#49b1f5\",\"display\":\"standalone\",\"scope\":\"/\",\"start_url\":\"/\",\"icons\":[{\"src\":\"images/pwaicons/36.png\",\"sizes\":\"36x36\",\"type\":\"image/png\"},{\"src\":\"images/pwaicons/48.png\",\"sizes\":\"48x48\",\"type\":\"image/png\"},{\"src\":\"images/pwaicons/72.png\",\"sizes\":\"72x72\",\"type\":\"image/png\"},{\"src\":\"images/pwaicons/96.png\",\"sizes\":\"96x96\",\"type\":\"image/png\"},{\"src\":\"images/pwaicons/144.png\",\"sizes\":\"144x144\",\"type\":\"image/png\"},{\"src\":\"images/pwaicons/192.png\",\"sizes\":\"192x192\",\"type\":\"image/png\"},{\"src\":\"images/pwaicons/512.png\",\"sizes\":\"512x512\",\"type\":\"image/png\"}],\"splash_pages\":null}"},{"title":"categories","date":"2022-10-21T07:13:17.000Z","type":"categories","layout":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2022-10-21 15:13:17\ntype: \"categories\"\nlayout: \"categories\"\n---\n","updated":"2022-10-21T07:15:58.510Z","path":"categories/index.html","comments":1,"_id":"cl9s8r0vd0002203k8g0n6u4j","content":"","site":{"data":{}},"cover":"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/cover6.JPG","excerpt":"","more":""},{"title":"tags","date":"2022-10-21T07:16:36.000Z","type":"tags","layout":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2022-10-21 15:16:36\ntype: \"tags\"\nlayout: \"tags\"\n---\n","updated":"2022-10-21T07:18:02.201Z","path":"tags/index.html","comments":1,"_id":"cl9s8r0vi0006203k2czyazjn","content":"","site":{"data":{}},"cover":"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/cover3.jpg","excerpt":"","more":""}],"Post":[{"typora-root-url":"./c-知识汇总","title":"c++知识汇总","mathjax":true,"date":"2022-10-26T09:24:46.000Z","_content":"\n# 数据对齐\n\n## 字节对齐\n字节对齐是指变量存储首地址是其类型长度的整数倍，例如4字节对齐是存储首地址是4的整数倍0x0000,0x0004,0x0008,0x000C,0x0010等\n\n## 字节对齐目的\n提高cpu访问效率以及内存管理，在字节对齐时cpu只需读取一次可以将数据全部提取出来，若字节不对齐要读区数次\n\n## 字节对齐实现方法\n在c++定义语句中添加\\__attribute__((aligned(n)))代码，添加位置可以在原定义语句之前或之后，$n=2^i$, 例如\n```c++\n//2字节对齐\ntypedef __attribute__((aligned(2))) struct A\n{\n\tchar a;\n\tchar b;\n\tint c;\n};\n//char a占1字节，char b占1字节，int c占4字节且起始地址必须是4的整数倍所以char b和int c之间补2字节，整个结构体的尾后地址也必须是2字节对齐(即2的整数倍地址)\n//1byte(a) + 1byte(b) + 2byte(变量间补) + 4byte(c) + 0byte(结构体补)\n//4字节对齐\ntypedef __attribute__((aligned(4))) struct A\n{\n\tchar a;\n\tchar b;\n\tint c;\n};\n//char a占1字节，char b占1字节，int c占4字节且起始地址必须是4的整数倍所以char b和int c之间补2字节，整个结构体的尾后地址也必须是4字节对齐(即4的整数倍地址)\n//1byte(a) + 1byte(b) + 2byte(变量间补) + 4byte(c) + 0byte(结构体补)\n```\n## 小结\n- 结构体中成员类型和数量相同的情况下，只改变位置占用空间可以得到优化，所以应从变量类型所占空间少的成员添加\n\n- 当结构体成员变量类型长度大于结构体指定对齐数目n时，（例如2字节对齐时有int c成员变量），则按照成员变量类型长度对结构体进行最后对齐\n\n- 1. 根据变量定义顺序和所占字节分配空间\n  2. 当变量存储的起始地址不满足对齐方式(自身对齐方式)时要填补字节知直到满足对齐方式\n  3. 所有变量分配空间完成后，整个结构体按照要求的对齐方式（ \\__attribute__((aligned(n))) ）,默认对齐方式是结构体中最大数据类型所占的空间)判断是否再添补空间\n\n\n\n![存储图](内存副本.jpg)\n\n图中上边区域是4字节对齐，下边是4字节对齐，类型数量都一致但是顺序不同\n\n# 类型转换\n\n<img src=\"基类派生类.png\" alt=\"基类派生类\" style=\"zoom:33%;\" />\n派生类转化成基类百分百成功，基类转派生类会出问题（转化后派生类自己的方法和属性丢失）。\n\n## static_cast\n编译时转化，运行时不检查所以没有安全性\n- 派生类(本身、指针、引用)->基类(本身、指针、引用)安全\n- 基类->派生类不安全\n- 非const类型->const类型，反之错误\n- 其他类型指针->void*\n- 基本数据类型之间相互转化\n\n## dynamic_cast\n运行时转化，派生类->基类转化与static_cast效果一致，基类->派生类有类型检查功能所以安全\n- 派生类(本身、指针、引用)->基类指针(本身、指针、引用)\n- 基类指针(本身、指针、引用)->派生类(本身、指针、引用)\n\n## reinterpret_cast\n允许将任意指针转换成其他类型指针，允许任意整数类型和任意指针类型转化，转化时是逐比特位的复制操作，但后续cpu读取内存时会根据转化后的类型进行。\n- reinterpret_cast不关心继承关系，不会在继承类间穿梭\n```c++\nclass A\n{\npublic:\n\tint a;\n\tvoid funA(){cout<<\"A::funA\"<<endl;}\n};\nclass B\n{\npublic:\n\tint b;\n\tvoid funB(){cout<<\"B::funB\"<<endl;}\n};\nclass D: public A, public B\n{\npublic:\n\tint d;\n\tvoid funD(){cout<<\"D::funD\"<<endl;}\n};\nB* pb;\nD d;\npb = &d;\n//pd1指向d对象中B类型部分的起始地址，即*pd1的地址和d的地址不一致\nD* pd1 = reinterpret_cast<D*>(pb);\n//pd2指向d对象中D类型部分的起始地址，即*pd2的地址和d的地址一致\nD* pd2 = static_cast<D*>(pb);\n```\n- reinterpret_cast不会强制去掉const, 例如\n```c++\n//创建函数\nvoid thump(char* p){*p = 'x';}\n//命名函数指针类型\ntypedef void (*PF)(const char*);\n//创建函数指针\nPF pf;\n//给函数指针赋值\npf = reinterpret_cast<PF>(&thump);\n//pf = static_cast<PF>(&thump);错误：无法将void (*)(char*)函数值指针转换成void (*)(const char*)函数指针\nconst char* str = 'h';\npf(str);\n```\n\n\n# 运算符重载\n\n## ->运算符\np->m被解释为(p.operator->())->m， p.perator->()函数的返回值必须是能够访问m成员的指针\n{% hideToggle display,bg,color %}\n{% endhideToggle %}\n```c++\n//实际类\nclass Obj\n{\n\t//静态成员声明\n\tstatic int i, j;\npublic:\n\tvoid f() const {cout << i++ << endl;}\n\tvoid g() const {cout << j++ << endl;}\n};\n//静态成员定义和初始化,不用加static限定符\nint Obj::i = 10;\nint Obj::j = 12;\n//容器类\nclass ObjContainer\n{\n\tvector<Obj*> a;\npublic:\n\tvoid add(Obj* obj)\n\t{\n\t\t//装入容器\n\t\ta.push_back(obj)\n\t}\n\t//声明友元类\n\tfriend class SmartPointer;\n};\n//智能指针，访问Obj类型成员，友元类定义处不用加friend限定符\nclass SmartPointer\n{\n\tObjContainer oc;\n\tint index;\npublic:\n\tSmartPointer(ObjContainer& objc)\n\t{\n\t\toc = objc;\n\t\tindex = 0;\n\t}\n\t//++前缀版\n\tbool operator++()\n\t{\n\t\tif (index >= oc.a.size()-1) return false;\n\t\tif (oc.a[++index] == 0) return false;\n\t\treturn true;\n\t}\n\t//++后缀版本\n\tbool operator++(int) \n\t{\n\t\treturn operator++();\n\t}\n\t//重载运算符->\n\tObj* operator->() const\n\t{\n\t\tif (!oc.a[index])\n\t\t{\n\t\t\tcout << \"Zero value\";\n\t\t\t//强制类型转化空地址\n\t\t\treturn (Obj*)0;\n\t\t}\n\t\treturn oc.a[index];\n\t}\n};\n\nint main()\n{\n\tconst int sz =10;\n\tObj o[sz];\n\tObjContainer oc;\n\tfor (int i=0; i<sz; i++)\n\t{\n\t\toc.add(&o[i]);\n\t}\n\tSmartPointer sp(oc);\n\tdo\n\t{\n\t\tsp->f();//(sp.operator->())->f()\n\t\tsp->g();\n\t}\n\twhile(sp++)\n}\n\n```\n\n## new\n$$\\mathrm{new}\\left\\{\\begin{aligned}\n\t&普通\\mathrm{new}在堆上:\\left\\{\\begin{aligned} \n\t\t\t&\\mathrm{Type}* \\quad ptr = \\mathrm{new} \\quad \\mathrm{Type}(初始值) \\\\\n\t\t\t&\\mathrm{Type}* \\quad ptr = \\mathrm{new} \\quad \\mathrm{Type}[num]\n  \t\\end{aligned}\\right.\\\\\n\t&定位\\mathrm{new}在指定位置:\\left\\{\\begin{aligned} \t\t\t    \t\t\t\t&\\mathrm{Type}* \\quad ptr = \\mathrm{new}(起始地址)\\quad \\mathrm{Type}(初始值) \\\\\n\t\t\t&\\mathrm{Type}* \\quad ptr = \\mathrm{new}(起始地址) \\quad \\mathrm{Type}[num]\n\t\t\\end{aligned}\\right.\n\\end{aligned}\\right.$$\n\n# 名称解析过程\n\nC++中如果编译器遇到一个名称，它会寻找这个名称代表什么。例如x*y, 如果x和y是变量名称，那么表达式就是乘法；如果x是类型名称，那么表达式就是声明一个指针。所以必须知道**上下文**才能知道表达式意义\n\n## 名称分类\n- qualified name即作用域被显式指明的名称，例如::, ->, .， this->count\n- dependent name即依赖于模版参数的名称，也就是访问运算法符::左面的表达式类型依赖于模版参数(模版参数未知), std::vector<T>::iterator\n\n##  名称查找\n\n- $$ordinary\\; lookup常规查找\\left\\{\\begin{align*} & qualified\\; name & & 在指定的作用域内查找 \\\\ & 非qualified\\; name & & 作用域由内到外依次查找 \\end{align*}\\right.$$\n- argument-dependent\\; lookup参数依赖查找，只查找**非qualified name**，除了由内到外查找还会将函数表达式中实参的associated namespace(关联名称空间)和associated class(关联类型)纳入到查找范围中，来查找函数名，注ADL查找会会略using\n\n## 解析依赖型模版名称\n\n编译时期有两个阶段：tokening（符号化）和parsing（解析化）。\n\n模版解析六方面：\n- 非模版中上下文相关性\n- 依赖型类型名称\n- 依赖型模版名称\n- using declaration中的依赖型名称\n- ADL和显式模版实参\n- 依赖型表达式\n\n通常而言， 编译器会把模板名称后面的<当做模板参数列表的开始，否则，<就是比较运算符。但是，当引用的模板名称是 Dependent Name 时，编译器不会假定它是一个模板名称，除非显示的使用 template 关键字来指明，模板代码中常见的->template、.template、::template就应用于这种场景中。\n\n````c++\ntemplate<unsigned long N>\nvoid printBitset (std::bitset<N> const& bs) \n{\n    std::cout << bs.template to_string<char, std::char_traits<char>, std::allocator<char>>();\n}\n```\n","source":"_posts/c-知识汇总.md","raw":"---\ntypora-root-url: ./c-知识汇总\ntitle: c++知识汇总\nmathjax: true\ndate: 2022-10-26 17:24:46\ncategories: \n- 编程语言\n- c++\n- 碎片化知识补丁\ntags: \n- 博客\n- c++\n---\n\n# 数据对齐\n\n## 字节对齐\n字节对齐是指变量存储首地址是其类型长度的整数倍，例如4字节对齐是存储首地址是4的整数倍0x0000,0x0004,0x0008,0x000C,0x0010等\n\n## 字节对齐目的\n提高cpu访问效率以及内存管理，在字节对齐时cpu只需读取一次可以将数据全部提取出来，若字节不对齐要读区数次\n\n## 字节对齐实现方法\n在c++定义语句中添加\\__attribute__((aligned(n)))代码，添加位置可以在原定义语句之前或之后，$n=2^i$, 例如\n```c++\n//2字节对齐\ntypedef __attribute__((aligned(2))) struct A\n{\n\tchar a;\n\tchar b;\n\tint c;\n};\n//char a占1字节，char b占1字节，int c占4字节且起始地址必须是4的整数倍所以char b和int c之间补2字节，整个结构体的尾后地址也必须是2字节对齐(即2的整数倍地址)\n//1byte(a) + 1byte(b) + 2byte(变量间补) + 4byte(c) + 0byte(结构体补)\n//4字节对齐\ntypedef __attribute__((aligned(4))) struct A\n{\n\tchar a;\n\tchar b;\n\tint c;\n};\n//char a占1字节，char b占1字节，int c占4字节且起始地址必须是4的整数倍所以char b和int c之间补2字节，整个结构体的尾后地址也必须是4字节对齐(即4的整数倍地址)\n//1byte(a) + 1byte(b) + 2byte(变量间补) + 4byte(c) + 0byte(结构体补)\n```\n## 小结\n- 结构体中成员类型和数量相同的情况下，只改变位置占用空间可以得到优化，所以应从变量类型所占空间少的成员添加\n\n- 当结构体成员变量类型长度大于结构体指定对齐数目n时，（例如2字节对齐时有int c成员变量），则按照成员变量类型长度对结构体进行最后对齐\n\n- 1. 根据变量定义顺序和所占字节分配空间\n  2. 当变量存储的起始地址不满足对齐方式(自身对齐方式)时要填补字节知直到满足对齐方式\n  3. 所有变量分配空间完成后，整个结构体按照要求的对齐方式（ \\__attribute__((aligned(n))) ）,默认对齐方式是结构体中最大数据类型所占的空间)判断是否再添补空间\n\n\n\n![存储图](内存副本.jpg)\n\n图中上边区域是4字节对齐，下边是4字节对齐，类型数量都一致但是顺序不同\n\n# 类型转换\n\n<img src=\"基类派生类.png\" alt=\"基类派生类\" style=\"zoom:33%;\" />\n派生类转化成基类百分百成功，基类转派生类会出问题（转化后派生类自己的方法和属性丢失）。\n\n## static_cast\n编译时转化，运行时不检查所以没有安全性\n- 派生类(本身、指针、引用)->基类(本身、指针、引用)安全\n- 基类->派生类不安全\n- 非const类型->const类型，反之错误\n- 其他类型指针->void*\n- 基本数据类型之间相互转化\n\n## dynamic_cast\n运行时转化，派生类->基类转化与static_cast效果一致，基类->派生类有类型检查功能所以安全\n- 派生类(本身、指针、引用)->基类指针(本身、指针、引用)\n- 基类指针(本身、指针、引用)->派生类(本身、指针、引用)\n\n## reinterpret_cast\n允许将任意指针转换成其他类型指针，允许任意整数类型和任意指针类型转化，转化时是逐比特位的复制操作，但后续cpu读取内存时会根据转化后的类型进行。\n- reinterpret_cast不关心继承关系，不会在继承类间穿梭\n```c++\nclass A\n{\npublic:\n\tint a;\n\tvoid funA(){cout<<\"A::funA\"<<endl;}\n};\nclass B\n{\npublic:\n\tint b;\n\tvoid funB(){cout<<\"B::funB\"<<endl;}\n};\nclass D: public A, public B\n{\npublic:\n\tint d;\n\tvoid funD(){cout<<\"D::funD\"<<endl;}\n};\nB* pb;\nD d;\npb = &d;\n//pd1指向d对象中B类型部分的起始地址，即*pd1的地址和d的地址不一致\nD* pd1 = reinterpret_cast<D*>(pb);\n//pd2指向d对象中D类型部分的起始地址，即*pd2的地址和d的地址一致\nD* pd2 = static_cast<D*>(pb);\n```\n- reinterpret_cast不会强制去掉const, 例如\n```c++\n//创建函数\nvoid thump(char* p){*p = 'x';}\n//命名函数指针类型\ntypedef void (*PF)(const char*);\n//创建函数指针\nPF pf;\n//给函数指针赋值\npf = reinterpret_cast<PF>(&thump);\n//pf = static_cast<PF>(&thump);错误：无法将void (*)(char*)函数值指针转换成void (*)(const char*)函数指针\nconst char* str = 'h';\npf(str);\n```\n\n\n# 运算符重载\n\n## ->运算符\np->m被解释为(p.operator->())->m， p.perator->()函数的返回值必须是能够访问m成员的指针\n{% hideToggle display,bg,color %}\n{% endhideToggle %}\n```c++\n//实际类\nclass Obj\n{\n\t//静态成员声明\n\tstatic int i, j;\npublic:\n\tvoid f() const {cout << i++ << endl;}\n\tvoid g() const {cout << j++ << endl;}\n};\n//静态成员定义和初始化,不用加static限定符\nint Obj::i = 10;\nint Obj::j = 12;\n//容器类\nclass ObjContainer\n{\n\tvector<Obj*> a;\npublic:\n\tvoid add(Obj* obj)\n\t{\n\t\t//装入容器\n\t\ta.push_back(obj)\n\t}\n\t//声明友元类\n\tfriend class SmartPointer;\n};\n//智能指针，访问Obj类型成员，友元类定义处不用加friend限定符\nclass SmartPointer\n{\n\tObjContainer oc;\n\tint index;\npublic:\n\tSmartPointer(ObjContainer& objc)\n\t{\n\t\toc = objc;\n\t\tindex = 0;\n\t}\n\t//++前缀版\n\tbool operator++()\n\t{\n\t\tif (index >= oc.a.size()-1) return false;\n\t\tif (oc.a[++index] == 0) return false;\n\t\treturn true;\n\t}\n\t//++后缀版本\n\tbool operator++(int) \n\t{\n\t\treturn operator++();\n\t}\n\t//重载运算符->\n\tObj* operator->() const\n\t{\n\t\tif (!oc.a[index])\n\t\t{\n\t\t\tcout << \"Zero value\";\n\t\t\t//强制类型转化空地址\n\t\t\treturn (Obj*)0;\n\t\t}\n\t\treturn oc.a[index];\n\t}\n};\n\nint main()\n{\n\tconst int sz =10;\n\tObj o[sz];\n\tObjContainer oc;\n\tfor (int i=0; i<sz; i++)\n\t{\n\t\toc.add(&o[i]);\n\t}\n\tSmartPointer sp(oc);\n\tdo\n\t{\n\t\tsp->f();//(sp.operator->())->f()\n\t\tsp->g();\n\t}\n\twhile(sp++)\n}\n\n```\n\n## new\n$$\\mathrm{new}\\left\\{\\begin{aligned}\n\t&普通\\mathrm{new}在堆上:\\left\\{\\begin{aligned} \n\t\t\t&\\mathrm{Type}* \\quad ptr = \\mathrm{new} \\quad \\mathrm{Type}(初始值) \\\\\n\t\t\t&\\mathrm{Type}* \\quad ptr = \\mathrm{new} \\quad \\mathrm{Type}[num]\n  \t\\end{aligned}\\right.\\\\\n\t&定位\\mathrm{new}在指定位置:\\left\\{\\begin{aligned} \t\t\t    \t\t\t\t&\\mathrm{Type}* \\quad ptr = \\mathrm{new}(起始地址)\\quad \\mathrm{Type}(初始值) \\\\\n\t\t\t&\\mathrm{Type}* \\quad ptr = \\mathrm{new}(起始地址) \\quad \\mathrm{Type}[num]\n\t\t\\end{aligned}\\right.\n\\end{aligned}\\right.$$\n\n# 名称解析过程\n\nC++中如果编译器遇到一个名称，它会寻找这个名称代表什么。例如x*y, 如果x和y是变量名称，那么表达式就是乘法；如果x是类型名称，那么表达式就是声明一个指针。所以必须知道**上下文**才能知道表达式意义\n\n## 名称分类\n- qualified name即作用域被显式指明的名称，例如::, ->, .， this->count\n- dependent name即依赖于模版参数的名称，也就是访问运算法符::左面的表达式类型依赖于模版参数(模版参数未知), std::vector<T>::iterator\n\n##  名称查找\n\n- $$ordinary\\; lookup常规查找\\left\\{\\begin{align*} & qualified\\; name & & 在指定的作用域内查找 \\\\ & 非qualified\\; name & & 作用域由内到外依次查找 \\end{align*}\\right.$$\n- argument-dependent\\; lookup参数依赖查找，只查找**非qualified name**，除了由内到外查找还会将函数表达式中实参的associated namespace(关联名称空间)和associated class(关联类型)纳入到查找范围中，来查找函数名，注ADL查找会会略using\n\n## 解析依赖型模版名称\n\n编译时期有两个阶段：tokening（符号化）和parsing（解析化）。\n\n模版解析六方面：\n- 非模版中上下文相关性\n- 依赖型类型名称\n- 依赖型模版名称\n- using declaration中的依赖型名称\n- ADL和显式模版实参\n- 依赖型表达式\n\n通常而言， 编译器会把模板名称后面的<当做模板参数列表的开始，否则，<就是比较运算符。但是，当引用的模板名称是 Dependent Name 时，编译器不会假定它是一个模板名称，除非显示的使用 template 关键字来指明，模板代码中常见的->template、.template、::template就应用于这种场景中。\n\n````c++\ntemplate<unsigned long N>\nvoid printBitset (std::bitset<N> const& bs) \n{\n    std::cout << bs.template to_string<char, std::char_traits<char>, std::allocator<char>>();\n}\n```\n","slug":"c-知识汇总","published":1,"updated":"2022-11-19T01:36:29.207Z","_id":"cl9s8r0va0001203k88x70g64","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"数据对齐\"><a href=\"#数据对齐\" class=\"headerlink\" title=\"数据对齐\"></a>数据对齐</h1><h2 id=\"字节对齐\"><a href=\"#字节对齐\" class=\"headerlink\" title=\"字节对齐\"></a>字节对齐</h2><p>字节对齐是指变量存储首地址是其类型长度的整数倍，例如4字节对齐是存储首地址是4的整数倍0x0000,0x0004,0x0008,0x000C,0x0010等</p>\n<h2 id=\"字节对齐目的\"><a href=\"#字节对齐目的\" class=\"headerlink\" title=\"字节对齐目的\"></a>字节对齐目的</h2><p>提高cpu访问效率以及内存管理，在字节对齐时cpu只需读取一次可以将数据全部提取出来，若字节不对齐要读区数次</p>\n<h2 id=\"字节对齐实现方法\"><a href=\"#字节对齐实现方法\" class=\"headerlink\" title=\"字节对齐实现方法\"></a>字节对齐实现方法</h2><p>在c++定义语句中添加__attribute__((aligned(n)))代码，添加位置可以在原定义语句之前或之后，$n=2^i$, 例如<br><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//2字节对齐</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> __attribute__((<span class=\"built_in\">aligned</span>(<span class=\"number\">2</span>))) <span class=\"keyword\">struct</span> <span class=\"title class_\">A</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\t<span class=\"type\">char</span> a;</span><br><span class=\"line\">\t<span class=\"type\">char</span> b;</span><br><span class=\"line\">\t<span class=\"type\">int</span> c;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"comment\">//char a占1字节，char b占1字节，int c占4字节且起始地址必须是4的整数倍所以char b和int c之间补2字节，整个结构体的尾后地址也必须是2字节对齐(即2的整数倍地址)</span></span><br><span class=\"line\"><span class=\"comment\">//1byte(a) + 1byte(b) + 2byte(变量间补) + 4byte(c) + 0byte(结构体补)</span></span><br><span class=\"line\"><span class=\"comment\">//4字节对齐</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> __attribute__((<span class=\"built_in\">aligned</span>(<span class=\"number\">4</span>))) <span class=\"keyword\">struct</span> <span class=\"title class_\">A</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\t<span class=\"type\">char</span> a;</span><br><span class=\"line\">\t<span class=\"type\">char</span> b;</span><br><span class=\"line\">\t<span class=\"type\">int</span> c;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"comment\">//char a占1字节，char b占1字节，int c占4字节且起始地址必须是4的整数倍所以char b和int c之间补2字节，整个结构体的尾后地址也必须是4字节对齐(即4的整数倍地址)</span></span><br><span class=\"line\"><span class=\"comment\">//1byte(a) + 1byte(b) + 2byte(变量间补) + 4byte(c) + 0byte(结构体补)</span></span><br></pre></td></tr></table></figure></p>\n<h2 id=\"小结\"><a href=\"#小结\" class=\"headerlink\" title=\"小结\"></a>小结</h2><ul>\n<li><p>结构体中成员类型和数量相同的情况下，只改变位置占用空间可以得到优化，所以应从变量类型所占空间少的成员添加</p>\n</li>\n<li><p>当结构体成员变量类型长度大于结构体指定对齐数目n时，（例如2字节对齐时有int c成员变量），则按照成员变量类型长度对结构体进行最后对齐</p>\n</li>\n<li><ol>\n<li>根据变量定义顺序和所占字节分配空间</li>\n<li>当变量存储的起始地址不满足对齐方式(自身对齐方式)时要填补字节知直到满足对齐方式</li>\n<li>所有变量分配空间完成后，整个结构体按照要求的对齐方式（ __attribute__((aligned(n))) ）,默认对齐方式是结构体中最大数据类型所占的空间)判断是否再添补空间</li>\n</ol>\n</li>\n</ul>\n<p><img src=\"内存副本.jpg\" alt=\"存储图\"></p>\n<p>图中上边区域是4字节对齐，下边是4字节对齐，类型数量都一致但是顺序不同</p>\n<h1 id=\"类型转换\"><a href=\"#类型转换\" class=\"headerlink\" title=\"类型转换\"></a>类型转换</h1><p><img src=\"基类派生类.png\" alt=\"基类派生类\" style=\"zoom:33%;\" /><br>派生类转化成基类百分百成功，基类转派生类会出问题（转化后派生类自己的方法和属性丢失）。</p>\n<h2 id=\"static-cast\"><a href=\"#static-cast\" class=\"headerlink\" title=\"static_cast\"></a>static_cast</h2><p>编译时转化，运行时不检查所以没有安全性</p>\n<ul>\n<li>派生类(本身、指针、引用)-&gt;基类(本身、指针、引用)安全</li>\n<li>基类-&gt;派生类不安全</li>\n<li>非const类型-&gt;const类型，反之错误</li>\n<li>其他类型指针-&gt;void*</li>\n<li>基本数据类型之间相互转化</li>\n</ul>\n<h2 id=\"dynamic-cast\"><a href=\"#dynamic-cast\" class=\"headerlink\" title=\"dynamic_cast\"></a>dynamic_cast</h2><p>运行时转化，派生类-&gt;基类转化与static_cast效果一致，基类-&gt;派生类有类型检查功能所以安全</p>\n<ul>\n<li>派生类(本身、指针、引用)-&gt;基类指针(本身、指针、引用)</li>\n<li>基类指针(本身、指针、引用)-&gt;派生类(本身、指针、引用)</li>\n</ul>\n<h2 id=\"reinterpret-cast\"><a href=\"#reinterpret-cast\" class=\"headerlink\" title=\"reinterpret_cast\"></a>reinterpret_cast</h2><p>允许将任意指针转换成其他类型指针，允许任意整数类型和任意指针类型转化，转化时是逐比特位的复制操作，但后续cpu读取内存时会根据转化后的类型进行。</p>\n<ul>\n<li>reinterpret_cast不关心继承关系，不会在继承类间穿梭<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">A</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">\t<span class=\"type\">int</span> a;</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">funA</span><span class=\"params\">()</span></span>&#123;cout&lt;&lt;<span class=\"string\">&quot;A::funA&quot;</span>&lt;&lt;endl;&#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">B</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">\t<span class=\"type\">int</span> b;</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">funB</span><span class=\"params\">()</span></span>&#123;cout&lt;&lt;<span class=\"string\">&quot;B::funB&quot;</span>&lt;&lt;endl;&#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">D</span>: <span class=\"keyword\">public</span> A, <span class=\"keyword\">public</span> B</span><br><span class=\"line\">&#123;</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">\t<span class=\"type\">int</span> d;</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">funD</span><span class=\"params\">()</span></span>&#123;cout&lt;&lt;<span class=\"string\">&quot;D::funD&quot;</span>&lt;&lt;endl;&#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\">B* pb;</span><br><span class=\"line\">D d;</span><br><span class=\"line\">pb = &amp;d;</span><br><span class=\"line\"><span class=\"comment\">//pd1指向d对象中B类型部分的起始地址，即*pd1的地址和d的地址不一致</span></span><br><span class=\"line\">D* pd1 = <span class=\"built_in\">reinterpret_cast</span>&lt;D*&gt;(pb);</span><br><span class=\"line\"><span class=\"comment\">//pd2指向d对象中D类型部分的起始地址，即*pd2的地址和d的地址一致</span></span><br><span class=\"line\">D* pd2 = <span class=\"built_in\">static_cast</span>&lt;D*&gt;(pb);</span><br></pre></td></tr></table></figure></li>\n<li>reinterpret_cast不会强制去掉const, 例如<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//创建函数</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">thump</span><span class=\"params\">(<span class=\"type\">char</span>* p)</span></span>&#123;*p = <span class=\"string\">&#x27;x&#x27;</span>;&#125;</span><br><span class=\"line\"><span class=\"comment\">//命名函数指针类型</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">typedef</span> <span class=\"title\">void</span> <span class=\"params\">(*PF)</span><span class=\"params\">(<span class=\"type\">const</span> <span class=\"type\">char</span>*)</span></span>;</span><br><span class=\"line\"><span class=\"comment\">//创建函数指针</span></span><br><span class=\"line\">PF pf;</span><br><span class=\"line\"><span class=\"comment\">//给函数指针赋值</span></span><br><span class=\"line\">pf = <span class=\"built_in\">reinterpret_cast</span>&lt;PF&gt;(&amp;thump);</span><br><span class=\"line\"><span class=\"comment\">//pf = static_cast&lt;PF&gt;(&amp;thump);错误：无法将void (*)(char*)函数值指针转换成void (*)(const char*)函数指针</span></span><br><span class=\"line\"><span class=\"type\">const</span> <span class=\"type\">char</span>* str = <span class=\"string\">&#x27;h&#x27;</span>;</span><br><span class=\"line\"><span class=\"built_in\">pf</span>(str);</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h1 id=\"运算符重载\"><a href=\"#运算符重载\" class=\"headerlink\" title=\"运算符重载\"></a>运算符重载</h1><h2 id=\"gt-运算符\"><a href=\"#gt-运算符\" class=\"headerlink\" title=\"-&gt;运算符\"></a>-&gt;运算符</h2><p>p-&gt;m被解释为(p.operator-&gt;())-&gt;m， p.perator-&gt;()函数的返回值必须是能够访问m成员的指针<br><details class=\"toggle\" style=\"border: 1px solid bg\"><summary class=\"toggle-button\" style=\"background-color: bg;color: color\">display</summary><div class=\"toggle-content\"></div></details><br><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//实际类</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Obj</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\t<span class=\"comment\">//静态成员声明</span></span><br><span class=\"line\">\t<span class=\"type\">static</span> <span class=\"type\">int</span> i, j;</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">f</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;cout &lt;&lt; i++ &lt;&lt; endl;&#125;</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">g</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;cout &lt;&lt; j++ &lt;&lt; endl;&#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"comment\">//静态成员定义和初始化,不用加static限定符</span></span><br><span class=\"line\"><span class=\"type\">int</span> Obj::i = <span class=\"number\">10</span>;</span><br><span class=\"line\"><span class=\"type\">int</span> Obj::j = <span class=\"number\">12</span>;</span><br><span class=\"line\"><span class=\"comment\">//容器类</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ObjContainer</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\tvector&lt;Obj*&gt; a;</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">add</span><span class=\"params\">(Obj* obj)</span></span></span><br><span class=\"line\"><span class=\"function\">\t</span>&#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">//装入容器</span></span><br><span class=\"line\">\t\ta.<span class=\"built_in\">push_back</span>(obj)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"comment\">//声明友元类</span></span><br><span class=\"line\">\t<span class=\"keyword\">friend</span> <span class=\"keyword\">class</span> <span class=\"title class_\">SmartPointer</span>;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"comment\">//智能指针，访问Obj类型成员，友元类定义处不用加friend限定符</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SmartPointer</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\tObjContainer oc;</span><br><span class=\"line\">\t<span class=\"type\">int</span> index;</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">\t<span class=\"built_in\">SmartPointer</span>(ObjContainer&amp; objc)</span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\toc = objc;</span><br><span class=\"line\">\t\tindex = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"comment\">//++前缀版</span></span><br><span class=\"line\">\t<span class=\"type\">bool</span> <span class=\"keyword\">operator</span>++()</span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (index &gt;= oc.a.<span class=\"built_in\">size</span>()<span class=\"number\">-1</span>) <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (oc.a[++index] == <span class=\"number\">0</span>) <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"comment\">//++后缀版本</span></span><br><span class=\"line\">\t<span class=\"type\">bool</span> <span class=\"keyword\">operator</span>++(<span class=\"type\">int</span>) </span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">operator</span>++();</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"comment\">//重载运算符-&gt;</span></span><br><span class=\"line\">\tObj* <span class=\"keyword\">operator</span>-&gt;() <span class=\"type\">const</span></span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (!oc.a[index])</span><br><span class=\"line\">\t\t&#123;</span><br><span class=\"line\">\t\t\tcout &lt;&lt; <span class=\"string\">&quot;Zero value&quot;</span>;</span><br><span class=\"line\">\t\t\t<span class=\"comment\">//强制类型转化空地址</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">return</span> (Obj*)<span class=\"number\">0</span>;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> oc.a[index];</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"type\">const</span> <span class=\"type\">int</span> sz =<span class=\"number\">10</span>;</span><br><span class=\"line\">\tObj o[sz];</span><br><span class=\"line\">\tObjContainer oc;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (<span class=\"type\">int</span> i=<span class=\"number\">0</span>; i&lt;sz; i++)</span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\toc.<span class=\"built_in\">add</span>(&amp;o[i]);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"function\">SmartPointer <span class=\"title\">sp</span><span class=\"params\">(oc)</span></span>;</span><br><span class=\"line\">\t<span class=\"keyword\">do</span></span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\tsp-&gt;<span class=\"built_in\">f</span>();<span class=\"comment\">//(sp.operator-&gt;())-&gt;f()</span></span><br><span class=\"line\">\t\tsp-&gt;<span class=\"built_in\">g</span>();</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span>(sp++)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></p>\n<h2 id=\"new\"><a href=\"#new\" class=\"headerlink\" title=\"new\"></a>new</h2><script type=\"math/tex; mode=display\">\\mathrm{new}\\left\\{\\begin{aligned}\n    &普通\\mathrm{new}在堆上:\\left\\{\\begin{aligned} \n            &\\mathrm{Type}* \\quad ptr = \\mathrm{new} \\quad \\mathrm{Type}(初始值) \\\\\n            &\\mathrm{Type}* \\quad ptr = \\mathrm{new} \\quad \\mathrm{Type}[num]\n      \\end{aligned}\\right.\\\\\n    &定位\\mathrm{new}在指定位置:\\left\\{\\begin{aligned}                                 &\\mathrm{Type}* \\quad ptr = \\mathrm{new}(起始地址)\\quad \\mathrm{Type}(初始值) \\\\\n            &\\mathrm{Type}* \\quad ptr = \\mathrm{new}(起始地址) \\quad \\mathrm{Type}[num]\n        \\end{aligned}\\right.\n\\end{aligned}\\right.</script><h1 id=\"名称解析过程\"><a href=\"#名称解析过程\" class=\"headerlink\" title=\"名称解析过程\"></a>名称解析过程</h1><p>C++中如果编译器遇到一个名称，它会寻找这个名称代表什么。例如x<em>y, 如果x和y是变量名称，那么表达式就是乘法；如果x是类型名称，那么表达式就是声明一个指针。所以必须知道<em>*上下文</em></em>才能知道表达式意义</p>\n<h2 id=\"名称分类\"><a href=\"#名称分类\" class=\"headerlink\" title=\"名称分类\"></a>名称分类</h2><ul>\n<li>qualified name即作用域被显式指明的名称，例如::, -&gt;, .， this-&gt;count</li>\n<li>dependent name即依赖于模版参数的名称，也就是访问运算法符::左面的表达式类型依赖于模版参数(模版参数未知), std::vector<T>::iterator</li>\n</ul>\n<h2 id=\"名称查找\"><a href=\"#名称查找\" class=\"headerlink\" title=\"名称查找\"></a>名称查找</h2><ul>\n<li><script type=\"math/tex; mode=display\">ordinary\\; lookup常规查找\\left\\{\\begin{align*} & qualified\\; name & & 在指定的作用域内查找 \\\\ & 非qualified\\; name & & 作用域由内到外依次查找 \\end{align*}\\right.</script></li>\n<li>argument-dependent\\; lookup参数依赖查找，只查找<strong>非qualified name</strong>，除了由内到外查找还会将函数表达式中实参的associated namespace(关联名称空间)和associated class(关联类型)纳入到查找范围中，来查找函数名，注ADL查找会会略using</li>\n</ul>\n<h2 id=\"解析依赖型模版名称\"><a href=\"#解析依赖型模版名称\" class=\"headerlink\" title=\"解析依赖型模版名称\"></a>解析依赖型模版名称</h2><p>编译时期有两个阶段：tokening（符号化）和parsing（解析化）。</p>\n<p>模版解析六方面：</p>\n<ul>\n<li>非模版中上下文相关性</li>\n<li>依赖型类型名称</li>\n<li>依赖型模版名称</li>\n<li>using declaration中的依赖型名称</li>\n<li>ADL和显式模版实参</li>\n<li>依赖型表达式</li>\n</ul>\n<p>通常而言， 编译器会把模板名称后面的&lt;当做模板参数列表的开始，否则，&lt;就是比较运算符。但是，当引用的模板名称是 Dependent Name 时，编译器不会假定它是一个模板名称，除非显示的使用 template 关键字来指明，模板代码中常见的-&gt;template、.template、::template就应用于这种场景中。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template&lt;unsigned long N&gt;</span><br><span class=\"line\">void printBitset (std::bitset&lt;N&gt; const&amp; bs) </span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    std::cout &lt;&lt; bs.template to_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt;&gt;();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"cover":"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/cover7.JPG","excerpt":"","more":"<h1 id=\"数据对齐\"><a href=\"#数据对齐\" class=\"headerlink\" title=\"数据对齐\"></a>数据对齐</h1><h2 id=\"字节对齐\"><a href=\"#字节对齐\" class=\"headerlink\" title=\"字节对齐\"></a>字节对齐</h2><p>字节对齐是指变量存储首地址是其类型长度的整数倍，例如4字节对齐是存储首地址是4的整数倍0x0000,0x0004,0x0008,0x000C,0x0010等</p>\n<h2 id=\"字节对齐目的\"><a href=\"#字节对齐目的\" class=\"headerlink\" title=\"字节对齐目的\"></a>字节对齐目的</h2><p>提高cpu访问效率以及内存管理，在字节对齐时cpu只需读取一次可以将数据全部提取出来，若字节不对齐要读区数次</p>\n<h2 id=\"字节对齐实现方法\"><a href=\"#字节对齐实现方法\" class=\"headerlink\" title=\"字节对齐实现方法\"></a>字节对齐实现方法</h2><p>在c++定义语句中添加__attribute__((aligned(n)))代码，添加位置可以在原定义语句之前或之后，$n=2^i$, 例如<br><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//2字节对齐</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> __attribute__((<span class=\"built_in\">aligned</span>(<span class=\"number\">2</span>))) <span class=\"keyword\">struct</span> <span class=\"title class_\">A</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\t<span class=\"type\">char</span> a;</span><br><span class=\"line\">\t<span class=\"type\">char</span> b;</span><br><span class=\"line\">\t<span class=\"type\">int</span> c;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"comment\">//char a占1字节，char b占1字节，int c占4字节且起始地址必须是4的整数倍所以char b和int c之间补2字节，整个结构体的尾后地址也必须是2字节对齐(即2的整数倍地址)</span></span><br><span class=\"line\"><span class=\"comment\">//1byte(a) + 1byte(b) + 2byte(变量间补) + 4byte(c) + 0byte(结构体补)</span></span><br><span class=\"line\"><span class=\"comment\">//4字节对齐</span></span><br><span class=\"line\"><span class=\"keyword\">typedef</span> __attribute__((<span class=\"built_in\">aligned</span>(<span class=\"number\">4</span>))) <span class=\"keyword\">struct</span> <span class=\"title class_\">A</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\t<span class=\"type\">char</span> a;</span><br><span class=\"line\">\t<span class=\"type\">char</span> b;</span><br><span class=\"line\">\t<span class=\"type\">int</span> c;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"comment\">//char a占1字节，char b占1字节，int c占4字节且起始地址必须是4的整数倍所以char b和int c之间补2字节，整个结构体的尾后地址也必须是4字节对齐(即4的整数倍地址)</span></span><br><span class=\"line\"><span class=\"comment\">//1byte(a) + 1byte(b) + 2byte(变量间补) + 4byte(c) + 0byte(结构体补)</span></span><br></pre></td></tr></table></figure></p>\n<h2 id=\"小结\"><a href=\"#小结\" class=\"headerlink\" title=\"小结\"></a>小结</h2><ul>\n<li><p>结构体中成员类型和数量相同的情况下，只改变位置占用空间可以得到优化，所以应从变量类型所占空间少的成员添加</p>\n</li>\n<li><p>当结构体成员变量类型长度大于结构体指定对齐数目n时，（例如2字节对齐时有int c成员变量），则按照成员变量类型长度对结构体进行最后对齐</p>\n</li>\n<li><ol>\n<li>根据变量定义顺序和所占字节分配空间</li>\n<li>当变量存储的起始地址不满足对齐方式(自身对齐方式)时要填补字节知直到满足对齐方式</li>\n<li>所有变量分配空间完成后，整个结构体按照要求的对齐方式（ __attribute__((aligned(n))) ）,默认对齐方式是结构体中最大数据类型所占的空间)判断是否再添补空间</li>\n</ol>\n</li>\n</ul>\n<p><img src=\"内存副本.jpg\" alt=\"存储图\"></p>\n<p>图中上边区域是4字节对齐，下边是4字节对齐，类型数量都一致但是顺序不同</p>\n<h1 id=\"类型转换\"><a href=\"#类型转换\" class=\"headerlink\" title=\"类型转换\"></a>类型转换</h1><p><img src=\"基类派生类.png\" alt=\"基类派生类\" style=\"zoom:33%;\" /><br>派生类转化成基类百分百成功，基类转派生类会出问题（转化后派生类自己的方法和属性丢失）。</p>\n<h2 id=\"static-cast\"><a href=\"#static-cast\" class=\"headerlink\" title=\"static_cast\"></a>static_cast</h2><p>编译时转化，运行时不检查所以没有安全性</p>\n<ul>\n<li>派生类(本身、指针、引用)-&gt;基类(本身、指针、引用)安全</li>\n<li>基类-&gt;派生类不安全</li>\n<li>非const类型-&gt;const类型，反之错误</li>\n<li>其他类型指针-&gt;void*</li>\n<li>基本数据类型之间相互转化</li>\n</ul>\n<h2 id=\"dynamic-cast\"><a href=\"#dynamic-cast\" class=\"headerlink\" title=\"dynamic_cast\"></a>dynamic_cast</h2><p>运行时转化，派生类-&gt;基类转化与static_cast效果一致，基类-&gt;派生类有类型检查功能所以安全</p>\n<ul>\n<li>派生类(本身、指针、引用)-&gt;基类指针(本身、指针、引用)</li>\n<li>基类指针(本身、指针、引用)-&gt;派生类(本身、指针、引用)</li>\n</ul>\n<h2 id=\"reinterpret-cast\"><a href=\"#reinterpret-cast\" class=\"headerlink\" title=\"reinterpret_cast\"></a>reinterpret_cast</h2><p>允许将任意指针转换成其他类型指针，允许任意整数类型和任意指针类型转化，转化时是逐比特位的复制操作，但后续cpu读取内存时会根据转化后的类型进行。</p>\n<ul>\n<li>reinterpret_cast不关心继承关系，不会在继承类间穿梭<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">A</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">\t<span class=\"type\">int</span> a;</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">funA</span><span class=\"params\">()</span></span>&#123;cout&lt;&lt;<span class=\"string\">&quot;A::funA&quot;</span>&lt;&lt;endl;&#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">B</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">\t<span class=\"type\">int</span> b;</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">funB</span><span class=\"params\">()</span></span>&#123;cout&lt;&lt;<span class=\"string\">&quot;B::funB&quot;</span>&lt;&lt;endl;&#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">D</span>: <span class=\"keyword\">public</span> A, <span class=\"keyword\">public</span> B</span><br><span class=\"line\">&#123;</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">\t<span class=\"type\">int</span> d;</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">funD</span><span class=\"params\">()</span></span>&#123;cout&lt;&lt;<span class=\"string\">&quot;D::funD&quot;</span>&lt;&lt;endl;&#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\">B* pb;</span><br><span class=\"line\">D d;</span><br><span class=\"line\">pb = &amp;d;</span><br><span class=\"line\"><span class=\"comment\">//pd1指向d对象中B类型部分的起始地址，即*pd1的地址和d的地址不一致</span></span><br><span class=\"line\">D* pd1 = <span class=\"built_in\">reinterpret_cast</span>&lt;D*&gt;(pb);</span><br><span class=\"line\"><span class=\"comment\">//pd2指向d对象中D类型部分的起始地址，即*pd2的地址和d的地址一致</span></span><br><span class=\"line\">D* pd2 = <span class=\"built_in\">static_cast</span>&lt;D*&gt;(pb);</span><br></pre></td></tr></table></figure></li>\n<li>reinterpret_cast不会强制去掉const, 例如<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//创建函数</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">thump</span><span class=\"params\">(<span class=\"type\">char</span>* p)</span></span>&#123;*p = <span class=\"string\">&#x27;x&#x27;</span>;&#125;</span><br><span class=\"line\"><span class=\"comment\">//命名函数指针类型</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">typedef</span> <span class=\"title\">void</span> <span class=\"params\">(*PF)</span><span class=\"params\">(<span class=\"type\">const</span> <span class=\"type\">char</span>*)</span></span>;</span><br><span class=\"line\"><span class=\"comment\">//创建函数指针</span></span><br><span class=\"line\">PF pf;</span><br><span class=\"line\"><span class=\"comment\">//给函数指针赋值</span></span><br><span class=\"line\">pf = <span class=\"built_in\">reinterpret_cast</span>&lt;PF&gt;(&amp;thump);</span><br><span class=\"line\"><span class=\"comment\">//pf = static_cast&lt;PF&gt;(&amp;thump);错误：无法将void (*)(char*)函数值指针转换成void (*)(const char*)函数指针</span></span><br><span class=\"line\"><span class=\"type\">const</span> <span class=\"type\">char</span>* str = <span class=\"string\">&#x27;h&#x27;</span>;</span><br><span class=\"line\"><span class=\"built_in\">pf</span>(str);</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h1 id=\"运算符重载\"><a href=\"#运算符重载\" class=\"headerlink\" title=\"运算符重载\"></a>运算符重载</h1><h2 id=\"gt-运算符\"><a href=\"#gt-运算符\" class=\"headerlink\" title=\"-&gt;运算符\"></a>-&gt;运算符</h2><p>p-&gt;m被解释为(p.operator-&gt;())-&gt;m， p.perator-&gt;()函数的返回值必须是能够访问m成员的指针<br><details class=\"toggle\" style=\"border: 1px solid bg\"><summary class=\"toggle-button\" style=\"background-color: bg;color: color\">display</summary><div class=\"toggle-content\"></div></details><br><figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">//实际类</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Obj</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\t<span class=\"comment\">//静态成员声明</span></span><br><span class=\"line\">\t<span class=\"type\">static</span> <span class=\"type\">int</span> i, j;</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">f</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;cout &lt;&lt; i++ &lt;&lt; endl;&#125;</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">g</span><span class=\"params\">()</span> <span class=\"type\">const</span> </span>&#123;cout &lt;&lt; j++ &lt;&lt; endl;&#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"comment\">//静态成员定义和初始化,不用加static限定符</span></span><br><span class=\"line\"><span class=\"type\">int</span> Obj::i = <span class=\"number\">10</span>;</span><br><span class=\"line\"><span class=\"type\">int</span> Obj::j = <span class=\"number\">12</span>;</span><br><span class=\"line\"><span class=\"comment\">//容器类</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ObjContainer</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\tvector&lt;Obj*&gt; a;</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">\t<span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">add</span><span class=\"params\">(Obj* obj)</span></span></span><br><span class=\"line\"><span class=\"function\">\t</span>&#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">//装入容器</span></span><br><span class=\"line\">\t\ta.<span class=\"built_in\">push_back</span>(obj)</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"comment\">//声明友元类</span></span><br><span class=\"line\">\t<span class=\"keyword\">friend</span> <span class=\"keyword\">class</span> <span class=\"title class_\">SmartPointer</span>;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"comment\">//智能指针，访问Obj类型成员，友元类定义处不用加friend限定符</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SmartPointer</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\tObjContainer oc;</span><br><span class=\"line\">\t<span class=\"type\">int</span> index;</span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">\t<span class=\"built_in\">SmartPointer</span>(ObjContainer&amp; objc)</span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\toc = objc;</span><br><span class=\"line\">\t\tindex = <span class=\"number\">0</span>;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"comment\">//++前缀版</span></span><br><span class=\"line\">\t<span class=\"type\">bool</span> <span class=\"keyword\">operator</span>++()</span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (index &gt;= oc.a.<span class=\"built_in\">size</span>()<span class=\"number\">-1</span>) <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (oc.a[++index] == <span class=\"number\">0</span>) <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"comment\">//++后缀版本</span></span><br><span class=\"line\">\t<span class=\"type\">bool</span> <span class=\"keyword\">operator</span>++(<span class=\"type\">int</span>) </span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">operator</span>++();</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"comment\">//重载运算符-&gt;</span></span><br><span class=\"line\">\tObj* <span class=\"keyword\">operator</span>-&gt;() <span class=\"type\">const</span></span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\t<span class=\"keyword\">if</span> (!oc.a[index])</span><br><span class=\"line\">\t\t&#123;</span><br><span class=\"line\">\t\t\tcout &lt;&lt; <span class=\"string\">&quot;Zero value&quot;</span>;</span><br><span class=\"line\">\t\t\t<span class=\"comment\">//强制类型转化空地址</span></span><br><span class=\"line\">\t\t\t<span class=\"keyword\">return</span> (Obj*)<span class=\"number\">0</span>;</span><br><span class=\"line\">\t\t&#125;</span><br><span class=\"line\">\t\t<span class=\"keyword\">return</span> oc.a[index];</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">\t<span class=\"type\">const</span> <span class=\"type\">int</span> sz =<span class=\"number\">10</span>;</span><br><span class=\"line\">\tObj o[sz];</span><br><span class=\"line\">\tObjContainer oc;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> (<span class=\"type\">int</span> i=<span class=\"number\">0</span>; i&lt;sz; i++)</span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\toc.<span class=\"built_in\">add</span>(&amp;o[i]);</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"function\">SmartPointer <span class=\"title\">sp</span><span class=\"params\">(oc)</span></span>;</span><br><span class=\"line\">\t<span class=\"keyword\">do</span></span><br><span class=\"line\">\t&#123;</span><br><span class=\"line\">\t\tsp-&gt;<span class=\"built_in\">f</span>();<span class=\"comment\">//(sp.operator-&gt;())-&gt;f()</span></span><br><span class=\"line\">\t\tsp-&gt;<span class=\"built_in\">g</span>();</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">\t<span class=\"keyword\">while</span>(sp++)</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></p>\n<h2 id=\"new\"><a href=\"#new\" class=\"headerlink\" title=\"new\"></a>new</h2><script type=\"math/tex; mode=display\">\\mathrm{new}\\left\\{\\begin{aligned}\n    &普通\\mathrm{new}在堆上:\\left\\{\\begin{aligned} \n            &\\mathrm{Type}* \\quad ptr = \\mathrm{new} \\quad \\mathrm{Type}(初始值) \\\\\n            &\\mathrm{Type}* \\quad ptr = \\mathrm{new} \\quad \\mathrm{Type}[num]\n      \\end{aligned}\\right.\\\\\n    &定位\\mathrm{new}在指定位置:\\left\\{\\begin{aligned}                                 &\\mathrm{Type}* \\quad ptr = \\mathrm{new}(起始地址)\\quad \\mathrm{Type}(初始值) \\\\\n            &\\mathrm{Type}* \\quad ptr = \\mathrm{new}(起始地址) \\quad \\mathrm{Type}[num]\n        \\end{aligned}\\right.\n\\end{aligned}\\right.</script><h1 id=\"名称解析过程\"><a href=\"#名称解析过程\" class=\"headerlink\" title=\"名称解析过程\"></a>名称解析过程</h1><p>C++中如果编译器遇到一个名称，它会寻找这个名称代表什么。例如x<em>y, 如果x和y是变量名称，那么表达式就是乘法；如果x是类型名称，那么表达式就是声明一个指针。所以必须知道<em>*上下文</em></em>才能知道表达式意义</p>\n<h2 id=\"名称分类\"><a href=\"#名称分类\" class=\"headerlink\" title=\"名称分类\"></a>名称分类</h2><ul>\n<li>qualified name即作用域被显式指明的名称，例如::, -&gt;, .， this-&gt;count</li>\n<li>dependent name即依赖于模版参数的名称，也就是访问运算法符::左面的表达式类型依赖于模版参数(模版参数未知), std::vector<T>::iterator</li>\n</ul>\n<h2 id=\"名称查找\"><a href=\"#名称查找\" class=\"headerlink\" title=\"名称查找\"></a>名称查找</h2><ul>\n<li><script type=\"math/tex; mode=display\">ordinary\\; lookup常规查找\\left\\{\\begin{align*} & qualified\\; name & & 在指定的作用域内查找 \\\\ & 非qualified\\; name & & 作用域由内到外依次查找 \\end{align*}\\right.</script></li>\n<li>argument-dependent\\; lookup参数依赖查找，只查找<strong>非qualified name</strong>，除了由内到外查找还会将函数表达式中实参的associated namespace(关联名称空间)和associated class(关联类型)纳入到查找范围中，来查找函数名，注ADL查找会会略using</li>\n</ul>\n<h2 id=\"解析依赖型模版名称\"><a href=\"#解析依赖型模版名称\" class=\"headerlink\" title=\"解析依赖型模版名称\"></a>解析依赖型模版名称</h2><p>编译时期有两个阶段：tokening（符号化）和parsing（解析化）。</p>\n<p>模版解析六方面：</p>\n<ul>\n<li>非模版中上下文相关性</li>\n<li>依赖型类型名称</li>\n<li>依赖型模版名称</li>\n<li>using declaration中的依赖型名称</li>\n<li>ADL和显式模版实参</li>\n<li>依赖型表达式</li>\n</ul>\n<p>通常而言， 编译器会把模板名称后面的&lt;当做模板参数列表的开始，否则，&lt;就是比较运算符。但是，当引用的模板名称是 Dependent Name 时，编译器不会假定它是一个模板名称，除非显示的使用 template 关键字来指明，模板代码中常见的-&gt;template、.template、::template就应用于这种场景中。</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">template&lt;unsigned long N&gt;</span><br><span class=\"line\">void printBitset (std::bitset&lt;N&gt; const&amp; bs) </span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    std::cout &lt;&lt; bs.template to_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt;&gt;();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n"},{"title":"myFirstBlog","date":"2022-08-19T09:47:40.000Z","_content":"","source":"_posts/myFirstBlog.md","raw":"---\ntitle: myFirstBlog\ndate: 2022-08-19 17:47:40\ntags:\n---\n","slug":"myFirstBlog","published":1,"updated":"2022-08-19T09:47:40.633Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl9s8r0vd0003203k7igo98pk","content":"","site":{"data":{}},"cover":"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/cover5.JPG","excerpt":"","more":""},{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","slug":"hello-world","published":1,"date":"2022-08-19T09:34:07.987Z","updated":"2022-08-19T09:34:07.987Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl9s8r0vj0007203kg5mecsto","content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n","site":{"data":{}},"cover":"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/cover3.jpg","excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n"},{"typora-root-url":"./robotic-operating-system","title":"机器人操作系统","mathjax":true,"_content":"\n\n\n\n\n# 第一章\n\n## 介绍\nros由一组工具软件构成，是一个元操作系统，特点: \n- 消息传递接口：进程通信\n- 硬件抽象：无需关注具体硬件\n- 包管理：ros节点以包的形式组织起来，每个包由原码文件、配置文件、编译文件等构成\n- 第三方库集成：Open-CV, PCL, Open-NI等\n- 低级设备控制：控制I/O pin, 端口传数据\n- 分布式计算：将计算任务分布到几块计算节点上\n- 代码复用\n- 语言独立：可用python, c++, Lisp\n- 易测试：内建测试框架\n- 扩展\n- 免费\n\n\n## ROS概念\n$组织等级\\left\\{\\begin{aligned}\n& ros文件系统\\\\\n& ros计算图\\\\\n& ros社区\n\\end{aligned}\n\\right.$\n\nros文件系统：ros文件在硬盘中如何组织\n- package: ros包是ros框架下的个体单元，包含了：原码、第三方库、配置文件\n- package manifests: 是一个*.xml文件记录了包的全部细节，例如：名字、描述、依赖\n- message type: 是一个数据结构，定义*.msg文件中，该文件放在msg文件夹下\n- service type: 是一个请求/应答服务，定义在*.srv文件中，该文件放在srv文件夹下\n\nros计算图：计算图是一个点对点的网络，基本特征有：节点node、主进程ros master、参数服务parameter server、ros对话ros topic、 消息message、服务service、袋bag\n- 节点：表示进程\n- 主进程：串联各个节点进程\n- 参数服务：全局变量，所有进程都可访问\n- ros话题：一对多通信模式（广播）在进程间通信，发送话题称为发表publish,接收话题称为订阅subscribe\n- 服务：一对一通信模式（请求/回复）在进程间通信，发送请求端称为客户 client,回复响应端称为服务service\n- 袋：通信格式\n![计算图](image-20221020164052499.png)\n\nros社区等级\n- $分布distribution:发布版本\\left\\{\\begin{aligned}\n& ROS Melodic Morenia & &2018.05.23\\\\\n&ROS Lunar Loggerhead & &2017.05.23\\\\\n& ROS Kinetic Kame & &2016.05.23\\\\\n& ROS Indigo Igloo & &2014.07.22\n\\end{aligned}\n\\right.$\n- 仓库repositories：git\n- 维基wiki：<http://wiki.ros.org>\n- 邮件mail list：<http://lists.ros.org/mailman/listinfo/ros-users>\n- 解答answer：<https://answers.ros.org/questions/>","source":"_posts/robotic-operating-system.md","raw":"---\ntypora-root-url: ./robotic-operating-system\ntitle: 机器人操作系统\ncategories:\n- 书籍\n- 机器人\ntags:\n- 机器人\n- 系统\nmathjax: true\n---\n\n\n\n\n\n# 第一章\n\n## 介绍\nros由一组工具软件构成，是一个元操作系统，特点: \n- 消息传递接口：进程通信\n- 硬件抽象：无需关注具体硬件\n- 包管理：ros节点以包的形式组织起来，每个包由原码文件、配置文件、编译文件等构成\n- 第三方库集成：Open-CV, PCL, Open-NI等\n- 低级设备控制：控制I/O pin, 端口传数据\n- 分布式计算：将计算任务分布到几块计算节点上\n- 代码复用\n- 语言独立：可用python, c++, Lisp\n- 易测试：内建测试框架\n- 扩展\n- 免费\n\n\n## ROS概念\n$组织等级\\left\\{\\begin{aligned}\n& ros文件系统\\\\\n& ros计算图\\\\\n& ros社区\n\\end{aligned}\n\\right.$\n\nros文件系统：ros文件在硬盘中如何组织\n- package: ros包是ros框架下的个体单元，包含了：原码、第三方库、配置文件\n- package manifests: 是一个*.xml文件记录了包的全部细节，例如：名字、描述、依赖\n- message type: 是一个数据结构，定义*.msg文件中，该文件放在msg文件夹下\n- service type: 是一个请求/应答服务，定义在*.srv文件中，该文件放在srv文件夹下\n\nros计算图：计算图是一个点对点的网络，基本特征有：节点node、主进程ros master、参数服务parameter server、ros对话ros topic、 消息message、服务service、袋bag\n- 节点：表示进程\n- 主进程：串联各个节点进程\n- 参数服务：全局变量，所有进程都可访问\n- ros话题：一对多通信模式（广播）在进程间通信，发送话题称为发表publish,接收话题称为订阅subscribe\n- 服务：一对一通信模式（请求/回复）在进程间通信，发送请求端称为客户 client,回复响应端称为服务service\n- 袋：通信格式\n![计算图](image-20221020164052499.png)\n\nros社区等级\n- $分布distribution:发布版本\\left\\{\\begin{aligned}\n& ROS Melodic Morenia & &2018.05.23\\\\\n&ROS Lunar Loggerhead & &2017.05.23\\\\\n& ROS Kinetic Kame & &2016.05.23\\\\\n& ROS Indigo Igloo & &2014.07.22\n\\end{aligned}\n\\right.$\n- 仓库repositories：git\n- 维基wiki：<http://wiki.ros.org>\n- 邮件mail list：<http://lists.ros.org/mailman/listinfo/ros-users>\n- 解答answer：<https://answers.ros.org/questions/>","slug":"robotic-operating-system","published":1,"date":"2022-10-20T07:05:50.619Z","updated":"2022-10-21T07:21:55.073Z","_id":"cl9s8r0vk0008203k01xr50hi","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"第一章\"><a href=\"#第一章\" class=\"headerlink\" title=\"第一章\"></a>第一章</h1><h2 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h2><p>ros由一组工具软件构成，是一个元操作系统，特点: </p>\n<ul>\n<li>消息传递接口：进程通信</li>\n<li>硬件抽象：无需关注具体硬件</li>\n<li>包管理：ros节点以包的形式组织起来，每个包由原码文件、配置文件、编译文件等构成</li>\n<li>第三方库集成：Open-CV, PCL, Open-NI等</li>\n<li>低级设备控制：控制I/O pin, 端口传数据</li>\n<li>分布式计算：将计算任务分布到几块计算节点上</li>\n<li>代码复用</li>\n<li>语言独立：可用python, c++, Lisp</li>\n<li>易测试：内建测试框架</li>\n<li>扩展</li>\n<li>免费</li>\n</ul>\n<h2 id=\"ROS概念\"><a href=\"#ROS概念\" class=\"headerlink\" title=\"ROS概念\"></a>ROS概念</h2><p>$组织等级\\left\\{\\begin{aligned}<br>&amp; ros文件系统\\\\<br>&amp; ros计算图\\\\<br>&amp; ros社区<br>\\end{aligned}<br>\\right.$</p>\n<p>ros文件系统：ros文件在硬盘中如何组织</p>\n<ul>\n<li>package: ros包是ros框架下的个体单元，包含了：原码、第三方库、配置文件</li>\n<li>package manifests: 是一个*.xml文件记录了包的全部细节，例如：名字、描述、依赖</li>\n<li>message type: 是一个数据结构，定义*.msg文件中，该文件放在msg文件夹下</li>\n<li>service type: 是一个请求/应答服务，定义在*.srv文件中，该文件放在srv文件夹下</li>\n</ul>\n<p>ros计算图：计算图是一个点对点的网络，基本特征有：节点node、主进程ros master、参数服务parameter server、ros对话ros topic、 消息message、服务service、袋bag</p>\n<ul>\n<li>节点：表示进程</li>\n<li>主进程：串联各个节点进程</li>\n<li>参数服务：全局变量，所有进程都可访问</li>\n<li>ros话题：一对多通信模式（广播）在进程间通信，发送话题称为发表publish,接收话题称为订阅subscribe</li>\n<li>服务：一对一通信模式（请求/回复）在进程间通信，发送请求端称为客户 client,回复响应端称为服务service</li>\n<li>袋：通信格式<br><img src=\"image-20221020164052499.png\" alt=\"计算图\"></li>\n</ul>\n<p>ros社区等级</p>\n<ul>\n<li>$分布distribution:发布版本\\left\\{\\begin{aligned}<br>&amp; ROS Melodic Morenia &amp; &amp;2018.05.23\\\\<br>&amp;ROS Lunar Loggerhead &amp; &amp;2017.05.23\\\\<br>&amp; ROS Kinetic Kame &amp; &amp;2016.05.23\\\\<br>&amp; ROS Indigo Igloo &amp; &amp;2014.07.22<br>\\end{aligned}<br>\\right.$</li>\n<li>仓库repositories：git</li>\n<li>维基wiki：<a href=\"http://wiki.ros.org\">http://wiki.ros.org</a></li>\n<li>邮件mail list：<a href=\"http://lists.ros.org/mailman/listinfo/ros-users\">http://lists.ros.org/mailman/listinfo/ros-users</a></li>\n<li>解答answer：<a href=\"https://answers.ros.org/questions/\">https://answers.ros.org/questions/</a></li>\n</ul>\n","site":{"data":{}},"cover":"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/cover5.JPG","excerpt":"","more":"<h1 id=\"第一章\"><a href=\"#第一章\" class=\"headerlink\" title=\"第一章\"></a>第一章</h1><h2 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h2><p>ros由一组工具软件构成，是一个元操作系统，特点: </p>\n<ul>\n<li>消息传递接口：进程通信</li>\n<li>硬件抽象：无需关注具体硬件</li>\n<li>包管理：ros节点以包的形式组织起来，每个包由原码文件、配置文件、编译文件等构成</li>\n<li>第三方库集成：Open-CV, PCL, Open-NI等</li>\n<li>低级设备控制：控制I/O pin, 端口传数据</li>\n<li>分布式计算：将计算任务分布到几块计算节点上</li>\n<li>代码复用</li>\n<li>语言独立：可用python, c++, Lisp</li>\n<li>易测试：内建测试框架</li>\n<li>扩展</li>\n<li>免费</li>\n</ul>\n<h2 id=\"ROS概念\"><a href=\"#ROS概念\" class=\"headerlink\" title=\"ROS概念\"></a>ROS概念</h2><p>$组织等级\\left\\{\\begin{aligned}<br>&amp; ros文件系统\\\\<br>&amp; ros计算图\\\\<br>&amp; ros社区<br>\\end{aligned}<br>\\right.$</p>\n<p>ros文件系统：ros文件在硬盘中如何组织</p>\n<ul>\n<li>package: ros包是ros框架下的个体单元，包含了：原码、第三方库、配置文件</li>\n<li>package manifests: 是一个*.xml文件记录了包的全部细节，例如：名字、描述、依赖</li>\n<li>message type: 是一个数据结构，定义*.msg文件中，该文件放在msg文件夹下</li>\n<li>service type: 是一个请求/应答服务，定义在*.srv文件中，该文件放在srv文件夹下</li>\n</ul>\n<p>ros计算图：计算图是一个点对点的网络，基本特征有：节点node、主进程ros master、参数服务parameter server、ros对话ros topic、 消息message、服务service、袋bag</p>\n<ul>\n<li>节点：表示进程</li>\n<li>主进程：串联各个节点进程</li>\n<li>参数服务：全局变量，所有进程都可访问</li>\n<li>ros话题：一对多通信模式（广播）在进程间通信，发送话题称为发表publish,接收话题称为订阅subscribe</li>\n<li>服务：一对一通信模式（请求/回复）在进程间通信，发送请求端称为客户 client,回复响应端称为服务service</li>\n<li>袋：通信格式<br><img src=\"image-20221020164052499.png\" alt=\"计算图\"></li>\n</ul>\n<p>ros社区等级</p>\n<ul>\n<li>$分布distribution:发布版本\\left\\{\\begin{aligned}<br>&amp; ROS Melodic Morenia &amp; &amp;2018.05.23\\\\<br>&amp;ROS Lunar Loggerhead &amp; &amp;2017.05.23\\\\<br>&amp; ROS Kinetic Kame &amp; &amp;2016.05.23\\\\<br>&amp; ROS Indigo Igloo &amp; &amp;2014.07.22<br>\\end{aligned}<br>\\right.$</li>\n<li>仓库repositories：git</li>\n<li>维基wiki：<a href=\"http://wiki.ros.org\">http://wiki.ros.org</a></li>\n<li>邮件mail list：<a href=\"http://lists.ros.org/mailman/listinfo/ros-users\">http://lists.ros.org/mailman/listinfo/ros-users</a></li>\n<li>解答answer：<a href=\"https://answers.ros.org/questions/\">https://answers.ros.org/questions/</a></li>\n</ul>\n"},{"typora-root-url":"./test","title":"test","mathjax":true,"date":"2022-10-27T06:27:32.000Z","_content":"","source":"_posts/test.md","raw":"---\ntypora-root-url: ./test\ntitle: test\nmathjax: true\ndate: 2022-10-27 14:27:32\ncategories:\ntags:\n---\n","slug":"test","published":1,"updated":"2022-10-27T06:27:32.673Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl9s8r0vl0009203k13105fvs","content":"","site":{"data":{}},"cover":"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/cover2.jpg","excerpt":"","more":""},{"typora-root-url":"./Xnect/姿态检测公式","title":"姿态检测公式","mathjax":true,"_content":"\n\n\n\n# 姿态检测公式\n\n- 已知图像I维度是w*h, 3D姿态为$\\{P^{3D}_k\\}^K_{k=1}$,其中K是图片中人体数目，$P_k^{3D}\\in\\mathbb{R}^{3\\times J}$表示J个身体节点坐标，坐标是相对于盆骨节点（根节点）的。\n- Stage I 预测：\n  \n\n核心网络+两个分支网络，一个分支预测2D姿态坐标，另一支预测3D姿态编码，核心网络输出图像尺度为$\\frac{w}{16}\\times\\frac{h}{16}$，两个分支网络输出图像尺度为$\\frac{w}{8}\\times\\frac{h}{8}$，且3D分支要引入2D分支中的部分特征\n    \n![Untitled](Untitled.png)\n    \n    - core network: selecsls net\nSelecSLS模型在选择方式上使用短长范围跳跃连接,在模块内部用短范围跳跃连接，在各模块间使用长范围跳跃连接。中间特征数用k表示，最终输出特征数用$n_0$表示，卷积跨步用s表示。长连接中每个后续模块都和第一个模块连接这是为了提升网络中的信息流\n        \n![Untitled](Untitled%201.png)\n        \n一个等级定义为一连串模块它们会输出相同的空间分辨率的特征图\n        \n    - 2D分支：\n      \n              预测2D热点图$H=\\{H_j\\in\\mathbb{R}^{\\frac{w}{8}\\times\\frac{h}{8}}\\}_{j=1}^J$,其中J代表所有关节数,此分支使用了Part Affinity Fields(PAF)网络$F=\\{F_j\\in\\mathbb{R}^{\\frac{w}{8}\\times\\frac{h}{8}\\times2}\\}_{j=1}^J$,它表示给身体关节所有者编码，编码使用了一个单位矢量场表示关节点到运动母体的方向且矢量场宽度为相应肢体的宽度，目的是检测图片中出现的个体以及可见的关节其次是关联可见的关节到个体上。该分支输出2D身体节点位置以图像绝对坐标定位$P^{2D}_k\\in\\mathbb{Z}_+^{2\\times J}$, 共有K个个体每个个体有J个关节，该分支还输出检测置信度$c_{j,k}$,它来自于热点图的最值。\n        \n    - 3D分支：\n      \n             预测中间3D姿态编码图 $L=\\{L_j\\in\\mathbb{R}^{\\frac{w}{8}\\times\\frac{h}{8}\\times 3}\\}_{j=1}^J$，该分支利用了核心网络的输出特征和2D分支网络的部分特征。在每一可见关节上的空间位置处的编码涵盖了它的3D位姿，此位姿是相对于在运动链上直接和此节点相连的其他节点的。从3D输出图L的$(u,v)_{j,k}$位置处提取向量$\\vec{l}_{j,k}$矢量形状$1\\times1\\times3J$,它包含的是身体全部节点的3D姿态，每个节点的相对于母体（根节点）的坐标位于不同的通道\n        \n        ![Untitled](Untitled%202.png)\n        \n        $\\vec{l}_{j,k}$只编码和本节点j在骨架上直接相连的节点，形式顺序是父节点-本节点和本节点-子节点，没有本节点-父节点形式顺序，且标记的坐标都是相对于父节点的，这样不用把所有节点都标记。\n        \n    - 输出到Stage II的特征：\n      \n              每一个个体k的可见关节的2D关节坐标 $P_k^{2D}$,关节置信度$\\{c_{j,k}\\}^J_{j=1}$,关节3D坐标编码$\\{l_{j,k}\\}^J_{j=1}$\n        \n    - Stage I分阶段训练，core network和2D分支在单人数据集中训练，然后再在多人数据集中训练；\n\n- Stage II预测：\n    - 全连接网络对每一个人的每一个可见节点预测相对于根节点的3D坐标$\\{P_k^{3D}\\}^K_{k=1}$\n    - 先把$P_k^{2D}$坐标转成相对于脖子节点的坐标\n    - 连接关节相对脖子的坐标$(u,v)_{j,k}$，关节检测置信度 $c_{j,k}$和关节3D坐标编码$l_{j,k}$为$S_k\\in\\mathbb{R}^{J\\times(3+3\\cdot J)}$,如果关节不可见就只连接成0矢量\n    \n    ![Untitled](Untitled%203.png)\n    \n    - Stage II是一个5层的全连接网络，它将$S_k$转成相对于根节点的3D坐标$P_k^{3D}$\n      \n        ![Untitled](Untitled%204.png)\n        \n    - Stage II 训练：\n      \n             Stage II 网络在MuCo-3DHP非截取的帧中训练,首先用Stage I网络在这个数据集里运行得到2D坐标和3D坐标编码，然后对每一个个体使用真实的相对于根节点的3D坐标$\\{(X_j,Y_j,Z_j)\\}^J_{j=1}$作为标签来训练Stage II, 使用smooth-L1 loss减轻遮挡产生的预测和标签的差异性\n    \n- Stage III:\n  \n    经过Stage I和Stage II得到了一个帧中一个人的相对于根节点的3D坐标，但是没有估计人的大小和距离相机的距离，同一人的身份没有在所有帧中进行追踪\n    \n    - 为了在不同帧中追踪识别同一个人坐标\n      \n               现将先前的变量加一个时间索引例如$\\{P_i[t]\\}^{K[t]}_{i=1}$, 我们建模和持续追踪人的样子通过半身区域的HSV颜色直方图，对每个人切割颜色和饱和度通道成30份，在包含人体关节的边界框中定义类别概率为人体模样$A_{i}[t]$，缺点是颜色相似的个体问题。为了在不同帧中匹配对象更有鲁棒性匹配当前探测的人体i上先前已知的人体k要利用3个相似性，分别是人体模样相似性$S_{i,k}^A=(A_i[t]-A_k[t-1])^2$，2D坐标相似性$S_{i,k}^{P^{2D}}=(P^{2D}_i[t]-P^{2D}_k[t-1])^2$和3D坐标相似性$S_{i,k}^{P^{3D}}=(P^{3D}_{i}[t]-P^{3D}_{k}[t-1])$,通过设置相似性阈值检测人体被遮挡、人离开图片、人进入图片。\n        \n    - 相对骨头长和绝对高度\n      \n                骨头长是一个尺度不变量，可以用$P_k^{3D}$计算，为了增加鲁棒性对连接节点的骨头长度$b_k$在10帧中做正态归一化处理。\n                \n                 将相对根节点2D坐标转成3D绝对坐标以cm为单位，这一任务很难只能凭借单目镜头猜测。先决定人在相机中的相对坐标，人的高度是从地面到一个交叉点的距离，该交叉点是放在足部位置处的虚拟广告牌和通过头部位置的视线的交点。\n    \n    \n    - Kinematic Skeleton Fitting\n      \n                 得到2D和3D坐标，优化骨骼坐标$\\{\\theta_k[t]\\}^{K[t]}_{k=1}$, $K[t]$是场景中t时刻所有的人数，$\\theta_k[t]\\in\\mathbb{R}^D$从固定骨架的关节角度加上全局根节点坐标描述人的姿态坐标, ，D=29是一个骨骼的自由度数。估计$\\theta_k[t]$使拟合能量最小化：\n        \n        $$\n        \\varepsilon(\\theta_1[t],...,\\theta_K[t])=w_{3D}E_{3D}+w_{2D}E_{2D}+w_{lim}E_{lim}+w_{temp}E_{temp}+w_{depth}E_{depth}\n        $$\n        \n        权重$w_{3D}=9e-1, w_{2D}=1e-5, w_{3D}=5e-1, w_{temp}=1e-7, w_{depth}=8e-6$；\n        \n        3D逆运动误差：\n        \n        误差：预测的相对根节点的3D坐标$P^{3D}_k[t]$和骨骼$\\mathscr{P}(\\theta_k[t],b_k)$坐标，它由每个人k的每个关节j和骨骼长度$b_k$正向运动构成\n        \n        $$\n        E_{3D}=\\sum_{k=1}^K\\sum_{j=1}^{J^{3D}}||\\mathscr{P}(\\theta_k[t],b_k)_j-P^{3D}_{k,j}[t]||_2^2\n        $$\n        \n        2D从投影误差：\n        \n        误差：预测的2D关节坐标$P^{2D}_k[t]$和骨骼投影 $\\mathcal{P}(\\theta_k[t],b_k)_j$坐标\n        \n        $$\n        E_{2D}=\\sum_{k=1}^K\\sum_{j=1}^{J^{2D}}w_j^{2D}c_{j,k}||\\Pi(h_k\\mathcal{P}(\\theta_k[t],b_k))_j-P^{2D}_{k,j}[t]||_2^2\n        $$\n        \n        这里$c$是2D预测的置信度，$w_j^{2D}$是每个节点的相对权重, 下肢关节权重相对于躯干关节（屁股、脖子、肩膀）是1.7，同理肘是1.5，手腕是2.0，$\\Pi$是相机投影矩阵，注意$\\mathcal{P}(\\theta_k[t],b_k))_j$输出单位高度，缩放因子$h_k$把它影射到测量坐标\n        \n        关节角度限制误差：\n        \n          误差：关节限制规范给关节旋转角度范围增加限制，这是基于解剖学的关节角度范围\n        \n        $$\n        E_{lim}=\\sum_{k=1}^K\\sum_{j=7}^{D=29}\\left\\{\\begin{aligned}&(\\theta_j^{min}-\\theta_{k,j}[t])^2 & &,if \\quad\\theta_{k,j}[t]<\\theta_j^{min}\\\\ &(\\theta_{k,j}[t]-\\theta_j^{max})^2  & &,if\\quad \\theta_{k,j}[t]>\\theta_j^{max}\\\\ &0 & &,otherwise\\end{aligned}\\right.\n        $$\n        \n        骨架自由度从7开始是因为对全局位置和旋转参数没有限制，神经网络的估计的关节位置对于关节角度没有限制\n        \n        时间平滑误差：\n        \n        误差：网络基于每帧估计的坐标会存在时间上的抖动\n        \n        $$\n        E_{temp}(\\Theta)=\\sum_{k=1}^K||\\nabla\\theta_k[t-1]-\\nabla\\theta_k[t]||^2_2\n        $$\n        \n           使用平滑项更多惩罚深度方向有更少约束的变量，$E_{depth}=||\\theta_{k,2}[t]_z-\\theta_{k,2}[t-1]_z||$,这里$\\theta_{k,2}$是自由度，它驱动了根节点的z坐标\n        \n        逆运动追踪初始化：\n        \n           对于新人追踪时的头一帧，局部关节角度拟合3D预测坐标且只考虑 $E_{3D}$和$E_{lim}$误差，之后这些关节角度锁死同时最小化$E_{2D}$,这是为了对骨骼的全局位移和旋转做最佳拟合，随后完整的能量公式$\\varepsilon(\\theta_1[t],...,\\theta_K[t])$被使用。","source":"_posts/Xnect/姿态检测公式.md","raw":"---\ntypora-root-url: ./Xnect/姿态检测公式\ntitle: 姿态检测公式\ncategories:\n- 文献\n- 人体姿态\ntags:\n- 神经网络\n- 机器学习\nmathjax: true\n---\n\n\n\n\n# 姿态检测公式\n\n- 已知图像I维度是w*h, 3D姿态为$\\{P^{3D}_k\\}^K_{k=1}$,其中K是图片中人体数目，$P_k^{3D}\\in\\mathbb{R}^{3\\times J}$表示J个身体节点坐标，坐标是相对于盆骨节点（根节点）的。\n- Stage I 预测：\n  \n\n核心网络+两个分支网络，一个分支预测2D姿态坐标，另一支预测3D姿态编码，核心网络输出图像尺度为$\\frac{w}{16}\\times\\frac{h}{16}$，两个分支网络输出图像尺度为$\\frac{w}{8}\\times\\frac{h}{8}$，且3D分支要引入2D分支中的部分特征\n    \n![Untitled](Untitled.png)\n    \n    - core network: selecsls net\nSelecSLS模型在选择方式上使用短长范围跳跃连接,在模块内部用短范围跳跃连接，在各模块间使用长范围跳跃连接。中间特征数用k表示，最终输出特征数用$n_0$表示，卷积跨步用s表示。长连接中每个后续模块都和第一个模块连接这是为了提升网络中的信息流\n        \n![Untitled](Untitled%201.png)\n        \n一个等级定义为一连串模块它们会输出相同的空间分辨率的特征图\n        \n    - 2D分支：\n      \n              预测2D热点图$H=\\{H_j\\in\\mathbb{R}^{\\frac{w}{8}\\times\\frac{h}{8}}\\}_{j=1}^J$,其中J代表所有关节数,此分支使用了Part Affinity Fields(PAF)网络$F=\\{F_j\\in\\mathbb{R}^{\\frac{w}{8}\\times\\frac{h}{8}\\times2}\\}_{j=1}^J$,它表示给身体关节所有者编码，编码使用了一个单位矢量场表示关节点到运动母体的方向且矢量场宽度为相应肢体的宽度，目的是检测图片中出现的个体以及可见的关节其次是关联可见的关节到个体上。该分支输出2D身体节点位置以图像绝对坐标定位$P^{2D}_k\\in\\mathbb{Z}_+^{2\\times J}$, 共有K个个体每个个体有J个关节，该分支还输出检测置信度$c_{j,k}$,它来自于热点图的最值。\n        \n    - 3D分支：\n      \n             预测中间3D姿态编码图 $L=\\{L_j\\in\\mathbb{R}^{\\frac{w}{8}\\times\\frac{h}{8}\\times 3}\\}_{j=1}^J$，该分支利用了核心网络的输出特征和2D分支网络的部分特征。在每一可见关节上的空间位置处的编码涵盖了它的3D位姿，此位姿是相对于在运动链上直接和此节点相连的其他节点的。从3D输出图L的$(u,v)_{j,k}$位置处提取向量$\\vec{l}_{j,k}$矢量形状$1\\times1\\times3J$,它包含的是身体全部节点的3D姿态，每个节点的相对于母体（根节点）的坐标位于不同的通道\n        \n        ![Untitled](Untitled%202.png)\n        \n        $\\vec{l}_{j,k}$只编码和本节点j在骨架上直接相连的节点，形式顺序是父节点-本节点和本节点-子节点，没有本节点-父节点形式顺序，且标记的坐标都是相对于父节点的，这样不用把所有节点都标记。\n        \n    - 输出到Stage II的特征：\n      \n              每一个个体k的可见关节的2D关节坐标 $P_k^{2D}$,关节置信度$\\{c_{j,k}\\}^J_{j=1}$,关节3D坐标编码$\\{l_{j,k}\\}^J_{j=1}$\n        \n    - Stage I分阶段训练，core network和2D分支在单人数据集中训练，然后再在多人数据集中训练；\n\n- Stage II预测：\n    - 全连接网络对每一个人的每一个可见节点预测相对于根节点的3D坐标$\\{P_k^{3D}\\}^K_{k=1}$\n    - 先把$P_k^{2D}$坐标转成相对于脖子节点的坐标\n    - 连接关节相对脖子的坐标$(u,v)_{j,k}$，关节检测置信度 $c_{j,k}$和关节3D坐标编码$l_{j,k}$为$S_k\\in\\mathbb{R}^{J\\times(3+3\\cdot J)}$,如果关节不可见就只连接成0矢量\n    \n    ![Untitled](Untitled%203.png)\n    \n    - Stage II是一个5层的全连接网络，它将$S_k$转成相对于根节点的3D坐标$P_k^{3D}$\n      \n        ![Untitled](Untitled%204.png)\n        \n    - Stage II 训练：\n      \n             Stage II 网络在MuCo-3DHP非截取的帧中训练,首先用Stage I网络在这个数据集里运行得到2D坐标和3D坐标编码，然后对每一个个体使用真实的相对于根节点的3D坐标$\\{(X_j,Y_j,Z_j)\\}^J_{j=1}$作为标签来训练Stage II, 使用smooth-L1 loss减轻遮挡产生的预测和标签的差异性\n    \n- Stage III:\n  \n    经过Stage I和Stage II得到了一个帧中一个人的相对于根节点的3D坐标，但是没有估计人的大小和距离相机的距离，同一人的身份没有在所有帧中进行追踪\n    \n    - 为了在不同帧中追踪识别同一个人坐标\n      \n               现将先前的变量加一个时间索引例如$\\{P_i[t]\\}^{K[t]}_{i=1}$, 我们建模和持续追踪人的样子通过半身区域的HSV颜色直方图，对每个人切割颜色和饱和度通道成30份，在包含人体关节的边界框中定义类别概率为人体模样$A_{i}[t]$，缺点是颜色相似的个体问题。为了在不同帧中匹配对象更有鲁棒性匹配当前探测的人体i上先前已知的人体k要利用3个相似性，分别是人体模样相似性$S_{i,k}^A=(A_i[t]-A_k[t-1])^2$，2D坐标相似性$S_{i,k}^{P^{2D}}=(P^{2D}_i[t]-P^{2D}_k[t-1])^2$和3D坐标相似性$S_{i,k}^{P^{3D}}=(P^{3D}_{i}[t]-P^{3D}_{k}[t-1])$,通过设置相似性阈值检测人体被遮挡、人离开图片、人进入图片。\n        \n    - 相对骨头长和绝对高度\n      \n                骨头长是一个尺度不变量，可以用$P_k^{3D}$计算，为了增加鲁棒性对连接节点的骨头长度$b_k$在10帧中做正态归一化处理。\n                \n                 将相对根节点2D坐标转成3D绝对坐标以cm为单位，这一任务很难只能凭借单目镜头猜测。先决定人在相机中的相对坐标，人的高度是从地面到一个交叉点的距离，该交叉点是放在足部位置处的虚拟广告牌和通过头部位置的视线的交点。\n    \n    \n    - Kinematic Skeleton Fitting\n      \n                 得到2D和3D坐标，优化骨骼坐标$\\{\\theta_k[t]\\}^{K[t]}_{k=1}$, $K[t]$是场景中t时刻所有的人数，$\\theta_k[t]\\in\\mathbb{R}^D$从固定骨架的关节角度加上全局根节点坐标描述人的姿态坐标, ，D=29是一个骨骼的自由度数。估计$\\theta_k[t]$使拟合能量最小化：\n        \n        $$\n        \\varepsilon(\\theta_1[t],...,\\theta_K[t])=w_{3D}E_{3D}+w_{2D}E_{2D}+w_{lim}E_{lim}+w_{temp}E_{temp}+w_{depth}E_{depth}\n        $$\n        \n        权重$w_{3D}=9e-1, w_{2D}=1e-5, w_{3D}=5e-1, w_{temp}=1e-7, w_{depth}=8e-6$；\n        \n        3D逆运动误差：\n        \n        误差：预测的相对根节点的3D坐标$P^{3D}_k[t]$和骨骼$\\mathscr{P}(\\theta_k[t],b_k)$坐标，它由每个人k的每个关节j和骨骼长度$b_k$正向运动构成\n        \n        $$\n        E_{3D}=\\sum_{k=1}^K\\sum_{j=1}^{J^{3D}}||\\mathscr{P}(\\theta_k[t],b_k)_j-P^{3D}_{k,j}[t]||_2^2\n        $$\n        \n        2D从投影误差：\n        \n        误差：预测的2D关节坐标$P^{2D}_k[t]$和骨骼投影 $\\mathcal{P}(\\theta_k[t],b_k)_j$坐标\n        \n        $$\n        E_{2D}=\\sum_{k=1}^K\\sum_{j=1}^{J^{2D}}w_j^{2D}c_{j,k}||\\Pi(h_k\\mathcal{P}(\\theta_k[t],b_k))_j-P^{2D}_{k,j}[t]||_2^2\n        $$\n        \n        这里$c$是2D预测的置信度，$w_j^{2D}$是每个节点的相对权重, 下肢关节权重相对于躯干关节（屁股、脖子、肩膀）是1.7，同理肘是1.5，手腕是2.0，$\\Pi$是相机投影矩阵，注意$\\mathcal{P}(\\theta_k[t],b_k))_j$输出单位高度，缩放因子$h_k$把它影射到测量坐标\n        \n        关节角度限制误差：\n        \n          误差：关节限制规范给关节旋转角度范围增加限制，这是基于解剖学的关节角度范围\n        \n        $$\n        E_{lim}=\\sum_{k=1}^K\\sum_{j=7}^{D=29}\\left\\{\\begin{aligned}&(\\theta_j^{min}-\\theta_{k,j}[t])^2 & &,if \\quad\\theta_{k,j}[t]<\\theta_j^{min}\\\\ &(\\theta_{k,j}[t]-\\theta_j^{max})^2  & &,if\\quad \\theta_{k,j}[t]>\\theta_j^{max}\\\\ &0 & &,otherwise\\end{aligned}\\right.\n        $$\n        \n        骨架自由度从7开始是因为对全局位置和旋转参数没有限制，神经网络的估计的关节位置对于关节角度没有限制\n        \n        时间平滑误差：\n        \n        误差：网络基于每帧估计的坐标会存在时间上的抖动\n        \n        $$\n        E_{temp}(\\Theta)=\\sum_{k=1}^K||\\nabla\\theta_k[t-1]-\\nabla\\theta_k[t]||^2_2\n        $$\n        \n           使用平滑项更多惩罚深度方向有更少约束的变量，$E_{depth}=||\\theta_{k,2}[t]_z-\\theta_{k,2}[t-1]_z||$,这里$\\theta_{k,2}$是自由度，它驱动了根节点的z坐标\n        \n        逆运动追踪初始化：\n        \n           对于新人追踪时的头一帧，局部关节角度拟合3D预测坐标且只考虑 $E_{3D}$和$E_{lim}$误差，之后这些关节角度锁死同时最小化$E_{2D}$,这是为了对骨骼的全局位移和旋转做最佳拟合，随后完整的能量公式$\\varepsilon(\\theta_1[t],...,\\theta_K[t])$被使用。","slug":"Xnect/姿态检测公式","published":1,"date":"2022-08-19T01:42:36.000Z","updated":"2022-10-21T15:16:26.339Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cl9s8r0vq000q203k2je24i63","content":"<h1 id=\"姿态检测公式\"><a href=\"#姿态检测公式\" class=\"headerlink\" title=\"姿态检测公式\"></a>姿态检测公式</h1><ul>\n<li>已知图像I维度是w*h, 3D姿态为$\\{P^{3D}_k\\}^K_{k=1}$,其中K是图片中人体数目，$P_k^{3D}\\in\\mathbb{R}^{3\\times J}$表示J个身体节点坐标，坐标是相对于盆骨节点（根节点）的。</li>\n<li>Stage I 预测：</li>\n</ul>\n<p>核心网络+两个分支网络，一个分支预测2D姿态坐标，另一支预测3D姿态编码，核心网络输出图像尺度为$\\frac{w}{16}\\times\\frac{h}{16}$，两个分支网络输出图像尺度为$\\frac{w}{8}\\times\\frac{h}{8}$，且3D分支要引入2D分支中的部分特征</p>\n<p><img src=\"Untitled.png\" alt=\"Untitled\"></p>\n<pre><code>- core network: selecsls net\n</code></pre><p>SelecSLS模型在选择方式上使用短长范围跳跃连接,在模块内部用短范围跳跃连接，在各模块间使用长范围跳跃连接。中间特征数用k表示，最终输出特征数用$n_0$表示，卷积跨步用s表示。长连接中每个后续模块都和第一个模块连接这是为了提升网络中的信息流</p>\n<p><img src=\"Untitled%201.png\" alt=\"Untitled\"></p>\n<p>一个等级定义为一连串模块它们会输出相同的空间分辨率的特征图</p>\n<pre><code>- 2D分支：\n\n          预测2D热点图$H=\\&#123;H_j\\in\\mathbb&#123;R&#125;^&#123;\\frac&#123;w&#125;&#123;8&#125;\\times\\frac&#123;h&#125;&#123;8&#125;&#125;\\&#125;_&#123;j=1&#125;^J$,其中J代表所有关节数,此分支使用了Part Affinity Fields(PAF)网络$F=\\&#123;F_j\\in\\mathbb&#123;R&#125;^&#123;\\frac&#123;w&#125;&#123;8&#125;\\times\\frac&#123;h&#125;&#123;8&#125;\\times2&#125;\\&#125;_&#123;j=1&#125;^J$,它表示给身体关节所有者编码，编码使用了一个单位矢量场表示关节点到运动母体的方向且矢量场宽度为相应肢体的宽度，目的是检测图片中出现的个体以及可见的关节其次是关联可见的关节到个体上。该分支输出2D身体节点位置以图像绝对坐标定位$P^&#123;2D&#125;_k\\in\\mathbb&#123;Z&#125;_+^&#123;2\\times J&#125;$, 共有K个个体每个个体有J个关节，该分支还输出检测置信度$c_&#123;j,k&#125;$,它来自于热点图的最值。\n\n- 3D分支：\n\n         预测中间3D姿态编码图 $L=\\&#123;L_j\\in\\mathbb&#123;R&#125;^&#123;\\frac&#123;w&#125;&#123;8&#125;\\times\\frac&#123;h&#125;&#123;8&#125;\\times 3&#125;\\&#125;_&#123;j=1&#125;^J$，该分支利用了核心网络的输出特征和2D分支网络的部分特征。在每一可见关节上的空间位置处的编码涵盖了它的3D位姿，此位姿是相对于在运动链上直接和此节点相连的其他节点的。从3D输出图L的$(u,v)_&#123;j,k&#125;$位置处提取向量$\\vec&#123;l&#125;_&#123;j,k&#125;$矢量形状$1\\times1\\times3J$,它包含的是身体全部节点的3D姿态，每个节点的相对于母体（根节点）的坐标位于不同的通道\n\n    ![Untitled](Untitled%202.png)\n\n    $\\vec&#123;l&#125;_&#123;j,k&#125;$只编码和本节点j在骨架上直接相连的节点，形式顺序是父节点-本节点和本节点-子节点，没有本节点-父节点形式顺序，且标记的坐标都是相对于父节点的，这样不用把所有节点都标记。\n\n- 输出到Stage II的特征：\n\n          每一个个体k的可见关节的2D关节坐标 $P_k^&#123;2D&#125;$,关节置信度$\\&#123;c_&#123;j,k&#125;\\&#125;^J_&#123;j=1&#125;$,关节3D坐标编码$\\&#123;l_&#123;j,k&#125;\\&#125;^J_&#123;j=1&#125;$\n\n- Stage I分阶段训练，core network和2D分支在单人数据集中训练，然后再在多人数据集中训练；\n</code></pre><ul>\n<li><p>Stage II预测：</p>\n<ul>\n<li>全连接网络对每一个人的每一个可见节点预测相对于根节点的3D坐标$\\{P_k^{3D}\\}^K_{k=1}$</li>\n<li>先把$P_k^{2D}$坐标转成相对于脖子节点的坐标</li>\n<li><p>连接关节相对脖子的坐标$(u,v)_{j,k}$，关节检测置信度 $c_{j,k}$和关节3D坐标编码$l_{j,k}$为$S_k\\in\\mathbb{R}^{J\\times(3+3\\cdot J)}$,如果关节不可见就只连接成0矢量</p>\n<p><img src=\"Untitled%203.png\" alt=\"Untitled\"></p>\n</li>\n<li><p>Stage II是一个5层的全连接网络，它将$S_k$转成相对于根节点的3D坐标$P_k^{3D}$</p>\n<p>  <img src=\"Untitled%204.png\" alt=\"Untitled\"></p>\n</li>\n<li><p>Stage II 训练：</p>\n<pre><code>   Stage II 网络在MuCo-3DHP非截取的帧中训练,首先用Stage I网络在这个数据集里运行得到2D坐标和3D坐标编码，然后对每一个个体使用真实的相对于根节点的3D坐标$\\&#123;(X_j,Y_j,Z_j)\\&#125;^J_&#123;j=1&#125;$作为标签来训练Stage II, 使用smooth-L1 loss减轻遮挡产生的预测和标签的差异性\n</code></pre></li>\n</ul>\n</li>\n<li><p>Stage III:</p>\n<p>  经过Stage I和Stage II得到了一个帧中一个人的相对于根节点的3D坐标，但是没有估计人的大小和距离相机的距离，同一人的身份没有在所有帧中进行追踪</p>\n<ul>\n<li><p>为了在不同帧中追踪识别同一个人坐标</p>\n<pre><code>     现将先前的变量加一个时间索引例如$\\&#123;P_i[t]\\&#125;^&#123;K[t]&#125;_&#123;i=1&#125;$, 我们建模和持续追踪人的样子通过半身区域的HSV颜色直方图，对每个人切割颜色和饱和度通道成30份，在包含人体关节的边界框中定义类别概率为人体模样$A_&#123;i&#125;[t]$，缺点是颜色相似的个体问题。为了在不同帧中匹配对象更有鲁棒性匹配当前探测的人体i上先前已知的人体k要利用3个相似性，分别是人体模样相似性$S_&#123;i,k&#125;^A=(A_i[t]-A_k[t-1])^2$，2D坐标相似性$S_&#123;i,k&#125;^&#123;P^&#123;2D&#125;&#125;=(P^&#123;2D&#125;_i[t]-P^&#123;2D&#125;_k[t-1])^2$和3D坐标相似性$S_&#123;i,k&#125;^&#123;P^&#123;3D&#125;&#125;=(P^&#123;3D&#125;_&#123;i&#125;[t]-P^&#123;3D&#125;_&#123;k&#125;[t-1])$,通过设置相似性阈值检测人体被遮挡、人离开图片、人进入图片。\n</code></pre></li>\n<li><p>相对骨头长和绝对高度</p>\n<pre><code>      骨头长是一个尺度不变量，可以用$P_k^&#123;3D&#125;$计算，为了增加鲁棒性对连接节点的骨头长度$b_k$在10帧中做正态归一化处理。\n\n       将相对根节点2D坐标转成3D绝对坐标以cm为单位，这一任务很难只能凭借单目镜头猜测。先决定人在相机中的相对坐标，人的高度是从地面到一个交叉点的距离，该交叉点是放在足部位置处的虚拟广告牌和通过头部位置的视线的交点。\n</code></pre></li>\n</ul>\n</li>\n</ul>\n<pre><code>- Kinematic Skeleton Fitting\n\n             得到2D和3D坐标，优化骨骼坐标$\\&#123;\\theta_k[t]\\&#125;^&#123;K[t]&#125;_&#123;k=1&#125;$, $K[t]$是场景中t时刻所有的人数，$\\theta_k[t]\\in\\mathbb&#123;R&#125;^D$从固定骨架的关节角度加上全局根节点坐标描述人的姿态坐标, ，D=29是一个骨骼的自由度数。估计$\\theta_k[t]$使拟合能量最小化：\n\n    $$\n    \\varepsilon(\\theta_1[t],...,\\theta_K[t])=w_&#123;3D&#125;E_&#123;3D&#125;+w_&#123;2D&#125;E_&#123;2D&#125;+w_&#123;lim&#125;E_&#123;lim&#125;+w_&#123;temp&#125;E_&#123;temp&#125;+w_&#123;depth&#125;E_&#123;depth&#125;\n    $$\n\n    权重$w_&#123;3D&#125;=9e-1, w_&#123;2D&#125;=1e-5, w_&#123;3D&#125;=5e-1, w_&#123;temp&#125;=1e-7, w_&#123;depth&#125;=8e-6$；\n\n    3D逆运动误差：\n\n    误差：预测的相对根节点的3D坐标$P^&#123;3D&#125;_k[t]$和骨骼$\\mathscr&#123;P&#125;(\\theta_k[t],b_k)$坐标，它由每个人k的每个关节j和骨骼长度$b_k$正向运动构成\n\n    $$\n    E_&#123;3D&#125;=\\sum_&#123;k=1&#125;^K\\sum_&#123;j=1&#125;^&#123;J^&#123;3D&#125;&#125;||\\mathscr&#123;P&#125;(\\theta_k[t],b_k)_j-P^&#123;3D&#125;_&#123;k,j&#125;[t]||_2^2\n    $$\n\n    2D从投影误差：\n\n    误差：预测的2D关节坐标$P^&#123;2D&#125;_k[t]$和骨骼投影 $\\mathcal&#123;P&#125;(\\theta_k[t],b_k)_j$坐标\n\n    $$\n    E_&#123;2D&#125;=\\sum_&#123;k=1&#125;^K\\sum_&#123;j=1&#125;^&#123;J^&#123;2D&#125;&#125;w_j^&#123;2D&#125;c_&#123;j,k&#125;||\\Pi(h_k\\mathcal&#123;P&#125;(\\theta_k[t],b_k))_j-P^&#123;2D&#125;_&#123;k,j&#125;[t]||_2^2\n    $$\n\n    这里$c$是2D预测的置信度，$w_j^&#123;2D&#125;$是每个节点的相对权重, 下肢关节权重相对于躯干关节（屁股、脖子、肩膀）是1.7，同理肘是1.5，手腕是2.0，$\\Pi$是相机投影矩阵，注意$\\mathcal&#123;P&#125;(\\theta_k[t],b_k))_j$输出单位高度，缩放因子$h_k$把它影射到测量坐标\n\n    关节角度限制误差：\n\n      误差：关节限制规范给关节旋转角度范围增加限制，这是基于解剖学的关节角度范围\n\n    $$\n    E_&#123;lim&#125;=\\sum_&#123;k=1&#125;^K\\sum_&#123;j=7&#125;^&#123;D=29&#125;\\left\\&#123;\\begin&#123;aligned&#125;&amp;(\\theta_j^&#123;min&#125;-\\theta_&#123;k,j&#125;[t])^2 &amp; &amp;,if \\quad\\theta_&#123;k,j&#125;[t]&lt;\\theta_j^&#123;min&#125;\\\\ &amp;(\\theta_&#123;k,j&#125;[t]-\\theta_j^&#123;max&#125;)^2  &amp; &amp;,if\\quad \\theta_&#123;k,j&#125;[t]&gt;\\theta_j^&#123;max&#125;\\\\ &amp;0 &amp; &amp;,otherwise\\end&#123;aligned&#125;\\right.\n    $$\n\n    骨架自由度从7开始是因为对全局位置和旋转参数没有限制，神经网络的估计的关节位置对于关节角度没有限制\n\n    时间平滑误差：\n\n    误差：网络基于每帧估计的坐标会存在时间上的抖动\n\n    $$\n    E_&#123;temp&#125;(\\Theta)=\\sum_&#123;k=1&#125;^K||\\nabla\\theta_k[t-1]-\\nabla\\theta_k[t]||^2_2\n    $$\n\n       使用平滑项更多惩罚深度方向有更少约束的变量，$E_&#123;depth&#125;=||\\theta_&#123;k,2&#125;[t]_z-\\theta_&#123;k,2&#125;[t-1]_z||$,这里$\\theta_&#123;k,2&#125;$是自由度，它驱动了根节点的z坐标\n\n    逆运动追踪初始化：\n\n       对于新人追踪时的头一帧，局部关节角度拟合3D预测坐标且只考虑 $E_&#123;3D&#125;$和$E_&#123;lim&#125;$误差，之后这些关节角度锁死同时最小化$E_&#123;2D&#125;$,这是为了对骨骼的全局位移和旋转做最佳拟合，随后完整的能量公式$\\varepsilon(\\theta_1[t],...,\\theta_K[t])$被使用。\n</code></pre>","site":{"data":{}},"cover":"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/cover4.jpg","excerpt":"","more":"<h1 id=\"姿态检测公式\"><a href=\"#姿态检测公式\" class=\"headerlink\" title=\"姿态检测公式\"></a>姿态检测公式</h1><ul>\n<li>已知图像I维度是w*h, 3D姿态为$\\{P^{3D}_k\\}^K_{k=1}$,其中K是图片中人体数目，$P_k^{3D}\\in\\mathbb{R}^{3\\times J}$表示J个身体节点坐标，坐标是相对于盆骨节点（根节点）的。</li>\n<li>Stage I 预测：</li>\n</ul>\n<p>核心网络+两个分支网络，一个分支预测2D姿态坐标，另一支预测3D姿态编码，核心网络输出图像尺度为$\\frac{w}{16}\\times\\frac{h}{16}$，两个分支网络输出图像尺度为$\\frac{w}{8}\\times\\frac{h}{8}$，且3D分支要引入2D分支中的部分特征</p>\n<p><img src=\"Untitled.png\" alt=\"Untitled\"></p>\n<pre><code>- core network: selecsls net\n</code></pre><p>SelecSLS模型在选择方式上使用短长范围跳跃连接,在模块内部用短范围跳跃连接，在各模块间使用长范围跳跃连接。中间特征数用k表示，最终输出特征数用$n_0$表示，卷积跨步用s表示。长连接中每个后续模块都和第一个模块连接这是为了提升网络中的信息流</p>\n<p><img src=\"Untitled%201.png\" alt=\"Untitled\"></p>\n<p>一个等级定义为一连串模块它们会输出相同的空间分辨率的特征图</p>\n<pre><code>- 2D分支：\n\n          预测2D热点图$H=\\&#123;H_j\\in\\mathbb&#123;R&#125;^&#123;\\frac&#123;w&#125;&#123;8&#125;\\times\\frac&#123;h&#125;&#123;8&#125;&#125;\\&#125;_&#123;j=1&#125;^J$,其中J代表所有关节数,此分支使用了Part Affinity Fields(PAF)网络$F=\\&#123;F_j\\in\\mathbb&#123;R&#125;^&#123;\\frac&#123;w&#125;&#123;8&#125;\\times\\frac&#123;h&#125;&#123;8&#125;\\times2&#125;\\&#125;_&#123;j=1&#125;^J$,它表示给身体关节所有者编码，编码使用了一个单位矢量场表示关节点到运动母体的方向且矢量场宽度为相应肢体的宽度，目的是检测图片中出现的个体以及可见的关节其次是关联可见的关节到个体上。该分支输出2D身体节点位置以图像绝对坐标定位$P^&#123;2D&#125;_k\\in\\mathbb&#123;Z&#125;_+^&#123;2\\times J&#125;$, 共有K个个体每个个体有J个关节，该分支还输出检测置信度$c_&#123;j,k&#125;$,它来自于热点图的最值。\n\n- 3D分支：\n\n         预测中间3D姿态编码图 $L=\\&#123;L_j\\in\\mathbb&#123;R&#125;^&#123;\\frac&#123;w&#125;&#123;8&#125;\\times\\frac&#123;h&#125;&#123;8&#125;\\times 3&#125;\\&#125;_&#123;j=1&#125;^J$，该分支利用了核心网络的输出特征和2D分支网络的部分特征。在每一可见关节上的空间位置处的编码涵盖了它的3D位姿，此位姿是相对于在运动链上直接和此节点相连的其他节点的。从3D输出图L的$(u,v)_&#123;j,k&#125;$位置处提取向量$\\vec&#123;l&#125;_&#123;j,k&#125;$矢量形状$1\\times1\\times3J$,它包含的是身体全部节点的3D姿态，每个节点的相对于母体（根节点）的坐标位于不同的通道\n\n    ![Untitled](Untitled%202.png)\n\n    $\\vec&#123;l&#125;_&#123;j,k&#125;$只编码和本节点j在骨架上直接相连的节点，形式顺序是父节点-本节点和本节点-子节点，没有本节点-父节点形式顺序，且标记的坐标都是相对于父节点的，这样不用把所有节点都标记。\n\n- 输出到Stage II的特征：\n\n          每一个个体k的可见关节的2D关节坐标 $P_k^&#123;2D&#125;$,关节置信度$\\&#123;c_&#123;j,k&#125;\\&#125;^J_&#123;j=1&#125;$,关节3D坐标编码$\\&#123;l_&#123;j,k&#125;\\&#125;^J_&#123;j=1&#125;$\n\n- Stage I分阶段训练，core network和2D分支在单人数据集中训练，然后再在多人数据集中训练；\n</code></pre><ul>\n<li><p>Stage II预测：</p>\n<ul>\n<li>全连接网络对每一个人的每一个可见节点预测相对于根节点的3D坐标$\\{P_k^{3D}\\}^K_{k=1}$</li>\n<li>先把$P_k^{2D}$坐标转成相对于脖子节点的坐标</li>\n<li><p>连接关节相对脖子的坐标$(u,v)_{j,k}$，关节检测置信度 $c_{j,k}$和关节3D坐标编码$l_{j,k}$为$S_k\\in\\mathbb{R}^{J\\times(3+3\\cdot J)}$,如果关节不可见就只连接成0矢量</p>\n<p><img src=\"Untitled%203.png\" alt=\"Untitled\"></p>\n</li>\n<li><p>Stage II是一个5层的全连接网络，它将$S_k$转成相对于根节点的3D坐标$P_k^{3D}$</p>\n<p>  <img src=\"Untitled%204.png\" alt=\"Untitled\"></p>\n</li>\n<li><p>Stage II 训练：</p>\n<pre><code>   Stage II 网络在MuCo-3DHP非截取的帧中训练,首先用Stage I网络在这个数据集里运行得到2D坐标和3D坐标编码，然后对每一个个体使用真实的相对于根节点的3D坐标$\\&#123;(X_j,Y_j,Z_j)\\&#125;^J_&#123;j=1&#125;$作为标签来训练Stage II, 使用smooth-L1 loss减轻遮挡产生的预测和标签的差异性\n</code></pre></li>\n</ul>\n</li>\n<li><p>Stage III:</p>\n<p>  经过Stage I和Stage II得到了一个帧中一个人的相对于根节点的3D坐标，但是没有估计人的大小和距离相机的距离，同一人的身份没有在所有帧中进行追踪</p>\n<ul>\n<li><p>为了在不同帧中追踪识别同一个人坐标</p>\n<pre><code>     现将先前的变量加一个时间索引例如$\\&#123;P_i[t]\\&#125;^&#123;K[t]&#125;_&#123;i=1&#125;$, 我们建模和持续追踪人的样子通过半身区域的HSV颜色直方图，对每个人切割颜色和饱和度通道成30份，在包含人体关节的边界框中定义类别概率为人体模样$A_&#123;i&#125;[t]$，缺点是颜色相似的个体问题。为了在不同帧中匹配对象更有鲁棒性匹配当前探测的人体i上先前已知的人体k要利用3个相似性，分别是人体模样相似性$S_&#123;i,k&#125;^A=(A_i[t]-A_k[t-1])^2$，2D坐标相似性$S_&#123;i,k&#125;^&#123;P^&#123;2D&#125;&#125;=(P^&#123;2D&#125;_i[t]-P^&#123;2D&#125;_k[t-1])^2$和3D坐标相似性$S_&#123;i,k&#125;^&#123;P^&#123;3D&#125;&#125;=(P^&#123;3D&#125;_&#123;i&#125;[t]-P^&#123;3D&#125;_&#123;k&#125;[t-1])$,通过设置相似性阈值检测人体被遮挡、人离开图片、人进入图片。\n</code></pre></li>\n<li><p>相对骨头长和绝对高度</p>\n<pre><code>      骨头长是一个尺度不变量，可以用$P_k^&#123;3D&#125;$计算，为了增加鲁棒性对连接节点的骨头长度$b_k$在10帧中做正态归一化处理。\n\n       将相对根节点2D坐标转成3D绝对坐标以cm为单位，这一任务很难只能凭借单目镜头猜测。先决定人在相机中的相对坐标，人的高度是从地面到一个交叉点的距离，该交叉点是放在足部位置处的虚拟广告牌和通过头部位置的视线的交点。\n</code></pre></li>\n</ul>\n</li>\n</ul>\n<pre><code>- Kinematic Skeleton Fitting\n\n             得到2D和3D坐标，优化骨骼坐标$\\&#123;\\theta_k[t]\\&#125;^&#123;K[t]&#125;_&#123;k=1&#125;$, $K[t]$是场景中t时刻所有的人数，$\\theta_k[t]\\in\\mathbb&#123;R&#125;^D$从固定骨架的关节角度加上全局根节点坐标描述人的姿态坐标, ，D=29是一个骨骼的自由度数。估计$\\theta_k[t]$使拟合能量最小化：\n\n    $$\n    \\varepsilon(\\theta_1[t],...,\\theta_K[t])=w_&#123;3D&#125;E_&#123;3D&#125;+w_&#123;2D&#125;E_&#123;2D&#125;+w_&#123;lim&#125;E_&#123;lim&#125;+w_&#123;temp&#125;E_&#123;temp&#125;+w_&#123;depth&#125;E_&#123;depth&#125;\n    $$\n\n    权重$w_&#123;3D&#125;=9e-1, w_&#123;2D&#125;=1e-5, w_&#123;3D&#125;=5e-1, w_&#123;temp&#125;=1e-7, w_&#123;depth&#125;=8e-6$；\n\n    3D逆运动误差：\n\n    误差：预测的相对根节点的3D坐标$P^&#123;3D&#125;_k[t]$和骨骼$\\mathscr&#123;P&#125;(\\theta_k[t],b_k)$坐标，它由每个人k的每个关节j和骨骼长度$b_k$正向运动构成\n\n    $$\n    E_&#123;3D&#125;=\\sum_&#123;k=1&#125;^K\\sum_&#123;j=1&#125;^&#123;J^&#123;3D&#125;&#125;||\\mathscr&#123;P&#125;(\\theta_k[t],b_k)_j-P^&#123;3D&#125;_&#123;k,j&#125;[t]||_2^2\n    $$\n\n    2D从投影误差：\n\n    误差：预测的2D关节坐标$P^&#123;2D&#125;_k[t]$和骨骼投影 $\\mathcal&#123;P&#125;(\\theta_k[t],b_k)_j$坐标\n\n    $$\n    E_&#123;2D&#125;=\\sum_&#123;k=1&#125;^K\\sum_&#123;j=1&#125;^&#123;J^&#123;2D&#125;&#125;w_j^&#123;2D&#125;c_&#123;j,k&#125;||\\Pi(h_k\\mathcal&#123;P&#125;(\\theta_k[t],b_k))_j-P^&#123;2D&#125;_&#123;k,j&#125;[t]||_2^2\n    $$\n\n    这里$c$是2D预测的置信度，$w_j^&#123;2D&#125;$是每个节点的相对权重, 下肢关节权重相对于躯干关节（屁股、脖子、肩膀）是1.7，同理肘是1.5，手腕是2.0，$\\Pi$是相机投影矩阵，注意$\\mathcal&#123;P&#125;(\\theta_k[t],b_k))_j$输出单位高度，缩放因子$h_k$把它影射到测量坐标\n\n    关节角度限制误差：\n\n      误差：关节限制规范给关节旋转角度范围增加限制，这是基于解剖学的关节角度范围\n\n    $$\n    E_&#123;lim&#125;=\\sum_&#123;k=1&#125;^K\\sum_&#123;j=7&#125;^&#123;D=29&#125;\\left\\&#123;\\begin&#123;aligned&#125;&amp;(\\theta_j^&#123;min&#125;-\\theta_&#123;k,j&#125;[t])^2 &amp; &amp;,if \\quad\\theta_&#123;k,j&#125;[t]&lt;\\theta_j^&#123;min&#125;\\\\ &amp;(\\theta_&#123;k,j&#125;[t]-\\theta_j^&#123;max&#125;)^2  &amp; &amp;,if\\quad \\theta_&#123;k,j&#125;[t]&gt;\\theta_j^&#123;max&#125;\\\\ &amp;0 &amp; &amp;,otherwise\\end&#123;aligned&#125;\\right.\n    $$\n\n    骨架自由度从7开始是因为对全局位置和旋转参数没有限制，神经网络的估计的关节位置对于关节角度没有限制\n\n    时间平滑误差：\n\n    误差：网络基于每帧估计的坐标会存在时间上的抖动\n\n    $$\n    E_&#123;temp&#125;(\\Theta)=\\sum_&#123;k=1&#125;^K||\\nabla\\theta_k[t-1]-\\nabla\\theta_k[t]||^2_2\n    $$\n\n       使用平滑项更多惩罚深度方向有更少约束的变量，$E_&#123;depth&#125;=||\\theta_&#123;k,2&#125;[t]_z-\\theta_&#123;k,2&#125;[t-1]_z||$,这里$\\theta_&#123;k,2&#125;$是自由度，它驱动了根节点的z坐标\n\n    逆运动追踪初始化：\n\n       对于新人追踪时的头一帧，局部关节角度拟合3D预测坐标且只考虑 $E_&#123;3D&#125;$和$E_&#123;lim&#125;$误差，之后这些关节角度锁死同时最小化$E_&#123;2D&#125;$,这是为了对骨骼的全局位移和旋转做最佳拟合，随后完整的能量公式$\\varepsilon(\\theta_1[t],...,\\theta_K[t])$被使用。\n</code></pre>"},{"typora-root-url":"./数学知识","title":"数学知识","mathjax":true,"date":"2022-11-04T02:35:57.000Z","_content":"\n# 概率论\n\n## 先验、后验、似然\n根据原因的可能性、结果的可能性、它们的先后顺序和条件关系判断\n**后验概率**\n由果溯因，由结果概率分布估计原因概率分布，记作$p(原因｜结果)$条件概率\n**先验概率**\n人为假设原因的概率分布，与结果概率分布无关，记作$p(原因)$条件概率\n**似然概率**\n由因溯果，由原因概率分布估计结果概率分布，记作$p(结果｜原因)$条件概率\n**evidence**\n实际经验给出结果的概率分布，不考虑原因的概率分布，记作$p(结果)$\n**贝叶斯公式**\n$$\np(\\theta|x)=\\frac{p(x|\\theta)p(\\theta)}{p(x)}\n$$\n$x$:观察测量（结果）\n$\\theta$:状态量（原因）\n$p(\\theta|x)$:后验几率\n$p(\\theta)$:先验几率\n$L(\\theta|x)或p(x|\\theta)$:似然几率\n$p(x)$:经验几率\n\n**MLE-最大似然估计**: 频率学派求模型参数$\\Theta$\n$$\n\\begin{aligned}\n\t\\hat{\\Theta}_{\\mathrm{MLE}}&=\\arg\\max P(X|\\Theta) \\\\\n\t&=\\arg\\max P(x_1|\\Theta)P(x_2|\\Theta)\\cdots P(x_n|\\Theta) \\\\\n\t&=\\arg\\max \\log\\prod_{i=1}^{n}P(x_i|\\Theta) \\\\\n\t&=\\arg\\max \\sum_{i=1}^{n}\\log P(x_i|\\Theta) \\\\\n\t&=\\arg\\min -\\sum_{i=1}^{n}\\log P(x_i|\\Theta)\n\\end{aligned}\n$$\n\n**MAP-最大后验估计**: 贝叶斯学派求模型参数$\\Theta$\n$$\n\\begin{aligned}\n\t\\hat{\\Theta}_{\\mathrm{MAP}}&=\\arg\\max P(\\Theta|X) \\\\\n\t&=\\arg\\min -\\log P(\\Theta|X) \\\\\n\t&=\\arg\\min -\\log\\frac{P(X|\\Theta)P(\\Theta)}{P(X)} \\\\\n\t&=\\arg\\min -\\log P(X|\\Theta)-\\log P(\\Theta) + \\log P(X)\\\\\n\t&=\\arg\\min -\\log P(X|\\Theta)-\\log P(\\Theta)\n\\end{aligned}\n$$\n\n","source":"_posts/数学知识.md","raw":"---\ntypora-root-url: ./数学知识\ntitle: 数学知识\nmathjax: true\ndate: 2022-11-04 10:35:57\ncategories:\n- 数学\ntags:\n- 数学\n- 博客\n---\n\n# 概率论\n\n## 先验、后验、似然\n根据原因的可能性、结果的可能性、它们的先后顺序和条件关系判断\n**后验概率**\n由果溯因，由结果概率分布估计原因概率分布，记作$p(原因｜结果)$条件概率\n**先验概率**\n人为假设原因的概率分布，与结果概率分布无关，记作$p(原因)$条件概率\n**似然概率**\n由因溯果，由原因概率分布估计结果概率分布，记作$p(结果｜原因)$条件概率\n**evidence**\n实际经验给出结果的概率分布，不考虑原因的概率分布，记作$p(结果)$\n**贝叶斯公式**\n$$\np(\\theta|x)=\\frac{p(x|\\theta)p(\\theta)}{p(x)}\n$$\n$x$:观察测量（结果）\n$\\theta$:状态量（原因）\n$p(\\theta|x)$:后验几率\n$p(\\theta)$:先验几率\n$L(\\theta|x)或p(x|\\theta)$:似然几率\n$p(x)$:经验几率\n\n**MLE-最大似然估计**: 频率学派求模型参数$\\Theta$\n$$\n\\begin{aligned}\n\t\\hat{\\Theta}_{\\mathrm{MLE}}&=\\arg\\max P(X|\\Theta) \\\\\n\t&=\\arg\\max P(x_1|\\Theta)P(x_2|\\Theta)\\cdots P(x_n|\\Theta) \\\\\n\t&=\\arg\\max \\log\\prod_{i=1}^{n}P(x_i|\\Theta) \\\\\n\t&=\\arg\\max \\sum_{i=1}^{n}\\log P(x_i|\\Theta) \\\\\n\t&=\\arg\\min -\\sum_{i=1}^{n}\\log P(x_i|\\Theta)\n\\end{aligned}\n$$\n\n**MAP-最大后验估计**: 贝叶斯学派求模型参数$\\Theta$\n$$\n\\begin{aligned}\n\t\\hat{\\Theta}_{\\mathrm{MAP}}&=\\arg\\max P(\\Theta|X) \\\\\n\t&=\\arg\\min -\\log P(\\Theta|X) \\\\\n\t&=\\arg\\min -\\log\\frac{P(X|\\Theta)P(\\Theta)}{P(X)} \\\\\n\t&=\\arg\\min -\\log P(X|\\Theta)-\\log P(\\Theta) + \\log P(X)\\\\\n\t&=\\arg\\min -\\log P(X|\\Theta)-\\log P(\\Theta)\n\\end{aligned}\n$$\n\n","slug":"数学知识","published":1,"updated":"2022-11-04T03:33:46.280Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cla22wp800000qt3k2a12avyd","content":"<h1 id=\"概率论\"><a href=\"#概率论\" class=\"headerlink\" title=\"概率论\"></a>概率论</h1><h2 id=\"先验、后验、似然\"><a href=\"#先验、后验、似然\" class=\"headerlink\" title=\"先验、后验、似然\"></a>先验、后验、似然</h2><p>根据原因的可能性、结果的可能性、它们的先后顺序和条件关系判断<br><strong>后验概率</strong><br>由果溯因，由结果概率分布估计原因概率分布，记作$p(原因｜结果)$条件概率<br><strong>先验概率</strong><br>人为假设原因的概率分布，与结果概率分布无关，记作$p(原因)$条件概率<br><strong>似然概率</strong><br>由因溯果，由原因概率分布估计结果概率分布，记作$p(结果｜原因)$条件概率<br><strong>evidence</strong><br>实际经验给出结果的概率分布，不考虑原因的概率分布，记作$p(结果)$<br><strong>贝叶斯公式</strong></p>\n<script type=\"math/tex; mode=display\">\np(\\theta|x)=\\frac{p(x|\\theta)p(\\theta)}{p(x)}</script><p>$x$:观察测量（结果）<br>$\\theta$:状态量（原因）<br>$p(\\theta|x)$:后验几率<br>$p(\\theta)$:先验几率<br>$L(\\theta|x)或p(x|\\theta)$:似然几率<br>$p(x)$:经验几率</p>\n<p><strong>MLE-最大似然估计</strong>: 频率学派求模型参数$\\Theta$</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n    \\hat{\\Theta}_{\\mathrm{MLE}}&=\\arg\\max P(X|\\Theta) \\\\\n    &=\\arg\\max P(x_1|\\Theta)P(x_2|\\Theta)\\cdots P(x_n|\\Theta) \\\\\n    &=\\arg\\max \\log\\prod_{i=1}^{n}P(x_i|\\Theta) \\\\\n    &=\\arg\\max \\sum_{i=1}^{n}\\log P(x_i|\\Theta) \\\\\n    &=\\arg\\min -\\sum_{i=1}^{n}\\log P(x_i|\\Theta)\n\\end{aligned}</script><p><strong>MAP-最大后验估计</strong>: 贝叶斯学派求模型参数$\\Theta$</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n    \\hat{\\Theta}_{\\mathrm{MAP}}&=\\arg\\max P(\\Theta|X) \\\\\n    &=\\arg\\min -\\log P(\\Theta|X) \\\\\n    &=\\arg\\min -\\log\\frac{P(X|\\Theta)P(\\Theta)}{P(X)} \\\\\n    &=\\arg\\min -\\log P(X|\\Theta)-\\log P(\\Theta) + \\log P(X)\\\\\n    &=\\arg\\min -\\log P(X|\\Theta)-\\log P(\\Theta)\n\\end{aligned}</script>","site":{"data":{}},"cover":"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/cover3.jpg","excerpt":"","more":"<h1 id=\"概率论\"><a href=\"#概率论\" class=\"headerlink\" title=\"概率论\"></a>概率论</h1><h2 id=\"先验、后验、似然\"><a href=\"#先验、后验、似然\" class=\"headerlink\" title=\"先验、后验、似然\"></a>先验、后验、似然</h2><p>根据原因的可能性、结果的可能性、它们的先后顺序和条件关系判断<br><strong>后验概率</strong><br>由果溯因，由结果概率分布估计原因概率分布，记作$p(原因｜结果)$条件概率<br><strong>先验概率</strong><br>人为假设原因的概率分布，与结果概率分布无关，记作$p(原因)$条件概率<br><strong>似然概率</strong><br>由因溯果，由原因概率分布估计结果概率分布，记作$p(结果｜原因)$条件概率<br><strong>evidence</strong><br>实际经验给出结果的概率分布，不考虑原因的概率分布，记作$p(结果)$<br><strong>贝叶斯公式</strong></p>\n<script type=\"math/tex; mode=display\">\np(\\theta|x)=\\frac{p(x|\\theta)p(\\theta)}{p(x)}</script><p>$x$:观察测量（结果）<br>$\\theta$:状态量（原因）<br>$p(\\theta|x)$:后验几率<br>$p(\\theta)$:先验几率<br>$L(\\theta|x)或p(x|\\theta)$:似然几率<br>$p(x)$:经验几率</p>\n<p><strong>MLE-最大似然估计</strong>: 频率学派求模型参数$\\Theta$</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n    \\hat{\\Theta}_{\\mathrm{MLE}}&=\\arg\\max P(X|\\Theta) \\\\\n    &=\\arg\\max P(x_1|\\Theta)P(x_2|\\Theta)\\cdots P(x_n|\\Theta) \\\\\n    &=\\arg\\max \\log\\prod_{i=1}^{n}P(x_i|\\Theta) \\\\\n    &=\\arg\\max \\sum_{i=1}^{n}\\log P(x_i|\\Theta) \\\\\n    &=\\arg\\min -\\sum_{i=1}^{n}\\log P(x_i|\\Theta)\n\\end{aligned}</script><p><strong>MAP-最大后验估计</strong>: 贝叶斯学派求模型参数$\\Theta$</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n    \\hat{\\Theta}_{\\mathrm{MAP}}&=\\arg\\max P(\\Theta|X) \\\\\n    &=\\arg\\min -\\log P(\\Theta|X) \\\\\n    &=\\arg\\min -\\log\\frac{P(X|\\Theta)P(\\Theta)}{P(X)} \\\\\n    &=\\arg\\min -\\log P(X|\\Theta)-\\log P(\\Theta) + \\log P(X)\\\\\n    &=\\arg\\min -\\log P(X|\\Theta)-\\log P(\\Theta)\n\\end{aligned}</script>"},{"typora-root-url":"./slam","title":"slam","mathjax":true,"date":"2022-11-04T01:21:41.000Z","_content":"\n# 后端\n\n## 状态估计概率解释\n\n视觉里程计只有短暂记忆，而我们希望整个运动轨迹在较长时间内都能保持最优的状态。用最新的知识更新较久远的状态。这是个状态估计问题。\n$$\n状态估计方式\\left\\{\\begin{align*}\n\t&批量式(\\mathrm{Batch}): 过去信息+当前信息+未来信息更新状态 \\\\\n\t&渐进式(\\mathrm{Incremental}): 只由过去信息更新状态\n\\end{align*}\\right.\n$$\n假设$t=0$到$t=N$时间内，有位姿$x_0$到$x_N$，并有路标$y_0$，...，$y_M$，运动方程和观测方程为\n$$\n\\begin{equation}\n\\left\\{\\begin{aligned}\n\t&\\vec{x}_k=f(\\vec{x}_{k-1},\\vec{u}_k)+\\vec{w}_k \\\\\n\t&\\vec{z}_{k,j}=h(\\vec{y}_j,\\vec{x}_k)+\\vec{v}_{k,j}\n\\end{aligned}\\right.\\quad\\quad k=1,\\cdots,N,\\quad j=1,\\cdots,M.\n\\end{equation}\n$$\n特点：\n- 运动方程数量小于甚至远小于观测方程\n- 没有运动方程相当于通过一组时序图像来恢复运动和结构\n\n方程中的位姿$\\vec{x}$和路标$\\vec{y}$受噪声影响，所以应该看成服从某种概率分布的随机变量，而不是一个数。\n\n**状态估计中的观测量和状态量**：\n$$\n\\left\\{\\begin{align*}\n\t&状态量：位姿\\vec{x}，路标\\vec{y} \\\\\n\t&观测量：运动输入数据\\vec{u}，观测数据\\vec{z}\n\\end{align*}\\right.\n$$\n\n**重新定义状态量**：$\\vec{x}_k\\overset{def}{=}\\{\\vec{x}_k,\\vec{y}_1,\\cdots,\\vec{y}_m\\}$表示k时刻状态量$\\vec{x}_k$由k时刻位姿$\\vec{x}_k$及$m$个路标$\\vec{y}_*$组成。\n\n**重新定义观测数据**：$\\vec{z}_k\\overset{def}{=}\\{\\vec{z}_{k,1},\\cdots,\\vec{z}_{k,m}\\}$表示k时刻对m个路标的观测数据\n\n**新的运动和观测方程**\n$$\n\\begin{equation}\n\\label{eq:kinemic_and_observation}\n\\left\\{\\begin{aligned}\n\t&\\vec{x}_k=f(\\vec{x}_{k-1},\\vec{u}_k)+\\vec{w}_k\\\\\n\t&\\vec{z}_{k}=h(\\vec{x}_k)+\\vec{v}_{k}\n\\end{aligned}\\right.\\quad\\quad k=1,\\cdots,N.\n\\end{equation}\n$$\n\n$k$时刻状态量几率函数用$0$到$k$时刻的观测量来估计，即**后验分布**记为$P(\\vec{x}_k|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k})$, 这里$\\vec{u}_{1:k}=\\{\\vec{u}_1,\\vec{u}_2,\\cdots,\\vec{u}_k\\}$，$\\vec{z}_{1:k}=\\{\\vec{z}_1,\\vec{z}_2,\\cdots,\\vec{z}_k\\}$，根据贝叶斯法则，将后验分布分解成**似然分布**和**先验分布**的乘积\n\n$$\n\\begin{equation}\nP(\\vec{x}_k|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k})\\propto P(\\vec{z}_k|\\vec{x}_k)P(\\vec{x}_k|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})\n\\end{equation}\n$$\n\n**先验分布**是基于过去所有状态估计得来，简化只受到$\\vec{x}_{k-1}$影响，联合分布$P(\\vec{x}_k，\\vec{x}_{k-1}|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})$对$\\vec{x}_{k-1}$积分得到边缘分布$P(\\vec{x}_k|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})$即**先验分布**，联合分布可以通过条件概率公式转化为$\\vec{x}_{k-1}$的条件概率形式，即$P(\\vec{x}_k|\\vec{x}_{k-1},\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})P(\\vec{x}_{k-1}|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})$，因此**先验分布**可以表示成：\n$$\n\\begin{equation}\\label{eq:xianyan}\nP(\\vec{x}_k|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})=\\int P(\\vec{x}_k|\\vec{x}_{k-1},\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})P(\\vec{x}_{k-1}|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})\\,\\mathrm{d}\\vec{x}_{k-1}\n\\end{equation}\n$$\n\n先验分布的处理方式：\n\n$$\n\\left\\{\\begin{align*}\n&k时刻状态\\vec{x}_k的分布只和k-1时刻的状态\\vec{x}_{k-1}有关即马尔可夫假设&&: 扩展卡尔曼滤波(EKF) \\\\\n&k时刻状态\\vec{x}_k的分布与之前所有状态\\vec{x}_{1:k}有关&&: 非线性优化\n\\end{align*}\\right.\n$$\n\n## 线性系统和卡尔曼滤波（KF）\n\n根据k时刻状态$\\vec{x}_k$的分布只和k-1时刻的状态$\\vec{x}_{k-1}$有关，所以可以去掉$\\vec{x}_{0:k-2}$这些状态量，k时刻状态量$\\vec{x}_k$和k时刻运动输入量$\\vec{u}_k$以及观测量$\\vec{z}_k$有关和其它时刻没关系，公式$\\ref{eq:xianyan}$中右侧第一部分可简化为\n\n$$\n\\begin{equation}\nP(\\vec{x}_k|\\vec{x}_{k-1},\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})=P(\\vec{x}_k|\\vec{x}_{k-1},\\vec{u}_k)\n\\end{equation}\n$$\n\n由于k-1时刻状态量$\\vec{x}_k$还是按照和之前状态都有关的假设，公式$\\ref{eq:xianyan}$中右侧第二部分可简化为\n\n$$\n\\begin{equation}\n\\label{eq:k-1zhuang_tai_liang}\nP(\\vec{x}_{k-1}|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})=P(\\vec{x}_{k-1}|\\vec{x}_{0},\\vec{u}_{1:k-1},\\vec{z}_{1:k-1})\n\\end{equation}\n$$\n\n公式$\\ref{eq:k-1zhuang_tai_liang}$就是k-1时刻的状态分布。\n\n**线性高斯系统**\n\n将公式$\\ref{eq:kinemic_and_observation}$中的运动方程和观测方程按照线性函数的形式展开\n\n$$\n\\begin{equation}\n\\left\\{\\begin{aligned}\n\t&\\vec{x}_k=A_k\\vec{x}_{k-1}+\\vec{u}_k+\\vec{w}_k \\\\\n\t&\\vec{z}_{k}=C_k\\vec{x}_k+\\vec{v}_{k}\n\\end{aligned}\\right.\\quad\\quad k=1,\\cdots,N.\n\\end{equation}\n$$\n\n运动噪声和观测噪声都服从0均值正态分布：\n\n$$\n\\begin{equation}\n\\vec{w}_k\\sim N(0,R_{k})\\quad\\quad\\quad \\vec{v}_k\\sim N(0,Q_{k})\n\\end{equation}\n$$\n\n**记号区分**：上帽子$\\hat{\\vec{x}}_k$表示后验，下帽子$\\check{\\vec{x}}_k$表示先验\n\n**已知**k-1时刻的后验状态估计为$\\hat{\\vec{x}}_{k-1}$及其协方差$\\hat{P}_{k-1}$，状态估计服从高斯分布，**求**k时刻的状态估计及其协方差\n\n**运动方程**确定$\\vec{x}_k$的**先验分布**（随机变量函数的分布函数），扰动传递规则\n$$\n\\begin{equation}\nP(\\vec{x}_k|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})=N(A_k\\hat{\\vec{x}}_{k-1}+\\vec{u}_k,A_k\\hat{P}_{k-1}A^T_k+R_{k})\n\\end{equation}\n$$\n**先验分布**得到k时刻状态估计，记为$\\check{\\vec{x}}=A_k\\hat{\\vec{x}}_{k-1}+\\vec{u}_k$，$\\check{P}_k=A_k\\hat{P}_{k-1}A_k^T+R_k$\n\n**观测方程**确定状态量的**似然函数**$P(\\vec{z}_k|\\vec{x}_k)=N(C_k\\vec{x}_k, Q_k)$\n\n**贝叶斯公式**确定状态量的**后验分布**\n$$\n\\begin{equation}\nN(\\hat{\\vec{x}}_k,\\hat{P}_k)=\\eta N(C_k\\vec{x}_k,Q_k)\\cdot N(\\check{\\vec{x}}_k,\\check{P}_k)\n\\end{equation}\n$$\n\n归纳为：预测和更新两步\n1. 预测\n$$\n\\begin{equation}\n\\check{\\vec{x}}_k=A_k\\hat{\\vec{x}}_{k-1}+\\vec{u}_k,\\quad\\quad \\check{P}_k=A_k\\hat{P}_{k-1}A^T_k+R_k\n\\end{equation}\n$$\n2. 更新：\n2.1.  先计算$K_k$，称为卡尔曼增益\n$$\n\\begin{equation}\nK=\\check{P}_kC_k^T(C_k\\check{P}_kC_k^T+Q_k)^{-1}\n\\end{equation}\n$$\n​      2.2. 计算后验分布的估计量\n$$\n\\begin{equation}\n\\hat{\\vec{x}}_k=\\check{\\vec{x}}_k+K_k(\\vec{z}_k-C_k\\check{\\vec{x}}_k), \\quad\\quad \\hat{P}_k=(I-K_kC_k)\\check{P}_k\n\\end{equation}\n$$\n\n\n## 非线性系统和扩展卡尔曼滤波（EKF）\n\nSLAM中的运动观测方程是非线性函数\n$$\n\\left\\{\\begin{align*}\n&高斯分布\\xrightarrow{线性变换}高斯分布\\\\\n&高斯分布\\xrightarrow{非线性变换}非高斯分布\n\\end{align*}\\right.\n$$\n解决方法：在状态量附近对**运动方程**和**观测方程**进行泰勒展开，保留一阶项，从而得到近似的线性部分，再按照线性系统进行推到。\n\n**已知**k-1时刻的后验状态估计为$\\hat{\\vec{x}}_{k-1}$及其协方差$\\hat{P}_{k-1}$，状态估计服从高斯分布，**求**k时刻的状态估计及其协方差，将**运动方程**和**观测方程**在$\\hat{\\vec{x}}_{k-1}$和$\\hat{P}_k$处进行**线性化**。\n\n对运动方程\n$$\n\\begin{equation}\n\\vec{x}_k\\approx f(\\hat{\\vec{x}}_{k-1},\\vec{u}_k)+\\left.\\frac{\\partial f}{\\partial \\vec{x}_{k-1}}\\right|_{\\hat{\\vec{x}}_{k-1}}(\\vec{x}_{k-1}-\\hat{\\vec{x}}_{k-1})+\\vec{w}_k\n\\end{equation}\n$$\n记偏导数为\n$$\n\\begin{equation}\nF=\\left.\\frac{\\partial f}{\\partial \\vec{x}_{k-1}}\\right|_{\\hat{\\vec{x}}_{k-1}}\n\\end{equation}\n$$\n对观测方程\n$$\n\\begin{equation}\n\\vec{z}_k\\approx h(\\check{\\vec{x}}_{k})+\\left.\\frac{\\partial h}{\\partial \\vec{x}_{k}}\\right|_{\\check{\\vec{x}}_{k}}(\\vec{x}_{k}-\\check{\\vec{x}}_{k-1})+\\vec{v}_k\n\\end{equation}\n$$\n记偏导数为\n$$\n\\begin{equation}\nH=\\left.\\frac{\\partial h}{\\partial \\vec{x}_{k}}\\right|_{\\check{\\vec{x}}_{k}}\n\\end{equation}\n$$\n\n仿照线性变换部分计算先验、似然和后验分布，根据分布求状态量的估计值\n归纳为：预测和更新两步\n1. 预测\n$$\n\\begin{equation}\n\\check{\\vec{x}}_k=f(\\hat{\\vec{x}}_{k-1},\\vec{u}_k),\\quad\\quad \\check{P}_k=F\\hat{P}_{k-1}F^T+R_k\n\\end{equation}\n$$\n2. 更新：\n2.1.  先计算$K_k$，称为卡尔曼增益\n$$\n\\begin{equation}\nK_k=\\check{P}_kH^T(H\\check{P}_kH^T+Q_k)^{-1}\n\\end{equation}\n$$\n​      2.2. 计算后验分布的估计量\n$$\n\\begin{equation}\n\\hat{\\vec{x}}_k=\\check{\\vec{x}}_k+K_k(\\vec{z}_k-h(\\check{\\vec{x}}_k)), \\quad\\quad \\hat{P}_k=(I-K_kH)\\check{P}_k\n\\end{equation}\n$$\n\n\n## 非线性优化：Bundle Adjustment（BA）\n\n1. 世界坐标系下的点记作$$\\vec{p}_w=[X_w,Y_w,Z_w]^T$$，\n2. 相机坐标系下的点$$\\vec{p}_c=[X_c,Y_c,Z_c]^T$$，\n3. 归一化平面上的点$$\\vec{p}_{c_n}=[u_c,v_c,1]^T=[X_c/Z_c,Y_c/Z_c,1]^T$$\n4. 归一化坐标的畸变情况，还原畸变矫正前的原始投影坐标，只考虑径向畸变\n$$\n\\begin{equation}\n\\left\\{\\begin{aligned}\n&u_c^\\prime =u_c(1+k_1r_c^2+k_2r_c^4)\\\\\n&v_c^\\prime =v_c(1+k_1r_c^2+k_2r_c^4)\n\\end{aligned}\\right. \\quad\\quad r_c^2=u_c^2+v_c^2\n\\end{equation}\n$$\n5. 根据相机内参（焦距，像素平面的原点坐标）计算像素坐标\n$$\n\\begin{equation}\n\\left\\{\\begin{aligned}\n&u_s=f_xu_c^\\prime+c_x\\\\\n&v_s=f_yu_c^\\prime+c_y\n\\end{aligned}\\right.\n\\end{equation}\n$$\n\n在**i时刻**，$\\vec{x_i}$指代相机**位姿**即$R_i，\\vec{t_i}$，其对应的李群为$T_i$，李代数为$\\vec{\\xi}_i$。**j路标**$\\vec{y}_j$即三维点$\\vec{p}_j$，**观测数据**是像素坐标$\\vec{z}_{i,j}\\overset{def}{=}[u_{s_{i,j}},v_{s_{i,j}}]^T$。观测误差\n$$\n\\begin{equation}\n\\vec{e_{i,j}}=\\vec{z}_{i,j}-h(T_i,\\vec{p}_j)\n\\end{equation}\n$$\n个时刻对所有路标观测的整体误差\n$$\n\\begin{equation}\n\\frac{1}{2}\\sum_{i=i}^{m}\\sum_{j=1}^{n}||\\vec{e}_{i,j}||^2=\\frac{1}{2}\\sum_{i=i}^{m}\\sum_{j=1}^{n}||\\vec{z}_{i,j}-h(T_i,\\vec{p}_j)||^2\n\\end{equation}\n$$\n\n\n### 求解BA\n\nBA目标函数的**自变量**定义成\n$$\n\\begin{equation}\n\\vec{x}=[T_1,\\cdots,T_m,\\vec{p}_1,\\cdots,\\vec{p}_n]^T\n\\end{equation}\n$$\n给自变量一个增量$\\Delta \\vec{x}$\n$$\n\\begin{align}\\label{eq:BA:object_fun}\n\\frac{1}{2}||f(\\vec{x}+\\Delta\\vec{x})||^2&\\approx\\frac{1}{2}\\sum_{i=i}^{m}\\sum_{j=1}^{n}||\\vec{e_{i,j}}+F_{i,j}\\Delta\\vec{\\xi}_{i}+E_{i,j}\\Delta\\vec{p}_j)||^2 \\\\\nF_{i,j}&=\\frac{\\partial \\vec{e}_{i,j}}{\\partial \\vec{\\xi}_i}\\\\\nE_{i,j}&=\\frac{\\partial \\vec{e}_{i,j}}{\\partial \\vec{p}_j}\n\\end{align}\n$$\n将**位姿**变量放在一起，**路标**变量放在一起，记作：\n$$\n\\begin{equation}\n\\vec{x}_c=[\\vec{\\xi}_1,\\vec{\\xi}_2,\\cdots,\\vec{\\xi}_m]^T\\in\\mathbb{R}^{6m}\n\\end{equation}\n$$\n$$\n\\begin{equation}\n\\vec{x}_p=[\\vec{p}_1,\\vec{p}_2,\\cdots,\\vec{p}_n]^T\\in\\mathbb{R}^{3m}\n\\end{equation}\n$$\n公式$\\ref{eq:BA:object_fun}$记作：\n$$\n\\begin{equation}\n\\frac{1}{2}||f(\\vec{x}+\\Delta\\vec{x})||^2\\approx\\frac{1}{2}\\sum_{i=i}^{m}\\sum_{j=1}^{n}||\\vec{e}+F\\Delta\\vec{x}_{c}+E\\Delta\\vec{x}_p)||^2\\quad这里的E和F是由若干小矩阵块E_{i,j}和F_{i,j}各自拼凑起来的\n\\end{equation}\n$$\n无论**高斯牛顿法**还是**列文伯格-马夸尔特方法**增量线性方程都是：\n$$\n\\begin{equation}\nH\\Delta\\vec{x}=\\vec{g}\n\\end{equation}\n$$\n$$\n区别: H\\left\\{\\begin{align*}\n&J^TJ & &高斯牛顿法\\\\\n&J^TJ+\\lambda I & &列文伯格-马夸尔特方法\n\\end{align*}\\right.\n$$\n注：有些博客用$H=J^T\\Omega J$,$\\Omega$应该是正态分布协方差的逆矩阵$\\Sigma^{-1}$，它是2x2矩阵，可以从$\\vec{e}$是二维向量推出,已将自变量归类成位姿和空间点两种形式，雅可比矩阵亦分块成\n$$\n\\begin{equation}\nJ=[F \\quad E]\n\\end{equation}\n$$\n\n求高斯牛顿$H$，逆运算复杂度$O(n^3)$，但$H$是有稀疏矩阵结构\n$$\n\\begin{equation}\nH=J^TJ=\\left[\\begin{aligned}\n&F^TF & &F^TE\\\\\n&E^TF & &E^TE\n\\end{aligned}\\right]\n\\end{equation}\n$$\n\n\n### 利用稀疏性和边缘化求解BA\n\n雅可比矩阵的结构\n$$\n\\begin{equation}\nJ_{i,j}(\\vec{x})=\\left(\\vec{0}_{2\\times6},\\cdots,\\vec{0}_{2\\times6},\\frac{\\partial\\vec{e}_{i,j}}{\\partial T_i},\\vec{0}_{2\\times6},\\cdots\\left|\\vec{0}_{2\\times3},\\cdots,\\vec{0}_{2\\times3},\\frac{\\partial\\vec{e}_{i,j}}{\\partial \\vec{p}_j},\\vec{0}_{2\\times3},\\cdots,\\vec{0}_{2\\times3}\\right.\\right)\n\\end{equation}\n$$\n![稀疏结构](image-20221107155224157.png)\n\n***\n举例：考虑一个场景内有2个相机位姿($C_1$,$C_2$)和6个路标点($P_1$,$P_2$,$P_3$,$P_4$,$P_5$,$P_6$)，相机在位姿$C_1$处观察到路标$P_1$,$P_2$,$P_3$,$P_4$，在位姿$C_2$处观察到路标$P_3$,$P_4$,$P_5$,$P_6$，如图![举例示例](image-20221107161443842.png)\n该场景下的BA目标函数\n$$\n\\begin{equation}\n\\frac{1}{2}\\left(||\\vec{e}_{11}||^2+||\\vec{e}_{12}||^2+||\\vec{e}_{13}||^2+||\\vec{e}_{14}||^2+||\\vec{e}_{23}||^2+||\\vec{e}_{24}||^2+||\\vec{e}_{25}||^2+||\\vec{e}_{26}||^2\\right)\n\\end{equation}\n$$\n$\\vec{e}_{11}$描述了$C_1$看到$P_1$这件事，与其它位姿和路标无关，所以对它们求导为0，按照变量$\\vec{x}=(\\vec{\\xi}_1,\\vec{\\xi}_2,\\vec{p}_1,\\cdots,\\vec{p}_6)$，有\n$$\n\\begin{equation}\nJ_{11}=\\frac{\\partial \\vec{e}_{11}}{\\partial \\vec{x}}=\\left(\\frac{\\vec{e}_{11}}{\\partial\\vec{\\xi}_1},\\vec{0}_{2\\times6},\\frac{\\vec{e}_{11}}{\\partial\\vec{p}_1},\\vec{0}_{2\\times3},\\vec{0}_{2\\times3},\\vec{0}_{2\\times3},\\vec{0}_{2\\times3},\\vec{0}_{2\\times3}\\right)\n\\end{equation}\n$$\n形状如图![分块雅可比矩阵稀疏结构](image-20221107162453662.png)\n拼接$J_{ij}$可以得到整体雅可比矩阵和$H$矩阵如图![完整雅可比矩阵稀疏结构](image-20221107162722048.png)\n\n***\n**图优化结构和增量方程的稀疏性存在联系**：$H$矩阵中的非对角部分的非零矩阵块可以理解为其对应的位姿和路标变量之间存在联系也可称为约束，如图![增量矩阵和图优化结构的联系](image-20221107163948504.png)\n当有m个相机位姿和n个路标点时，且路标数量远大于位姿数量，即$n\\gg m$，$H$矩阵左上角区域会非常小，右下脚区域非常大，非对角部分分布散乱的约束块，形状如箭头\n\n<center><img src=\"image-20221107164511662.png\" alt=\"一般情况下H矩阵块\" style=\"zoom: 33%;\" /></center>\n\n**Schur消元**即**Marginalization边缘化**加速计算稀疏矩阵$H$，$H$矩阵分成4块，分别为$B,E,E^T,C$如图。\n<center><img src=\"image-20221107165342106.png\" alt=\"H矩阵分块\" style=\"zoom: 33%;\"  /></center>\n$$\n\\begin{equation}\\label{eq:Joint_probability_distribution}\n\\left[\\begin{aligned}\n&B & &E\\\\\n&E^T & &C\n\\end{aligned}\\right]\n\\left[\\begin{aligned}\n&\\Delta\\vec{x}_c\\\\\n&\\Delta\\vec{x}_p\n\\end{aligned}\\right]\n=\n\\left[\\begin{aligned}\n&\\vec{v}\\\\\n&\\vec{w}\n\\end{aligned}\\right]\n\\end{equation}\n$$\n其中$B$是对角块矩阵，每个对角块的维度和相机位姿的维度相同，对角块的个数和位姿的个数相同(李群：9维，李代数：6维)；同理$C$是对角块矩阵，每个对角块的维度和路标的维度(三维)相同，对角块的个数和路标个数相同， 公式$\\ref{eq:Joint_probability_distribution}$可以理解成**联合概率分布**。\n\n**边缘化**即消去路标变量对应的系数矩阵，使方程中只含有位姿变量和其系数矩阵\n$$\n\\begin{equation}\n\\left[\\begin{aligned}\n&I & &-EC^{-1}\\\\\n&[0]_{n\\times m} & &I\n\\end{aligned}\\right]\n\\left[\\begin{aligned}\n&B & &E\\\\\n&E^T & &C\n\\end{aligned}\\right]\n\\left[\\begin{aligned}\n&\\Delta\\vec{x}_c\\\\\n&\\Delta\\vec{x}_p\n\\end{aligned}\\right]\n=\n\\left[\\begin{aligned}\n&I & &-EC^{-1}\\\\\n&[0]_{n\\times m} & &I\n\\end{aligned}\\right]\n\\left[\\begin{aligned}\n&\\vec{v}\\\\\n&\\vec{w}\n\\end{aligned}\\right]\n\\end{equation}\n$$\n\n位姿部分的增量方程为\n$$\n\\begin{equation}\n\\label{eq:marginalization}\n\\left[B-EC^{-1}\\right]\\Delta \\vec{x}_c=\\vec{v}-EC^{-1}\\vec{w}\n\\end{equation}\n$$\n这可以理解成$\\Delta\\vec{x}_c$的**先验函数**，求解$\\Delta\\vec{x}_c$后再求解$\\Delta\\vec{x}_p$，求解$\\Delta\\vec{x}_p$的方程可以理解成**条件概率**（已知$\\Delta\\vec{x}_c$），利用对角块矩阵$C$的逆运算容易求解\n$$\n\\begin{equation}\n\\Delta\\vec{x}_p=C^{-1}(\\vec{w}-E^T\\Delta\\vec{x}_c)\n\\end{equation}\n$$\n\n公式$\\ref{eq:marginalization}$的系数矩阵$\\left[B-EC^{-1}\\right]\\Delta \\vec{x}_c$记作$S$，它的稀疏性是不规则的，如图\n<center><img src=\"image-20221108112428467.png\" alt=\"边缘化系数矩阵稀疏结构\" style=\"zoom: 33%;\" /></center>\n\n如同$H$矩阵的**非对角块处的非零块**对应着**相机位姿**和**路标**的联系，$S$矩阵的**非对角块处的非零块**对应着**相机的两个位姿**之间存在着**共同观测的路标**也称**共视路标**\n\n### 鲁棒核函数\n\n目标函数采用最小化误差项的二范数平方和的方式，**存在的问题**是误匹配导致优化时对误匹配的误差进行了优化从而忽略了对正确匹配的误差的优化，这是由于误差很大时二范数增长太快。**解决方法**采用核函数，保证每个数据误差不会因为大得没边而掩盖其它数据误差。核函数的性质：1、增长不快，2、函数光滑（可导），例如Huber核函数：\n$$\n\\begin{equation}\nH(e)=\\left\\{\\begin{aligned}\n&\\frac{1}{2}e^2 & &当|e|\\le\\delta,\\\\\n&\\delta(|e|-\\frac{1}{2}\\delta) & &其它\n\\end{aligned}\\right.\n\\end{equation}\n$$\n<img src=\"image-20221108141628283.png\" alt=\"huber核函数\" style=\"zoom:33%;\" />\n\n","source":"_posts/slam.md","raw":"---\ntypora-root-url: ./slam\ntitle: slam\nmathjax: true\ndate: 2022-11-04 09:21:41\ncategories:\n- 建图和定位\ntags:\n- slam14讲\n---\n\n# 后端\n\n## 状态估计概率解释\n\n视觉里程计只有短暂记忆，而我们希望整个运动轨迹在较长时间内都能保持最优的状态。用最新的知识更新较久远的状态。这是个状态估计问题。\n$$\n状态估计方式\\left\\{\\begin{align*}\n\t&批量式(\\mathrm{Batch}): 过去信息+当前信息+未来信息更新状态 \\\\\n\t&渐进式(\\mathrm{Incremental}): 只由过去信息更新状态\n\\end{align*}\\right.\n$$\n假设$t=0$到$t=N$时间内，有位姿$x_0$到$x_N$，并有路标$y_0$，...，$y_M$，运动方程和观测方程为\n$$\n\\begin{equation}\n\\left\\{\\begin{aligned}\n\t&\\vec{x}_k=f(\\vec{x}_{k-1},\\vec{u}_k)+\\vec{w}_k \\\\\n\t&\\vec{z}_{k,j}=h(\\vec{y}_j,\\vec{x}_k)+\\vec{v}_{k,j}\n\\end{aligned}\\right.\\quad\\quad k=1,\\cdots,N,\\quad j=1,\\cdots,M.\n\\end{equation}\n$$\n特点：\n- 运动方程数量小于甚至远小于观测方程\n- 没有运动方程相当于通过一组时序图像来恢复运动和结构\n\n方程中的位姿$\\vec{x}$和路标$\\vec{y}$受噪声影响，所以应该看成服从某种概率分布的随机变量，而不是一个数。\n\n**状态估计中的观测量和状态量**：\n$$\n\\left\\{\\begin{align*}\n\t&状态量：位姿\\vec{x}，路标\\vec{y} \\\\\n\t&观测量：运动输入数据\\vec{u}，观测数据\\vec{z}\n\\end{align*}\\right.\n$$\n\n**重新定义状态量**：$\\vec{x}_k\\overset{def}{=}\\{\\vec{x}_k,\\vec{y}_1,\\cdots,\\vec{y}_m\\}$表示k时刻状态量$\\vec{x}_k$由k时刻位姿$\\vec{x}_k$及$m$个路标$\\vec{y}_*$组成。\n\n**重新定义观测数据**：$\\vec{z}_k\\overset{def}{=}\\{\\vec{z}_{k,1},\\cdots,\\vec{z}_{k,m}\\}$表示k时刻对m个路标的观测数据\n\n**新的运动和观测方程**\n$$\n\\begin{equation}\n\\label{eq:kinemic_and_observation}\n\\left\\{\\begin{aligned}\n\t&\\vec{x}_k=f(\\vec{x}_{k-1},\\vec{u}_k)+\\vec{w}_k\\\\\n\t&\\vec{z}_{k}=h(\\vec{x}_k)+\\vec{v}_{k}\n\\end{aligned}\\right.\\quad\\quad k=1,\\cdots,N.\n\\end{equation}\n$$\n\n$k$时刻状态量几率函数用$0$到$k$时刻的观测量来估计，即**后验分布**记为$P(\\vec{x}_k|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k})$, 这里$\\vec{u}_{1:k}=\\{\\vec{u}_1,\\vec{u}_2,\\cdots,\\vec{u}_k\\}$，$\\vec{z}_{1:k}=\\{\\vec{z}_1,\\vec{z}_2,\\cdots,\\vec{z}_k\\}$，根据贝叶斯法则，将后验分布分解成**似然分布**和**先验分布**的乘积\n\n$$\n\\begin{equation}\nP(\\vec{x}_k|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k})\\propto P(\\vec{z}_k|\\vec{x}_k)P(\\vec{x}_k|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})\n\\end{equation}\n$$\n\n**先验分布**是基于过去所有状态估计得来，简化只受到$\\vec{x}_{k-1}$影响，联合分布$P(\\vec{x}_k，\\vec{x}_{k-1}|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})$对$\\vec{x}_{k-1}$积分得到边缘分布$P(\\vec{x}_k|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})$即**先验分布**，联合分布可以通过条件概率公式转化为$\\vec{x}_{k-1}$的条件概率形式，即$P(\\vec{x}_k|\\vec{x}_{k-1},\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})P(\\vec{x}_{k-1}|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})$，因此**先验分布**可以表示成：\n$$\n\\begin{equation}\\label{eq:xianyan}\nP(\\vec{x}_k|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})=\\int P(\\vec{x}_k|\\vec{x}_{k-1},\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})P(\\vec{x}_{k-1}|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})\\,\\mathrm{d}\\vec{x}_{k-1}\n\\end{equation}\n$$\n\n先验分布的处理方式：\n\n$$\n\\left\\{\\begin{align*}\n&k时刻状态\\vec{x}_k的分布只和k-1时刻的状态\\vec{x}_{k-1}有关即马尔可夫假设&&: 扩展卡尔曼滤波(EKF) \\\\\n&k时刻状态\\vec{x}_k的分布与之前所有状态\\vec{x}_{1:k}有关&&: 非线性优化\n\\end{align*}\\right.\n$$\n\n## 线性系统和卡尔曼滤波（KF）\n\n根据k时刻状态$\\vec{x}_k$的分布只和k-1时刻的状态$\\vec{x}_{k-1}$有关，所以可以去掉$\\vec{x}_{0:k-2}$这些状态量，k时刻状态量$\\vec{x}_k$和k时刻运动输入量$\\vec{u}_k$以及观测量$\\vec{z}_k$有关和其它时刻没关系，公式$\\ref{eq:xianyan}$中右侧第一部分可简化为\n\n$$\n\\begin{equation}\nP(\\vec{x}_k|\\vec{x}_{k-1},\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})=P(\\vec{x}_k|\\vec{x}_{k-1},\\vec{u}_k)\n\\end{equation}\n$$\n\n由于k-1时刻状态量$\\vec{x}_k$还是按照和之前状态都有关的假设，公式$\\ref{eq:xianyan}$中右侧第二部分可简化为\n\n$$\n\\begin{equation}\n\\label{eq:k-1zhuang_tai_liang}\nP(\\vec{x}_{k-1}|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})=P(\\vec{x}_{k-1}|\\vec{x}_{0},\\vec{u}_{1:k-1},\\vec{z}_{1:k-1})\n\\end{equation}\n$$\n\n公式$\\ref{eq:k-1zhuang_tai_liang}$就是k-1时刻的状态分布。\n\n**线性高斯系统**\n\n将公式$\\ref{eq:kinemic_and_observation}$中的运动方程和观测方程按照线性函数的形式展开\n\n$$\n\\begin{equation}\n\\left\\{\\begin{aligned}\n\t&\\vec{x}_k=A_k\\vec{x}_{k-1}+\\vec{u}_k+\\vec{w}_k \\\\\n\t&\\vec{z}_{k}=C_k\\vec{x}_k+\\vec{v}_{k}\n\\end{aligned}\\right.\\quad\\quad k=1,\\cdots,N.\n\\end{equation}\n$$\n\n运动噪声和观测噪声都服从0均值正态分布：\n\n$$\n\\begin{equation}\n\\vec{w}_k\\sim N(0,R_{k})\\quad\\quad\\quad \\vec{v}_k\\sim N(0,Q_{k})\n\\end{equation}\n$$\n\n**记号区分**：上帽子$\\hat{\\vec{x}}_k$表示后验，下帽子$\\check{\\vec{x}}_k$表示先验\n\n**已知**k-1时刻的后验状态估计为$\\hat{\\vec{x}}_{k-1}$及其协方差$\\hat{P}_{k-1}$，状态估计服从高斯分布，**求**k时刻的状态估计及其协方差\n\n**运动方程**确定$\\vec{x}_k$的**先验分布**（随机变量函数的分布函数），扰动传递规则\n$$\n\\begin{equation}\nP(\\vec{x}_k|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})=N(A_k\\hat{\\vec{x}}_{k-1}+\\vec{u}_k,A_k\\hat{P}_{k-1}A^T_k+R_{k})\n\\end{equation}\n$$\n**先验分布**得到k时刻状态估计，记为$\\check{\\vec{x}}=A_k\\hat{\\vec{x}}_{k-1}+\\vec{u}_k$，$\\check{P}_k=A_k\\hat{P}_{k-1}A_k^T+R_k$\n\n**观测方程**确定状态量的**似然函数**$P(\\vec{z}_k|\\vec{x}_k)=N(C_k\\vec{x}_k, Q_k)$\n\n**贝叶斯公式**确定状态量的**后验分布**\n$$\n\\begin{equation}\nN(\\hat{\\vec{x}}_k,\\hat{P}_k)=\\eta N(C_k\\vec{x}_k,Q_k)\\cdot N(\\check{\\vec{x}}_k,\\check{P}_k)\n\\end{equation}\n$$\n\n归纳为：预测和更新两步\n1. 预测\n$$\n\\begin{equation}\n\\check{\\vec{x}}_k=A_k\\hat{\\vec{x}}_{k-1}+\\vec{u}_k,\\quad\\quad \\check{P}_k=A_k\\hat{P}_{k-1}A^T_k+R_k\n\\end{equation}\n$$\n2. 更新：\n2.1.  先计算$K_k$，称为卡尔曼增益\n$$\n\\begin{equation}\nK=\\check{P}_kC_k^T(C_k\\check{P}_kC_k^T+Q_k)^{-1}\n\\end{equation}\n$$\n​      2.2. 计算后验分布的估计量\n$$\n\\begin{equation}\n\\hat{\\vec{x}}_k=\\check{\\vec{x}}_k+K_k(\\vec{z}_k-C_k\\check{\\vec{x}}_k), \\quad\\quad \\hat{P}_k=(I-K_kC_k)\\check{P}_k\n\\end{equation}\n$$\n\n\n## 非线性系统和扩展卡尔曼滤波（EKF）\n\nSLAM中的运动观测方程是非线性函数\n$$\n\\left\\{\\begin{align*}\n&高斯分布\\xrightarrow{线性变换}高斯分布\\\\\n&高斯分布\\xrightarrow{非线性变换}非高斯分布\n\\end{align*}\\right.\n$$\n解决方法：在状态量附近对**运动方程**和**观测方程**进行泰勒展开，保留一阶项，从而得到近似的线性部分，再按照线性系统进行推到。\n\n**已知**k-1时刻的后验状态估计为$\\hat{\\vec{x}}_{k-1}$及其协方差$\\hat{P}_{k-1}$，状态估计服从高斯分布，**求**k时刻的状态估计及其协方差，将**运动方程**和**观测方程**在$\\hat{\\vec{x}}_{k-1}$和$\\hat{P}_k$处进行**线性化**。\n\n对运动方程\n$$\n\\begin{equation}\n\\vec{x}_k\\approx f(\\hat{\\vec{x}}_{k-1},\\vec{u}_k)+\\left.\\frac{\\partial f}{\\partial \\vec{x}_{k-1}}\\right|_{\\hat{\\vec{x}}_{k-1}}(\\vec{x}_{k-1}-\\hat{\\vec{x}}_{k-1})+\\vec{w}_k\n\\end{equation}\n$$\n记偏导数为\n$$\n\\begin{equation}\nF=\\left.\\frac{\\partial f}{\\partial \\vec{x}_{k-1}}\\right|_{\\hat{\\vec{x}}_{k-1}}\n\\end{equation}\n$$\n对观测方程\n$$\n\\begin{equation}\n\\vec{z}_k\\approx h(\\check{\\vec{x}}_{k})+\\left.\\frac{\\partial h}{\\partial \\vec{x}_{k}}\\right|_{\\check{\\vec{x}}_{k}}(\\vec{x}_{k}-\\check{\\vec{x}}_{k-1})+\\vec{v}_k\n\\end{equation}\n$$\n记偏导数为\n$$\n\\begin{equation}\nH=\\left.\\frac{\\partial h}{\\partial \\vec{x}_{k}}\\right|_{\\check{\\vec{x}}_{k}}\n\\end{equation}\n$$\n\n仿照线性变换部分计算先验、似然和后验分布，根据分布求状态量的估计值\n归纳为：预测和更新两步\n1. 预测\n$$\n\\begin{equation}\n\\check{\\vec{x}}_k=f(\\hat{\\vec{x}}_{k-1},\\vec{u}_k),\\quad\\quad \\check{P}_k=F\\hat{P}_{k-1}F^T+R_k\n\\end{equation}\n$$\n2. 更新：\n2.1.  先计算$K_k$，称为卡尔曼增益\n$$\n\\begin{equation}\nK_k=\\check{P}_kH^T(H\\check{P}_kH^T+Q_k)^{-1}\n\\end{equation}\n$$\n​      2.2. 计算后验分布的估计量\n$$\n\\begin{equation}\n\\hat{\\vec{x}}_k=\\check{\\vec{x}}_k+K_k(\\vec{z}_k-h(\\check{\\vec{x}}_k)), \\quad\\quad \\hat{P}_k=(I-K_kH)\\check{P}_k\n\\end{equation}\n$$\n\n\n## 非线性优化：Bundle Adjustment（BA）\n\n1. 世界坐标系下的点记作$$\\vec{p}_w=[X_w,Y_w,Z_w]^T$$，\n2. 相机坐标系下的点$$\\vec{p}_c=[X_c,Y_c,Z_c]^T$$，\n3. 归一化平面上的点$$\\vec{p}_{c_n}=[u_c,v_c,1]^T=[X_c/Z_c,Y_c/Z_c,1]^T$$\n4. 归一化坐标的畸变情况，还原畸变矫正前的原始投影坐标，只考虑径向畸变\n$$\n\\begin{equation}\n\\left\\{\\begin{aligned}\n&u_c^\\prime =u_c(1+k_1r_c^2+k_2r_c^4)\\\\\n&v_c^\\prime =v_c(1+k_1r_c^2+k_2r_c^4)\n\\end{aligned}\\right. \\quad\\quad r_c^2=u_c^2+v_c^2\n\\end{equation}\n$$\n5. 根据相机内参（焦距，像素平面的原点坐标）计算像素坐标\n$$\n\\begin{equation}\n\\left\\{\\begin{aligned}\n&u_s=f_xu_c^\\prime+c_x\\\\\n&v_s=f_yu_c^\\prime+c_y\n\\end{aligned}\\right.\n\\end{equation}\n$$\n\n在**i时刻**，$\\vec{x_i}$指代相机**位姿**即$R_i，\\vec{t_i}$，其对应的李群为$T_i$，李代数为$\\vec{\\xi}_i$。**j路标**$\\vec{y}_j$即三维点$\\vec{p}_j$，**观测数据**是像素坐标$\\vec{z}_{i,j}\\overset{def}{=}[u_{s_{i,j}},v_{s_{i,j}}]^T$。观测误差\n$$\n\\begin{equation}\n\\vec{e_{i,j}}=\\vec{z}_{i,j}-h(T_i,\\vec{p}_j)\n\\end{equation}\n$$\n个时刻对所有路标观测的整体误差\n$$\n\\begin{equation}\n\\frac{1}{2}\\sum_{i=i}^{m}\\sum_{j=1}^{n}||\\vec{e}_{i,j}||^2=\\frac{1}{2}\\sum_{i=i}^{m}\\sum_{j=1}^{n}||\\vec{z}_{i,j}-h(T_i,\\vec{p}_j)||^2\n\\end{equation}\n$$\n\n\n### 求解BA\n\nBA目标函数的**自变量**定义成\n$$\n\\begin{equation}\n\\vec{x}=[T_1,\\cdots,T_m,\\vec{p}_1,\\cdots,\\vec{p}_n]^T\n\\end{equation}\n$$\n给自变量一个增量$\\Delta \\vec{x}$\n$$\n\\begin{align}\\label{eq:BA:object_fun}\n\\frac{1}{2}||f(\\vec{x}+\\Delta\\vec{x})||^2&\\approx\\frac{1}{2}\\sum_{i=i}^{m}\\sum_{j=1}^{n}||\\vec{e_{i,j}}+F_{i,j}\\Delta\\vec{\\xi}_{i}+E_{i,j}\\Delta\\vec{p}_j)||^2 \\\\\nF_{i,j}&=\\frac{\\partial \\vec{e}_{i,j}}{\\partial \\vec{\\xi}_i}\\\\\nE_{i,j}&=\\frac{\\partial \\vec{e}_{i,j}}{\\partial \\vec{p}_j}\n\\end{align}\n$$\n将**位姿**变量放在一起，**路标**变量放在一起，记作：\n$$\n\\begin{equation}\n\\vec{x}_c=[\\vec{\\xi}_1,\\vec{\\xi}_2,\\cdots,\\vec{\\xi}_m]^T\\in\\mathbb{R}^{6m}\n\\end{equation}\n$$\n$$\n\\begin{equation}\n\\vec{x}_p=[\\vec{p}_1,\\vec{p}_2,\\cdots,\\vec{p}_n]^T\\in\\mathbb{R}^{3m}\n\\end{equation}\n$$\n公式$\\ref{eq:BA:object_fun}$记作：\n$$\n\\begin{equation}\n\\frac{1}{2}||f(\\vec{x}+\\Delta\\vec{x})||^2\\approx\\frac{1}{2}\\sum_{i=i}^{m}\\sum_{j=1}^{n}||\\vec{e}+F\\Delta\\vec{x}_{c}+E\\Delta\\vec{x}_p)||^2\\quad这里的E和F是由若干小矩阵块E_{i,j}和F_{i,j}各自拼凑起来的\n\\end{equation}\n$$\n无论**高斯牛顿法**还是**列文伯格-马夸尔特方法**增量线性方程都是：\n$$\n\\begin{equation}\nH\\Delta\\vec{x}=\\vec{g}\n\\end{equation}\n$$\n$$\n区别: H\\left\\{\\begin{align*}\n&J^TJ & &高斯牛顿法\\\\\n&J^TJ+\\lambda I & &列文伯格-马夸尔特方法\n\\end{align*}\\right.\n$$\n注：有些博客用$H=J^T\\Omega J$,$\\Omega$应该是正态分布协方差的逆矩阵$\\Sigma^{-1}$，它是2x2矩阵，可以从$\\vec{e}$是二维向量推出,已将自变量归类成位姿和空间点两种形式，雅可比矩阵亦分块成\n$$\n\\begin{equation}\nJ=[F \\quad E]\n\\end{equation}\n$$\n\n求高斯牛顿$H$，逆运算复杂度$O(n^3)$，但$H$是有稀疏矩阵结构\n$$\n\\begin{equation}\nH=J^TJ=\\left[\\begin{aligned}\n&F^TF & &F^TE\\\\\n&E^TF & &E^TE\n\\end{aligned}\\right]\n\\end{equation}\n$$\n\n\n### 利用稀疏性和边缘化求解BA\n\n雅可比矩阵的结构\n$$\n\\begin{equation}\nJ_{i,j}(\\vec{x})=\\left(\\vec{0}_{2\\times6},\\cdots,\\vec{0}_{2\\times6},\\frac{\\partial\\vec{e}_{i,j}}{\\partial T_i},\\vec{0}_{2\\times6},\\cdots\\left|\\vec{0}_{2\\times3},\\cdots,\\vec{0}_{2\\times3},\\frac{\\partial\\vec{e}_{i,j}}{\\partial \\vec{p}_j},\\vec{0}_{2\\times3},\\cdots,\\vec{0}_{2\\times3}\\right.\\right)\n\\end{equation}\n$$\n![稀疏结构](image-20221107155224157.png)\n\n***\n举例：考虑一个场景内有2个相机位姿($C_1$,$C_2$)和6个路标点($P_1$,$P_2$,$P_3$,$P_4$,$P_5$,$P_6$)，相机在位姿$C_1$处观察到路标$P_1$,$P_2$,$P_3$,$P_4$，在位姿$C_2$处观察到路标$P_3$,$P_4$,$P_5$,$P_6$，如图![举例示例](image-20221107161443842.png)\n该场景下的BA目标函数\n$$\n\\begin{equation}\n\\frac{1}{2}\\left(||\\vec{e}_{11}||^2+||\\vec{e}_{12}||^2+||\\vec{e}_{13}||^2+||\\vec{e}_{14}||^2+||\\vec{e}_{23}||^2+||\\vec{e}_{24}||^2+||\\vec{e}_{25}||^2+||\\vec{e}_{26}||^2\\right)\n\\end{equation}\n$$\n$\\vec{e}_{11}$描述了$C_1$看到$P_1$这件事，与其它位姿和路标无关，所以对它们求导为0，按照变量$\\vec{x}=(\\vec{\\xi}_1,\\vec{\\xi}_2,\\vec{p}_1,\\cdots,\\vec{p}_6)$，有\n$$\n\\begin{equation}\nJ_{11}=\\frac{\\partial \\vec{e}_{11}}{\\partial \\vec{x}}=\\left(\\frac{\\vec{e}_{11}}{\\partial\\vec{\\xi}_1},\\vec{0}_{2\\times6},\\frac{\\vec{e}_{11}}{\\partial\\vec{p}_1},\\vec{0}_{2\\times3},\\vec{0}_{2\\times3},\\vec{0}_{2\\times3},\\vec{0}_{2\\times3},\\vec{0}_{2\\times3}\\right)\n\\end{equation}\n$$\n形状如图![分块雅可比矩阵稀疏结构](image-20221107162453662.png)\n拼接$J_{ij}$可以得到整体雅可比矩阵和$H$矩阵如图![完整雅可比矩阵稀疏结构](image-20221107162722048.png)\n\n***\n**图优化结构和增量方程的稀疏性存在联系**：$H$矩阵中的非对角部分的非零矩阵块可以理解为其对应的位姿和路标变量之间存在联系也可称为约束，如图![增量矩阵和图优化结构的联系](image-20221107163948504.png)\n当有m个相机位姿和n个路标点时，且路标数量远大于位姿数量，即$n\\gg m$，$H$矩阵左上角区域会非常小，右下脚区域非常大，非对角部分分布散乱的约束块，形状如箭头\n\n<center><img src=\"image-20221107164511662.png\" alt=\"一般情况下H矩阵块\" style=\"zoom: 33%;\" /></center>\n\n**Schur消元**即**Marginalization边缘化**加速计算稀疏矩阵$H$，$H$矩阵分成4块，分别为$B,E,E^T,C$如图。\n<center><img src=\"image-20221107165342106.png\" alt=\"H矩阵分块\" style=\"zoom: 33%;\"  /></center>\n$$\n\\begin{equation}\\label{eq:Joint_probability_distribution}\n\\left[\\begin{aligned}\n&B & &E\\\\\n&E^T & &C\n\\end{aligned}\\right]\n\\left[\\begin{aligned}\n&\\Delta\\vec{x}_c\\\\\n&\\Delta\\vec{x}_p\n\\end{aligned}\\right]\n=\n\\left[\\begin{aligned}\n&\\vec{v}\\\\\n&\\vec{w}\n\\end{aligned}\\right]\n\\end{equation}\n$$\n其中$B$是对角块矩阵，每个对角块的维度和相机位姿的维度相同，对角块的个数和位姿的个数相同(李群：9维，李代数：6维)；同理$C$是对角块矩阵，每个对角块的维度和路标的维度(三维)相同，对角块的个数和路标个数相同， 公式$\\ref{eq:Joint_probability_distribution}$可以理解成**联合概率分布**。\n\n**边缘化**即消去路标变量对应的系数矩阵，使方程中只含有位姿变量和其系数矩阵\n$$\n\\begin{equation}\n\\left[\\begin{aligned}\n&I & &-EC^{-1}\\\\\n&[0]_{n\\times m} & &I\n\\end{aligned}\\right]\n\\left[\\begin{aligned}\n&B & &E\\\\\n&E^T & &C\n\\end{aligned}\\right]\n\\left[\\begin{aligned}\n&\\Delta\\vec{x}_c\\\\\n&\\Delta\\vec{x}_p\n\\end{aligned}\\right]\n=\n\\left[\\begin{aligned}\n&I & &-EC^{-1}\\\\\n&[0]_{n\\times m} & &I\n\\end{aligned}\\right]\n\\left[\\begin{aligned}\n&\\vec{v}\\\\\n&\\vec{w}\n\\end{aligned}\\right]\n\\end{equation}\n$$\n\n位姿部分的增量方程为\n$$\n\\begin{equation}\n\\label{eq:marginalization}\n\\left[B-EC^{-1}\\right]\\Delta \\vec{x}_c=\\vec{v}-EC^{-1}\\vec{w}\n\\end{equation}\n$$\n这可以理解成$\\Delta\\vec{x}_c$的**先验函数**，求解$\\Delta\\vec{x}_c$后再求解$\\Delta\\vec{x}_p$，求解$\\Delta\\vec{x}_p$的方程可以理解成**条件概率**（已知$\\Delta\\vec{x}_c$），利用对角块矩阵$C$的逆运算容易求解\n$$\n\\begin{equation}\n\\Delta\\vec{x}_p=C^{-1}(\\vec{w}-E^T\\Delta\\vec{x}_c)\n\\end{equation}\n$$\n\n公式$\\ref{eq:marginalization}$的系数矩阵$\\left[B-EC^{-1}\\right]\\Delta \\vec{x}_c$记作$S$，它的稀疏性是不规则的，如图\n<center><img src=\"image-20221108112428467.png\" alt=\"边缘化系数矩阵稀疏结构\" style=\"zoom: 33%;\" /></center>\n\n如同$H$矩阵的**非对角块处的非零块**对应着**相机位姿**和**路标**的联系，$S$矩阵的**非对角块处的非零块**对应着**相机的两个位姿**之间存在着**共同观测的路标**也称**共视路标**\n\n### 鲁棒核函数\n\n目标函数采用最小化误差项的二范数平方和的方式，**存在的问题**是误匹配导致优化时对误匹配的误差进行了优化从而忽略了对正确匹配的误差的优化，这是由于误差很大时二范数增长太快。**解决方法**采用核函数，保证每个数据误差不会因为大得没边而掩盖其它数据误差。核函数的性质：1、增长不快，2、函数光滑（可导），例如Huber核函数：\n$$\n\\begin{equation}\nH(e)=\\left\\{\\begin{aligned}\n&\\frac{1}{2}e^2 & &当|e|\\le\\delta,\\\\\n&\\delta(|e|-\\frac{1}{2}\\delta) & &其它\n\\end{aligned}\\right.\n\\end{equation}\n$$\n<img src=\"image-20221108141628283.png\" alt=\"huber核函数\" style=\"zoom:33%;\" />\n\n","slug":"slam","published":1,"updated":"2022-11-11T09:09:54.497Z","_id":"cla22wp820001qt3k8e0ademl","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"后端\"><a href=\"#后端\" class=\"headerlink\" title=\"后端\"></a>后端</h1><h2 id=\"状态估计概率解释\"><a href=\"#状态估计概率解释\" class=\"headerlink\" title=\"状态估计概率解释\"></a>状态估计概率解释</h2><p>视觉里程计只有短暂记忆，而我们希望整个运动轨迹在较长时间内都能保持最优的状态。用最新的知识更新较久远的状态。这是个状态估计问题。</p>\n<script type=\"math/tex; mode=display\">\n状态估计方式\\left\\{\\begin{align*}\n    &批量式(\\mathrm{Batch}): 过去信息+当前信息+未来信息更新状态 \\\\\n    &渐进式(\\mathrm{Incremental}): 只由过去信息更新状态\n\\end{align*}\\right.</script><p>假设$t=0$到$t=N$时间内，有位姿$x_0$到$x_N$，并有路标$y_0$，…，$y_M$，运动方程和观测方程为</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\left\\{\\begin{aligned}\n    &\\vec{x}_k=f(\\vec{x}_{k-1},\\vec{u}_k)+\\vec{w}_k \\\\\n    &\\vec{z}_{k,j}=h(\\vec{y}_j,\\vec{x}_k)+\\vec{v}_{k,j}\n\\end{aligned}\\right.\\quad\\quad k=1,\\cdots,N,\\quad j=1,\\cdots,M.\n\\end{equation}</script><p>特点：</p>\n<ul>\n<li>运动方程数量小于甚至远小于观测方程</li>\n<li>没有运动方程相当于通过一组时序图像来恢复运动和结构</li>\n</ul>\n<p>方程中的位姿$\\vec{x}$和路标$\\vec{y}$受噪声影响，所以应该看成服从某种概率分布的随机变量，而不是一个数。</p>\n<p><strong>状态估计中的观测量和状态量</strong>：</p>\n<script type=\"math/tex; mode=display\">\n\\left\\{\\begin{align*}\n    &状态量：位姿\\vec{x}，路标\\vec{y} \\\\\n    &观测量：运动输入数据\\vec{u}，观测数据\\vec{z}\n\\end{align*}\\right.</script><p><strong>重新定义状态量</strong>：$\\vec{x}_k\\overset{def}{=}\\{\\vec{x}_k,\\vec{y}_1,\\cdots,\\vec{y}_m\\}$表示k时刻状态量$\\vec{x}_k$由k时刻位姿$\\vec{x}_k$及$m$个路标$\\vec{y}_*$组成。</p>\n<p><strong>重新定义观测数据</strong>：$\\vec{z}_k\\overset{def}{=}\\{\\vec{z}_{k,1},\\cdots,\\vec{z}_{k,m}\\}$表示k时刻对m个路标的观测数据</p>\n<p><strong>新的运动和观测方程</strong></p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\label{eq:kinemic_and_observation}\n\\left\\{\\begin{aligned}\n    &\\vec{x}_k=f(\\vec{x}_{k-1},\\vec{u}_k)+\\vec{w}_k\\\\\n    &\\vec{z}_{k}=h(\\vec{x}_k)+\\vec{v}_{k}\n\\end{aligned}\\right.\\quad\\quad k=1,\\cdots,N.\n\\end{equation}</script><p>$k$时刻状态量几率函数用$0$到$k$时刻的观测量来估计，即<strong>后验分布</strong>记为$P(\\vec{x}_k|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k})$, 这里$\\vec{u}_{1:k}=\\{\\vec{u}_1,\\vec{u}_2,\\cdots,\\vec{u}_k\\}$，$\\vec{z}_{1:k}=\\{\\vec{z}_1,\\vec{z}_2,\\cdots,\\vec{z}_k\\}$，根据贝叶斯法则，将后验分布分解成<strong>似然分布</strong>和<strong>先验分布</strong>的乘积</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\nP(\\vec{x}_k|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k})\\propto P(\\vec{z}_k|\\vec{x}_k)P(\\vec{x}_k|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})\n\\end{equation}</script><p><strong>先验分布</strong>是基于过去所有状态估计得来，简化只受到$\\vec{x}_{k-1}$影响，联合分布$P(\\vec{x}_k，\\vec{x}_{k-1}|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})$对$\\vec{x}_{k-1}$积分得到边缘分布$P(\\vec{x}_k|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})$即<strong>先验分布</strong>，联合分布可以通过条件概率公式转化为$\\vec{x}_{k-1}$的条件概率形式，即$P(\\vec{x}_k|\\vec{x}_{k-1},\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})P(\\vec{x}_{k-1}|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})$，因此<strong>先验分布</strong>可以表示成：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\\label{eq:xianyan}\nP(\\vec{x}_k|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})=\\int P(\\vec{x}_k|\\vec{x}_{k-1},\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})P(\\vec{x}_{k-1}|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})\\,\\mathrm{d}\\vec{x}_{k-1}\n\\end{equation}</script><p>先验分布的处理方式：</p>\n<script type=\"math/tex; mode=display\">\n\\left\\{\\begin{align*}\n&k时刻状态\\vec{x}_k的分布只和k-1时刻的状态\\vec{x}_{k-1}有关即马尔可夫假设&&: 扩展卡尔曼滤波(EKF) \\\\\n&k时刻状态\\vec{x}_k的分布与之前所有状态\\vec{x}_{1:k}有关&&: 非线性优化\n\\end{align*}\\right.</script><h2 id=\"线性系统和卡尔曼滤波（KF）\"><a href=\"#线性系统和卡尔曼滤波（KF）\" class=\"headerlink\" title=\"线性系统和卡尔曼滤波（KF）\"></a>线性系统和卡尔曼滤波（KF）</h2><p>根据k时刻状态$\\vec{x}_k$的分布只和k-1时刻的状态$\\vec{x}_{k-1}$有关，所以可以去掉$\\vec{x}_{0:k-2}$这些状态量，k时刻状态量$\\vec{x}_k$和k时刻运动输入量$\\vec{u}_k$以及观测量$\\vec{z}_k$有关和其它时刻没关系，公式$\\ref{eq:xianyan}$中右侧第一部分可简化为</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\nP(\\vec{x}_k|\\vec{x}_{k-1},\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})=P(\\vec{x}_k|\\vec{x}_{k-1},\\vec{u}_k)\n\\end{equation}</script><p>由于k-1时刻状态量$\\vec{x}_k$还是按照和之前状态都有关的假设，公式$\\ref{eq:xianyan}$中右侧第二部分可简化为</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\label{eq:k-1zhuang_tai_liang}\nP(\\vec{x}_{k-1}|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})=P(\\vec{x}_{k-1}|\\vec{x}_{0},\\vec{u}_{1:k-1},\\vec{z}_{1:k-1})\n\\end{equation}</script><p>公式$\\ref{eq:k-1zhuang_tai_liang}$就是k-1时刻的状态分布。</p>\n<p><strong>线性高斯系统</strong></p>\n<p>将公式$\\ref{eq:kinemic_and_observation}$中的运动方程和观测方程按照线性函数的形式展开</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\left\\{\\begin{aligned}\n    &\\vec{x}_k=A_k\\vec{x}_{k-1}+\\vec{u}_k+\\vec{w}_k \\\\\n    &\\vec{z}_{k}=C_k\\vec{x}_k+\\vec{v}_{k}\n\\end{aligned}\\right.\\quad\\quad k=1,\\cdots,N.\n\\end{equation}</script><p>运动噪声和观测噪声都服从0均值正态分布：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\vec{w}_k\\sim N(0,R_{k})\\quad\\quad\\quad \\vec{v}_k\\sim N(0,Q_{k})\n\\end{equation}</script><p><strong>记号区分</strong>：上帽子$\\hat{\\vec{x}}_k$表示后验，下帽子$\\check{\\vec{x}}_k$表示先验</p>\n<p><strong>已知</strong>k-1时刻的后验状态估计为$\\hat{\\vec{x}}_{k-1}$及其协方差$\\hat{P}_{k-1}$，状态估计服从高斯分布，<strong>求</strong>k时刻的状态估计及其协方差</p>\n<p><strong>运动方程</strong>确定$\\vec{x}_k$的<strong>先验分布</strong>（随机变量函数的分布函数），扰动传递规则</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\nP(\\vec{x}_k|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})=N(A_k\\hat{\\vec{x}}_{k-1}+\\vec{u}_k,A_k\\hat{P}_{k-1}A^T_k+R_{k})\n\\end{equation}</script><p><strong>先验分布</strong>得到k时刻状态估计，记为$\\check{\\vec{x}}=A_k\\hat{\\vec{x}}_{k-1}+\\vec{u}_k$，$\\check{P}_k=A_k\\hat{P}_{k-1}A_k^T+R_k$</p>\n<p><strong>观测方程</strong>确定状态量的<strong>似然函数</strong>$P(\\vec{z}_k|\\vec{x}_k)=N(C_k\\vec{x}_k, Q_k)$</p>\n<p><strong>贝叶斯公式</strong>确定状态量的<strong>后验分布</strong></p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\nN(\\hat{\\vec{x}}_k,\\hat{P}_k)=\\eta N(C_k\\vec{x}_k,Q_k)\\cdot N(\\check{\\vec{x}}_k,\\check{P}_k)\n\\end{equation}</script><p>归纳为：预测和更新两步</p>\n<ol>\n<li>预测<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\check{\\vec{x}}_k=A_k\\hat{\\vec{x}}_{k-1}+\\vec{u}_k,\\quad\\quad \\check{P}_k=A_k\\hat{P}_{k-1}A^T_k+R_k\n\\end{equation}</script></li>\n<li>更新：<br>2.1.  先计算$K_k$，称为卡尔曼增益<script type=\"math/tex; mode=display\">\n\\begin{equation}\nK=\\check{P}_kC_k^T(C_k\\check{P}_kC_k^T+Q_k)^{-1}\n\\end{equation}</script>​      2.2. 计算后验分布的估计量<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\hat{\\vec{x}}_k=\\check{\\vec{x}}_k+K_k(\\vec{z}_k-C_k\\check{\\vec{x}}_k), \\quad\\quad \\hat{P}_k=(I-K_kC_k)\\check{P}_k\n\\end{equation}</script></li>\n</ol>\n<h2 id=\"非线性系统和扩展卡尔曼滤波（EKF）\"><a href=\"#非线性系统和扩展卡尔曼滤波（EKF）\" class=\"headerlink\" title=\"非线性系统和扩展卡尔曼滤波（EKF）\"></a>非线性系统和扩展卡尔曼滤波（EKF）</h2><p>SLAM中的运动观测方程是非线性函数</p>\n<script type=\"math/tex; mode=display\">\n\\left\\{\\begin{align*}\n&高斯分布\\xrightarrow{线性变换}高斯分布\\\\\n&高斯分布\\xrightarrow{非线性变换}非高斯分布\n\\end{align*}\\right.</script><p>解决方法：在状态量附近对<strong>运动方程</strong>和<strong>观测方程</strong>进行泰勒展开，保留一阶项，从而得到近似的线性部分，再按照线性系统进行推到。</p>\n<p><strong>已知</strong>k-1时刻的后验状态估计为$\\hat{\\vec{x}}_{k-1}$及其协方差$\\hat{P}_{k-1}$，状态估计服从高斯分布，<strong>求</strong>k时刻的状态估计及其协方差，将<strong>运动方程</strong>和<strong>观测方程</strong>在$\\hat{\\vec{x}}_{k-1}$和$\\hat{P}_k$处进行<strong>线性化</strong>。</p>\n<p>对运动方程</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\vec{x}_k\\approx f(\\hat{\\vec{x}}_{k-1},\\vec{u}_k)+\\left.\\frac{\\partial f}{\\partial \\vec{x}_{k-1}}\\right|_{\\hat{\\vec{x}}_{k-1}}(\\vec{x}_{k-1}-\\hat{\\vec{x}}_{k-1})+\\vec{w}_k\n\\end{equation}</script><p>记偏导数为</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\nF=\\left.\\frac{\\partial f}{\\partial \\vec{x}_{k-1}}\\right|_{\\hat{\\vec{x}}_{k-1}}\n\\end{equation}</script><p>对观测方程</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\vec{z}_k\\approx h(\\check{\\vec{x}}_{k})+\\left.\\frac{\\partial h}{\\partial \\vec{x}_{k}}\\right|_{\\check{\\vec{x}}_{k}}(\\vec{x}_{k}-\\check{\\vec{x}}_{k-1})+\\vec{v}_k\n\\end{equation}</script><p>记偏导数为</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\nH=\\left.\\frac{\\partial h}{\\partial \\vec{x}_{k}}\\right|_{\\check{\\vec{x}}_{k}}\n\\end{equation}</script><p>仿照线性变换部分计算先验、似然和后验分布，根据分布求状态量的估计值<br>归纳为：预测和更新两步</p>\n<ol>\n<li>预测<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\check{\\vec{x}}_k=f(\\hat{\\vec{x}}_{k-1},\\vec{u}_k),\\quad\\quad \\check{P}_k=F\\hat{P}_{k-1}F^T+R_k\n\\end{equation}</script></li>\n<li>更新：<br>2.1.  先计算$K_k$，称为卡尔曼增益<script type=\"math/tex; mode=display\">\n\\begin{equation}\nK_k=\\check{P}_kH^T(H\\check{P}_kH^T+Q_k)^{-1}\n\\end{equation}</script>​      2.2. 计算后验分布的估计量<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\hat{\\vec{x}}_k=\\check{\\vec{x}}_k+K_k(\\vec{z}_k-h(\\check{\\vec{x}}_k)), \\quad\\quad \\hat{P}_k=(I-K_kH)\\check{P}_k\n\\end{equation}</script></li>\n</ol>\n<h2 id=\"非线性优化：Bundle-Adjustment（BA）\"><a href=\"#非线性优化：Bundle-Adjustment（BA）\" class=\"headerlink\" title=\"非线性优化：Bundle Adjustment（BA）\"></a>非线性优化：Bundle Adjustment（BA）</h2><ol>\n<li>世界坐标系下的点记作<script type=\"math/tex\">\\vec{p}_w=[X_w,Y_w,Z_w]^T</script>，</li>\n<li>相机坐标系下的点<script type=\"math/tex\">\\vec{p}_c=[X_c,Y_c,Z_c]^T</script>，</li>\n<li>归一化平面上的点<script type=\"math/tex\">\\vec{p}_{c_n}=[u_c,v_c,1]^T=[X_c/Z_c,Y_c/Z_c,1]^T</script></li>\n<li>归一化坐标的畸变情况，还原畸变矫正前的原始投影坐标，只考虑径向畸变<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\left\\{\\begin{aligned}\n&u_c^\\prime =u_c(1+k_1r_c^2+k_2r_c^4)\\\\\n&v_c^\\prime =v_c(1+k_1r_c^2+k_2r_c^4)\n\\end{aligned}\\right. \\quad\\quad r_c^2=u_c^2+v_c^2\n\\end{equation}</script></li>\n<li>根据相机内参（焦距，像素平面的原点坐标）计算像素坐标<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\left\\{\\begin{aligned}\n&u_s=f_xu_c^\\prime+c_x\\\\\n&v_s=f_yu_c^\\prime+c_y\n\\end{aligned}\\right.\n\\end{equation}</script></li>\n</ol>\n<p>在<strong>i时刻</strong>，$\\vec{x_i}$指代相机<strong>位姿</strong>即$R_i，\\vec{t_i}$，其对应的李群为$T_i$，李代数为$\\vec{\\xi}_i$。<strong>j路标</strong>$\\vec{y}_j$即三维点$\\vec{p}_j$，<strong>观测数据</strong>是像素坐标$\\vec{z}_{i,j}\\overset{def}{=}[u_{s_{i,j}},v_{s_{i,j}}]^T$。观测误差</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\vec{e_{i,j}}=\\vec{z}_{i,j}-h(T_i,\\vec{p}_j)\n\\end{equation}</script><p>个时刻对所有路标观测的整体误差</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\frac{1}{2}\\sum_{i=i}^{m}\\sum_{j=1}^{n}||\\vec{e}_{i,j}||^2=\\frac{1}{2}\\sum_{i=i}^{m}\\sum_{j=1}^{n}||\\vec{z}_{i,j}-h(T_i,\\vec{p}_j)||^2\n\\end{equation}</script><h3 id=\"求解BA\"><a href=\"#求解BA\" class=\"headerlink\" title=\"求解BA\"></a>求解BA</h3><p>BA目标函数的<strong>自变量</strong>定义成</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\vec{x}=[T_1,\\cdots,T_m,\\vec{p}_1,\\cdots,\\vec{p}_n]^T\n\\end{equation}</script><p>给自变量一个增量$\\Delta \\vec{x}$</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\\label{eq:BA:object_fun}\n\\frac{1}{2}||f(\\vec{x}+\\Delta\\vec{x})||^2&\\approx\\frac{1}{2}\\sum_{i=i}^{m}\\sum_{j=1}^{n}||\\vec{e_{i,j}}+F_{i,j}\\Delta\\vec{\\xi}_{i}+E_{i,j}\\Delta\\vec{p}_j)||^2 \\\\\nF_{i,j}&=\\frac{\\partial \\vec{e}_{i,j}}{\\partial \\vec{\\xi}_i}\\\\\nE_{i,j}&=\\frac{\\partial \\vec{e}_{i,j}}{\\partial \\vec{p}_j}\n\\end{align}</script><p>将<strong>位姿</strong>变量放在一起，<strong>路标</strong>变量放在一起，记作：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\vec{x}_c=[\\vec{\\xi}_1,\\vec{\\xi}_2,\\cdots,\\vec{\\xi}_m]^T\\in\\mathbb{R}^{6m}\n\\end{equation}</script><script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\vec{x}_p=[\\vec{p}_1,\\vec{p}_2,\\cdots,\\vec{p}_n]^T\\in\\mathbb{R}^{3m}\n\\end{equation}</script><p>公式$\\ref{eq:BA:object_fun}$记作：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\frac{1}{2}||f(\\vec{x}+\\Delta\\vec{x})||^2\\approx\\frac{1}{2}\\sum_{i=i}^{m}\\sum_{j=1}^{n}||\\vec{e}+F\\Delta\\vec{x}_{c}+E\\Delta\\vec{x}_p)||^2\\quad这里的E和F是由若干小矩阵块E_{i,j}和F_{i,j}各自拼凑起来的\n\\end{equation}</script><p>无论<strong>高斯牛顿法</strong>还是<strong>列文伯格-马夸尔特方法</strong>增量线性方程都是：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\nH\\Delta\\vec{x}=\\vec{g}\n\\end{equation}</script><script type=\"math/tex; mode=display\">\n区别: H\\left\\{\\begin{align*}\n&J^TJ & &高斯牛顿法\\\\\n&J^TJ+\\lambda I & &列文伯格-马夸尔特方法\n\\end{align*}\\right.</script><p>注：有些博客用$H=J^T\\Omega J$,$\\Omega$应该是正态分布协方差的逆矩阵$\\Sigma^{-1}$，它是2x2矩阵，可以从$\\vec{e}$是二维向量推出,已将自变量归类成位姿和空间点两种形式，雅可比矩阵亦分块成</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\nJ=[F \\quad E]\n\\end{equation}</script><p>求高斯牛顿$H$，逆运算复杂度$O(n^3)$，但$H$是有稀疏矩阵结构</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\nH=J^TJ=\\left[\\begin{aligned}\n&F^TF & &F^TE\\\\\n&E^TF & &E^TE\n\\end{aligned}\\right]\n\\end{equation}</script><h3 id=\"利用稀疏性和边缘化求解BA\"><a href=\"#利用稀疏性和边缘化求解BA\" class=\"headerlink\" title=\"利用稀疏性和边缘化求解BA\"></a>利用稀疏性和边缘化求解BA</h3><p>雅可比矩阵的结构</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\nJ_{i,j}(\\vec{x})=\\left(\\vec{0}_{2\\times6},\\cdots,\\vec{0}_{2\\times6},\\frac{\\partial\\vec{e}_{i,j}}{\\partial T_i},\\vec{0}_{2\\times6},\\cdots\\left|\\vec{0}_{2\\times3},\\cdots,\\vec{0}_{2\\times3},\\frac{\\partial\\vec{e}_{i,j}}{\\partial \\vec{p}_j},\\vec{0}_{2\\times3},\\cdots,\\vec{0}_{2\\times3}\\right.\\right)\n\\end{equation}</script><p><img src=\"image-20221107155224157.png\" alt=\"稀疏结构\"></p>\n<hr>\n<p>举例：考虑一个场景内有2个相机位姿($C_1$,$C_2$)和6个路标点($P_1$,$P_2$,$P_3$,$P_4$,$P_5$,$P_6$)，相机在位姿$C_1$处观察到路标$P_1$,$P_2$,$P_3$,$P_4$，在位姿$C_2$处观察到路标$P_3$,$P_4$,$P_5$,$P_6$，如图<img src=\"image-20221107161443842.png\" alt=\"举例示例\"><br>该场景下的BA目标函数</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\frac{1}{2}\\left(||\\vec{e}_{11}||^2+||\\vec{e}_{12}||^2+||\\vec{e}_{13}||^2+||\\vec{e}_{14}||^2+||\\vec{e}_{23}||^2+||\\vec{e}_{24}||^2+||\\vec{e}_{25}||^2+||\\vec{e}_{26}||^2\\right)\n\\end{equation}</script><p>$\\vec{e}_{11}$描述了$C_1$看到$P_1$这件事，与其它位姿和路标无关，所以对它们求导为0，按照变量$\\vec{x}=(\\vec{\\xi}_1,\\vec{\\xi}_2,\\vec{p}_1,\\cdots,\\vec{p}_6)$，有</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\nJ_{11}=\\frac{\\partial \\vec{e}_{11}}{\\partial \\vec{x}}=\\left(\\frac{\\vec{e}_{11}}{\\partial\\vec{\\xi}_1},\\vec{0}_{2\\times6},\\frac{\\vec{e}_{11}}{\\partial\\vec{p}_1},\\vec{0}_{2\\times3},\\vec{0}_{2\\times3},\\vec{0}_{2\\times3},\\vec{0}_{2\\times3},\\vec{0}_{2\\times3}\\right)\n\\end{equation}</script><p>形状如图<img src=\"image-20221107162453662.png\" alt=\"分块雅可比矩阵稀疏结构\"><br>拼接$J_{ij}$可以得到整体雅可比矩阵和$H$矩阵如图<img src=\"image-20221107162722048.png\" alt=\"完整雅可比矩阵稀疏结构\"></p>\n<hr>\n<p><strong>图优化结构和增量方程的稀疏性存在联系</strong>：$H$矩阵中的非对角部分的非零矩阵块可以理解为其对应的位姿和路标变量之间存在联系也可称为约束，如图<img src=\"image-20221107163948504.png\" alt=\"增量矩阵和图优化结构的联系\"><br>当有m个相机位姿和n个路标点时，且路标数量远大于位姿数量，即$n\\gg m$，$H$矩阵左上角区域会非常小，右下脚区域非常大，非对角部分分布散乱的约束块，形状如箭头</p>\n<center><img src=\"image-20221107164511662.png\" alt=\"一般情况下H矩阵块\" style=\"zoom: 33%;\" /></center>\n\n<p><strong>Schur消元</strong>即<strong>Marginalization边缘化</strong>加速计算稀疏矩阵$H$，$H$矩阵分成4块，分别为$B,E,E^T,C$如图。</p>\n<center><img src=\"image-20221107165342106.png\" alt=\"H矩阵分块\" style=\"zoom: 33%;\"  /></center>\n$$\n\\begin{equation}\\label{eq:Joint_probability_distribution}\n\\left[\\begin{aligned}\n&B & &E\\\\\n&E^T & &C\n\\end{aligned}\\right]\n\\left[\\begin{aligned}\n&\\Delta\\vec{x}_c\\\\\n&\\Delta\\vec{x}_p\n\\end{aligned}\\right]\n=\n\\left[\\begin{aligned}\n&\\vec{v}\\\\\n&\\vec{w}\n\\end{aligned}\\right]\n\\end{equation}\n$$\n其中$B$是对角块矩阵，每个对角块的维度和相机位姿的维度相同，对角块的个数和位姿的个数相同(李群：9维，李代数：6维)；同理$C$是对角块矩阵，每个对角块的维度和路标的维度(三维)相同，对角块的个数和路标个数相同， 公式$\\ref{eq:Joint_probability_distribution}$可以理解成**联合概率分布**。\n\n**边缘化**即消去路标变量对应的系数矩阵，使方程中只含有位姿变量和其系数矩阵\n$$\n\\begin{equation}\n\\left[\\begin{aligned}\n&I & &-EC^{-1}\\\\\n&[0]_{n\\times m} & &I\n\\end{aligned}\\right]\n\\left[\\begin{aligned}\n&B & &E\\\\\n&E^T & &C\n\\end{aligned}\\right]\n\\left[\\begin{aligned}\n&\\Delta\\vec{x}_c\\\\\n&\\Delta\\vec{x}_p\n\\end{aligned}\\right]\n=\n\\left[\\begin{aligned}\n&I & &-EC^{-1}\\\\\n&[0]_{n\\times m} & &I\n\\end{aligned}\\right]\n\\left[\\begin{aligned}\n&\\vec{v}\\\\\n&\\vec{w}\n\\end{aligned}\\right]\n\\end{equation}\n$$\n\n位姿部分的增量方程为\n$$\n\\begin{equation}\n\\label{eq:marginalization}\n\\left[B-EC^{-1}\\right]\\Delta \\vec{x}_c=\\vec{v}-EC^{-1}\\vec{w}\n\\end{equation}\n$$\n这可以理解成$\\Delta\\vec{x}_c$的**先验函数**，求解$\\Delta\\vec{x}_c$后再求解$\\Delta\\vec{x}_p$，求解$\\Delta\\vec{x}_p$的方程可以理解成**条件概率**（已知$\\Delta\\vec{x}_c$），利用对角块矩阵$C$的逆运算容易求解\n$$\n\\begin{equation}\n\\Delta\\vec{x}_p=C^{-1}(\\vec{w}-E^T\\Delta\\vec{x}_c)\n\\end{equation}\n$$\n\n公式$\\ref{eq:marginalization}$的系数矩阵$\\left[B-EC^{-1}\\right]\\Delta \\vec{x}_c$记作$S$，它的稀疏性是不规则的，如图\n<center><img src=\"image-20221108112428467.png\" alt=\"边缘化系数矩阵稀疏结构\" style=\"zoom: 33%;\" /></center>\n\n<p>如同$H$矩阵的<strong>非对角块处的非零块</strong>对应着<strong>相机位姿</strong>和<strong>路标</strong>的联系，$S$矩阵的<strong>非对角块处的非零块</strong>对应着<strong>相机的两个位姿</strong>之间存在着<strong>共同观测的路标</strong>也称<strong>共视路标</strong></p>\n<h3 id=\"鲁棒核函数\"><a href=\"#鲁棒核函数\" class=\"headerlink\" title=\"鲁棒核函数\"></a>鲁棒核函数</h3><p>目标函数采用最小化误差项的二范数平方和的方式，<strong>存在的问题</strong>是误匹配导致优化时对误匹配的误差进行了优化从而忽略了对正确匹配的误差的优化，这是由于误差很大时二范数增长太快。<strong>解决方法</strong>采用核函数，保证每个数据误差不会因为大得没边而掩盖其它数据误差。核函数的性质：1、增长不快，2、函数光滑（可导），例如Huber核函数：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\nH(e)=\\left\\{\\begin{aligned}\n&\\frac{1}{2}e^2 & &当|e|\\le\\delta,\\\\\n&\\delta(|e|-\\frac{1}{2}\\delta) & &其它\n\\end{aligned}\\right.\n\\end{equation}</script><p><img src=\"image-20221108141628283.png\" alt=\"huber核函数\" style=\"zoom:33%;\" /></p>\n","site":{"data":{}},"cover":"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/cover8.JPG","excerpt":"","more":"<h1 id=\"后端\"><a href=\"#后端\" class=\"headerlink\" title=\"后端\"></a>后端</h1><h2 id=\"状态估计概率解释\"><a href=\"#状态估计概率解释\" class=\"headerlink\" title=\"状态估计概率解释\"></a>状态估计概率解释</h2><p>视觉里程计只有短暂记忆，而我们希望整个运动轨迹在较长时间内都能保持最优的状态。用最新的知识更新较久远的状态。这是个状态估计问题。</p>\n<script type=\"math/tex; mode=display\">\n状态估计方式\\left\\{\\begin{align*}\n    &批量式(\\mathrm{Batch}): 过去信息+当前信息+未来信息更新状态 \\\\\n    &渐进式(\\mathrm{Incremental}): 只由过去信息更新状态\n\\end{align*}\\right.</script><p>假设$t=0$到$t=N$时间内，有位姿$x_0$到$x_N$，并有路标$y_0$，…，$y_M$，运动方程和观测方程为</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\left\\{\\begin{aligned}\n    &\\vec{x}_k=f(\\vec{x}_{k-1},\\vec{u}_k)+\\vec{w}_k \\\\\n    &\\vec{z}_{k,j}=h(\\vec{y}_j,\\vec{x}_k)+\\vec{v}_{k,j}\n\\end{aligned}\\right.\\quad\\quad k=1,\\cdots,N,\\quad j=1,\\cdots,M.\n\\end{equation}</script><p>特点：</p>\n<ul>\n<li>运动方程数量小于甚至远小于观测方程</li>\n<li>没有运动方程相当于通过一组时序图像来恢复运动和结构</li>\n</ul>\n<p>方程中的位姿$\\vec{x}$和路标$\\vec{y}$受噪声影响，所以应该看成服从某种概率分布的随机变量，而不是一个数。</p>\n<p><strong>状态估计中的观测量和状态量</strong>：</p>\n<script type=\"math/tex; mode=display\">\n\\left\\{\\begin{align*}\n    &状态量：位姿\\vec{x}，路标\\vec{y} \\\\\n    &观测量：运动输入数据\\vec{u}，观测数据\\vec{z}\n\\end{align*}\\right.</script><p><strong>重新定义状态量</strong>：$\\vec{x}_k\\overset{def}{=}\\{\\vec{x}_k,\\vec{y}_1,\\cdots,\\vec{y}_m\\}$表示k时刻状态量$\\vec{x}_k$由k时刻位姿$\\vec{x}_k$及$m$个路标$\\vec{y}_*$组成。</p>\n<p><strong>重新定义观测数据</strong>：$\\vec{z}_k\\overset{def}{=}\\{\\vec{z}_{k,1},\\cdots,\\vec{z}_{k,m}\\}$表示k时刻对m个路标的观测数据</p>\n<p><strong>新的运动和观测方程</strong></p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\label{eq:kinemic_and_observation}\n\\left\\{\\begin{aligned}\n    &\\vec{x}_k=f(\\vec{x}_{k-1},\\vec{u}_k)+\\vec{w}_k\\\\\n    &\\vec{z}_{k}=h(\\vec{x}_k)+\\vec{v}_{k}\n\\end{aligned}\\right.\\quad\\quad k=1,\\cdots,N.\n\\end{equation}</script><p>$k$时刻状态量几率函数用$0$到$k$时刻的观测量来估计，即<strong>后验分布</strong>记为$P(\\vec{x}_k|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k})$, 这里$\\vec{u}_{1:k}=\\{\\vec{u}_1,\\vec{u}_2,\\cdots,\\vec{u}_k\\}$，$\\vec{z}_{1:k}=\\{\\vec{z}_1,\\vec{z}_2,\\cdots,\\vec{z}_k\\}$，根据贝叶斯法则，将后验分布分解成<strong>似然分布</strong>和<strong>先验分布</strong>的乘积</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\nP(\\vec{x}_k|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k})\\propto P(\\vec{z}_k|\\vec{x}_k)P(\\vec{x}_k|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})\n\\end{equation}</script><p><strong>先验分布</strong>是基于过去所有状态估计得来，简化只受到$\\vec{x}_{k-1}$影响，联合分布$P(\\vec{x}_k，\\vec{x}_{k-1}|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})$对$\\vec{x}_{k-1}$积分得到边缘分布$P(\\vec{x}_k|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})$即<strong>先验分布</strong>，联合分布可以通过条件概率公式转化为$\\vec{x}_{k-1}$的条件概率形式，即$P(\\vec{x}_k|\\vec{x}_{k-1},\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})P(\\vec{x}_{k-1}|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})$，因此<strong>先验分布</strong>可以表示成：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\\label{eq:xianyan}\nP(\\vec{x}_k|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})=\\int P(\\vec{x}_k|\\vec{x}_{k-1},\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})P(\\vec{x}_{k-1}|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})\\,\\mathrm{d}\\vec{x}_{k-1}\n\\end{equation}</script><p>先验分布的处理方式：</p>\n<script type=\"math/tex; mode=display\">\n\\left\\{\\begin{align*}\n&k时刻状态\\vec{x}_k的分布只和k-1时刻的状态\\vec{x}_{k-1}有关即马尔可夫假设&&: 扩展卡尔曼滤波(EKF) \\\\\n&k时刻状态\\vec{x}_k的分布与之前所有状态\\vec{x}_{1:k}有关&&: 非线性优化\n\\end{align*}\\right.</script><h2 id=\"线性系统和卡尔曼滤波（KF）\"><a href=\"#线性系统和卡尔曼滤波（KF）\" class=\"headerlink\" title=\"线性系统和卡尔曼滤波（KF）\"></a>线性系统和卡尔曼滤波（KF）</h2><p>根据k时刻状态$\\vec{x}_k$的分布只和k-1时刻的状态$\\vec{x}_{k-1}$有关，所以可以去掉$\\vec{x}_{0:k-2}$这些状态量，k时刻状态量$\\vec{x}_k$和k时刻运动输入量$\\vec{u}_k$以及观测量$\\vec{z}_k$有关和其它时刻没关系，公式$\\ref{eq:xianyan}$中右侧第一部分可简化为</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\nP(\\vec{x}_k|\\vec{x}_{k-1},\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})=P(\\vec{x}_k|\\vec{x}_{k-1},\\vec{u}_k)\n\\end{equation}</script><p>由于k-1时刻状态量$\\vec{x}_k$还是按照和之前状态都有关的假设，公式$\\ref{eq:xianyan}$中右侧第二部分可简化为</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\label{eq:k-1zhuang_tai_liang}\nP(\\vec{x}_{k-1}|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})=P(\\vec{x}_{k-1}|\\vec{x}_{0},\\vec{u}_{1:k-1},\\vec{z}_{1:k-1})\n\\end{equation}</script><p>公式$\\ref{eq:k-1zhuang_tai_liang}$就是k-1时刻的状态分布。</p>\n<p><strong>线性高斯系统</strong></p>\n<p>将公式$\\ref{eq:kinemic_and_observation}$中的运动方程和观测方程按照线性函数的形式展开</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\left\\{\\begin{aligned}\n    &\\vec{x}_k=A_k\\vec{x}_{k-1}+\\vec{u}_k+\\vec{w}_k \\\\\n    &\\vec{z}_{k}=C_k\\vec{x}_k+\\vec{v}_{k}\n\\end{aligned}\\right.\\quad\\quad k=1,\\cdots,N.\n\\end{equation}</script><p>运动噪声和观测噪声都服从0均值正态分布：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\vec{w}_k\\sim N(0,R_{k})\\quad\\quad\\quad \\vec{v}_k\\sim N(0,Q_{k})\n\\end{equation}</script><p><strong>记号区分</strong>：上帽子$\\hat{\\vec{x}}_k$表示后验，下帽子$\\check{\\vec{x}}_k$表示先验</p>\n<p><strong>已知</strong>k-1时刻的后验状态估计为$\\hat{\\vec{x}}_{k-1}$及其协方差$\\hat{P}_{k-1}$，状态估计服从高斯分布，<strong>求</strong>k时刻的状态估计及其协方差</p>\n<p><strong>运动方程</strong>确定$\\vec{x}_k$的<strong>先验分布</strong>（随机变量函数的分布函数），扰动传递规则</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\nP(\\vec{x}_k|\\vec{x}_0,\\vec{u}_{1:k},\\vec{z}_{1:k-1})=N(A_k\\hat{\\vec{x}}_{k-1}+\\vec{u}_k,A_k\\hat{P}_{k-1}A^T_k+R_{k})\n\\end{equation}</script><p><strong>先验分布</strong>得到k时刻状态估计，记为$\\check{\\vec{x}}=A_k\\hat{\\vec{x}}_{k-1}+\\vec{u}_k$，$\\check{P}_k=A_k\\hat{P}_{k-1}A_k^T+R_k$</p>\n<p><strong>观测方程</strong>确定状态量的<strong>似然函数</strong>$P(\\vec{z}_k|\\vec{x}_k)=N(C_k\\vec{x}_k, Q_k)$</p>\n<p><strong>贝叶斯公式</strong>确定状态量的<strong>后验分布</strong></p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\nN(\\hat{\\vec{x}}_k,\\hat{P}_k)=\\eta N(C_k\\vec{x}_k,Q_k)\\cdot N(\\check{\\vec{x}}_k,\\check{P}_k)\n\\end{equation}</script><p>归纳为：预测和更新两步</p>\n<ol>\n<li>预测<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\check{\\vec{x}}_k=A_k\\hat{\\vec{x}}_{k-1}+\\vec{u}_k,\\quad\\quad \\check{P}_k=A_k\\hat{P}_{k-1}A^T_k+R_k\n\\end{equation}</script></li>\n<li>更新：<br>2.1.  先计算$K_k$，称为卡尔曼增益<script type=\"math/tex; mode=display\">\n\\begin{equation}\nK=\\check{P}_kC_k^T(C_k\\check{P}_kC_k^T+Q_k)^{-1}\n\\end{equation}</script>​      2.2. 计算后验分布的估计量<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\hat{\\vec{x}}_k=\\check{\\vec{x}}_k+K_k(\\vec{z}_k-C_k\\check{\\vec{x}}_k), \\quad\\quad \\hat{P}_k=(I-K_kC_k)\\check{P}_k\n\\end{equation}</script></li>\n</ol>\n<h2 id=\"非线性系统和扩展卡尔曼滤波（EKF）\"><a href=\"#非线性系统和扩展卡尔曼滤波（EKF）\" class=\"headerlink\" title=\"非线性系统和扩展卡尔曼滤波（EKF）\"></a>非线性系统和扩展卡尔曼滤波（EKF）</h2><p>SLAM中的运动观测方程是非线性函数</p>\n<script type=\"math/tex; mode=display\">\n\\left\\{\\begin{align*}\n&高斯分布\\xrightarrow{线性变换}高斯分布\\\\\n&高斯分布\\xrightarrow{非线性变换}非高斯分布\n\\end{align*}\\right.</script><p>解决方法：在状态量附近对<strong>运动方程</strong>和<strong>观测方程</strong>进行泰勒展开，保留一阶项，从而得到近似的线性部分，再按照线性系统进行推到。</p>\n<p><strong>已知</strong>k-1时刻的后验状态估计为$\\hat{\\vec{x}}_{k-1}$及其协方差$\\hat{P}_{k-1}$，状态估计服从高斯分布，<strong>求</strong>k时刻的状态估计及其协方差，将<strong>运动方程</strong>和<strong>观测方程</strong>在$\\hat{\\vec{x}}_{k-1}$和$\\hat{P}_k$处进行<strong>线性化</strong>。</p>\n<p>对运动方程</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\vec{x}_k\\approx f(\\hat{\\vec{x}}_{k-1},\\vec{u}_k)+\\left.\\frac{\\partial f}{\\partial \\vec{x}_{k-1}}\\right|_{\\hat{\\vec{x}}_{k-1}}(\\vec{x}_{k-1}-\\hat{\\vec{x}}_{k-1})+\\vec{w}_k\n\\end{equation}</script><p>记偏导数为</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\nF=\\left.\\frac{\\partial f}{\\partial \\vec{x}_{k-1}}\\right|_{\\hat{\\vec{x}}_{k-1}}\n\\end{equation}</script><p>对观测方程</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\vec{z}_k\\approx h(\\check{\\vec{x}}_{k})+\\left.\\frac{\\partial h}{\\partial \\vec{x}_{k}}\\right|_{\\check{\\vec{x}}_{k}}(\\vec{x}_{k}-\\check{\\vec{x}}_{k-1})+\\vec{v}_k\n\\end{equation}</script><p>记偏导数为</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\nH=\\left.\\frac{\\partial h}{\\partial \\vec{x}_{k}}\\right|_{\\check{\\vec{x}}_{k}}\n\\end{equation}</script><p>仿照线性变换部分计算先验、似然和后验分布，根据分布求状态量的估计值<br>归纳为：预测和更新两步</p>\n<ol>\n<li>预测<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\check{\\vec{x}}_k=f(\\hat{\\vec{x}}_{k-1},\\vec{u}_k),\\quad\\quad \\check{P}_k=F\\hat{P}_{k-1}F^T+R_k\n\\end{equation}</script></li>\n<li>更新：<br>2.1.  先计算$K_k$，称为卡尔曼增益<script type=\"math/tex; mode=display\">\n\\begin{equation}\nK_k=\\check{P}_kH^T(H\\check{P}_kH^T+Q_k)^{-1}\n\\end{equation}</script>​      2.2. 计算后验分布的估计量<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\hat{\\vec{x}}_k=\\check{\\vec{x}}_k+K_k(\\vec{z}_k-h(\\check{\\vec{x}}_k)), \\quad\\quad \\hat{P}_k=(I-K_kH)\\check{P}_k\n\\end{equation}</script></li>\n</ol>\n<h2 id=\"非线性优化：Bundle-Adjustment（BA）\"><a href=\"#非线性优化：Bundle-Adjustment（BA）\" class=\"headerlink\" title=\"非线性优化：Bundle Adjustment（BA）\"></a>非线性优化：Bundle Adjustment（BA）</h2><ol>\n<li>世界坐标系下的点记作<script type=\"math/tex\">\\vec{p}_w=[X_w,Y_w,Z_w]^T</script>，</li>\n<li>相机坐标系下的点<script type=\"math/tex\">\\vec{p}_c=[X_c,Y_c,Z_c]^T</script>，</li>\n<li>归一化平面上的点<script type=\"math/tex\">\\vec{p}_{c_n}=[u_c,v_c,1]^T=[X_c/Z_c,Y_c/Z_c,1]^T</script></li>\n<li>归一化坐标的畸变情况，还原畸变矫正前的原始投影坐标，只考虑径向畸变<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\left\\{\\begin{aligned}\n&u_c^\\prime =u_c(1+k_1r_c^2+k_2r_c^4)\\\\\n&v_c^\\prime =v_c(1+k_1r_c^2+k_2r_c^4)\n\\end{aligned}\\right. \\quad\\quad r_c^2=u_c^2+v_c^2\n\\end{equation}</script></li>\n<li>根据相机内参（焦距，像素平面的原点坐标）计算像素坐标<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\left\\{\\begin{aligned}\n&u_s=f_xu_c^\\prime+c_x\\\\\n&v_s=f_yu_c^\\prime+c_y\n\\end{aligned}\\right.\n\\end{equation}</script></li>\n</ol>\n<p>在<strong>i时刻</strong>，$\\vec{x_i}$指代相机<strong>位姿</strong>即$R_i，\\vec{t_i}$，其对应的李群为$T_i$，李代数为$\\vec{\\xi}_i$。<strong>j路标</strong>$\\vec{y}_j$即三维点$\\vec{p}_j$，<strong>观测数据</strong>是像素坐标$\\vec{z}_{i,j}\\overset{def}{=}[u_{s_{i,j}},v_{s_{i,j}}]^T$。观测误差</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\vec{e_{i,j}}=\\vec{z}_{i,j}-h(T_i,\\vec{p}_j)\n\\end{equation}</script><p>个时刻对所有路标观测的整体误差</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\frac{1}{2}\\sum_{i=i}^{m}\\sum_{j=1}^{n}||\\vec{e}_{i,j}||^2=\\frac{1}{2}\\sum_{i=i}^{m}\\sum_{j=1}^{n}||\\vec{z}_{i,j}-h(T_i,\\vec{p}_j)||^2\n\\end{equation}</script><h3 id=\"求解BA\"><a href=\"#求解BA\" class=\"headerlink\" title=\"求解BA\"></a>求解BA</h3><p>BA目标函数的<strong>自变量</strong>定义成</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\vec{x}=[T_1,\\cdots,T_m,\\vec{p}_1,\\cdots,\\vec{p}_n]^T\n\\end{equation}</script><p>给自变量一个增量$\\Delta \\vec{x}$</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\\label{eq:BA:object_fun}\n\\frac{1}{2}||f(\\vec{x}+\\Delta\\vec{x})||^2&\\approx\\frac{1}{2}\\sum_{i=i}^{m}\\sum_{j=1}^{n}||\\vec{e_{i,j}}+F_{i,j}\\Delta\\vec{\\xi}_{i}+E_{i,j}\\Delta\\vec{p}_j)||^2 \\\\\nF_{i,j}&=\\frac{\\partial \\vec{e}_{i,j}}{\\partial \\vec{\\xi}_i}\\\\\nE_{i,j}&=\\frac{\\partial \\vec{e}_{i,j}}{\\partial \\vec{p}_j}\n\\end{align}</script><p>将<strong>位姿</strong>变量放在一起，<strong>路标</strong>变量放在一起，记作：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\vec{x}_c=[\\vec{\\xi}_1,\\vec{\\xi}_2,\\cdots,\\vec{\\xi}_m]^T\\in\\mathbb{R}^{6m}\n\\end{equation}</script><script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\vec{x}_p=[\\vec{p}_1,\\vec{p}_2,\\cdots,\\vec{p}_n]^T\\in\\mathbb{R}^{3m}\n\\end{equation}</script><p>公式$\\ref{eq:BA:object_fun}$记作：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\frac{1}{2}||f(\\vec{x}+\\Delta\\vec{x})||^2\\approx\\frac{1}{2}\\sum_{i=i}^{m}\\sum_{j=1}^{n}||\\vec{e}+F\\Delta\\vec{x}_{c}+E\\Delta\\vec{x}_p)||^2\\quad这里的E和F是由若干小矩阵块E_{i,j}和F_{i,j}各自拼凑起来的\n\\end{equation}</script><p>无论<strong>高斯牛顿法</strong>还是<strong>列文伯格-马夸尔特方法</strong>增量线性方程都是：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\nH\\Delta\\vec{x}=\\vec{g}\n\\end{equation}</script><script type=\"math/tex; mode=display\">\n区别: H\\left\\{\\begin{align*}\n&J^TJ & &高斯牛顿法\\\\\n&J^TJ+\\lambda I & &列文伯格-马夸尔特方法\n\\end{align*}\\right.</script><p>注：有些博客用$H=J^T\\Omega J$,$\\Omega$应该是正态分布协方差的逆矩阵$\\Sigma^{-1}$，它是2x2矩阵，可以从$\\vec{e}$是二维向量推出,已将自变量归类成位姿和空间点两种形式，雅可比矩阵亦分块成</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\nJ=[F \\quad E]\n\\end{equation}</script><p>求高斯牛顿$H$，逆运算复杂度$O(n^3)$，但$H$是有稀疏矩阵结构</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\nH=J^TJ=\\left[\\begin{aligned}\n&F^TF & &F^TE\\\\\n&E^TF & &E^TE\n\\end{aligned}\\right]\n\\end{equation}</script><h3 id=\"利用稀疏性和边缘化求解BA\"><a href=\"#利用稀疏性和边缘化求解BA\" class=\"headerlink\" title=\"利用稀疏性和边缘化求解BA\"></a>利用稀疏性和边缘化求解BA</h3><p>雅可比矩阵的结构</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\nJ_{i,j}(\\vec{x})=\\left(\\vec{0}_{2\\times6},\\cdots,\\vec{0}_{2\\times6},\\frac{\\partial\\vec{e}_{i,j}}{\\partial T_i},\\vec{0}_{2\\times6},\\cdots\\left|\\vec{0}_{2\\times3},\\cdots,\\vec{0}_{2\\times3},\\frac{\\partial\\vec{e}_{i,j}}{\\partial \\vec{p}_j},\\vec{0}_{2\\times3},\\cdots,\\vec{0}_{2\\times3}\\right.\\right)\n\\end{equation}</script><p><img src=\"image-20221107155224157.png\" alt=\"稀疏结构\"></p>\n<hr>\n<p>举例：考虑一个场景内有2个相机位姿($C_1$,$C_2$)和6个路标点($P_1$,$P_2$,$P_3$,$P_4$,$P_5$,$P_6$)，相机在位姿$C_1$处观察到路标$P_1$,$P_2$,$P_3$,$P_4$，在位姿$C_2$处观察到路标$P_3$,$P_4$,$P_5$,$P_6$，如图<img src=\"image-20221107161443842.png\" alt=\"举例示例\"><br>该场景下的BA目标函数</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\n\\frac{1}{2}\\left(||\\vec{e}_{11}||^2+||\\vec{e}_{12}||^2+||\\vec{e}_{13}||^2+||\\vec{e}_{14}||^2+||\\vec{e}_{23}||^2+||\\vec{e}_{24}||^2+||\\vec{e}_{25}||^2+||\\vec{e}_{26}||^2\\right)\n\\end{equation}</script><p>$\\vec{e}_{11}$描述了$C_1$看到$P_1$这件事，与其它位姿和路标无关，所以对它们求导为0，按照变量$\\vec{x}=(\\vec{\\xi}_1,\\vec{\\xi}_2,\\vec{p}_1,\\cdots,\\vec{p}_6)$，有</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\nJ_{11}=\\frac{\\partial \\vec{e}_{11}}{\\partial \\vec{x}}=\\left(\\frac{\\vec{e}_{11}}{\\partial\\vec{\\xi}_1},\\vec{0}_{2\\times6},\\frac{\\vec{e}_{11}}{\\partial\\vec{p}_1},\\vec{0}_{2\\times3},\\vec{0}_{2\\times3},\\vec{0}_{2\\times3},\\vec{0}_{2\\times3},\\vec{0}_{2\\times3}\\right)\n\\end{equation}</script><p>形状如图<img src=\"image-20221107162453662.png\" alt=\"分块雅可比矩阵稀疏结构\"><br>拼接$J_{ij}$可以得到整体雅可比矩阵和$H$矩阵如图<img src=\"image-20221107162722048.png\" alt=\"完整雅可比矩阵稀疏结构\"></p>\n<hr>\n<p><strong>图优化结构和增量方程的稀疏性存在联系</strong>：$H$矩阵中的非对角部分的非零矩阵块可以理解为其对应的位姿和路标变量之间存在联系也可称为约束，如图<img src=\"image-20221107163948504.png\" alt=\"增量矩阵和图优化结构的联系\"><br>当有m个相机位姿和n个路标点时，且路标数量远大于位姿数量，即$n\\gg m$，$H$矩阵左上角区域会非常小，右下脚区域非常大，非对角部分分布散乱的约束块，形状如箭头</p>\n<center><img src=\"image-20221107164511662.png\" alt=\"一般情况下H矩阵块\" style=\"zoom: 33%;\" /></center>\n\n<p><strong>Schur消元</strong>即<strong>Marginalization边缘化</strong>加速计算稀疏矩阵$H$，$H$矩阵分成4块，分别为$B,E,E^T,C$如图。</p>\n<center><img src=\"image-20221107165342106.png\" alt=\"H矩阵分块\" style=\"zoom: 33%;\"  /></center>\n$$\n\\begin{equation}\\label{eq:Joint_probability_distribution}\n\\left[\\begin{aligned}\n&B & &E\\\\\n&E^T & &C\n\\end{aligned}\\right]\n\\left[\\begin{aligned}\n&\\Delta\\vec{x}_c\\\\\n&\\Delta\\vec{x}_p\n\\end{aligned}\\right]\n=\n\\left[\\begin{aligned}\n&\\vec{v}\\\\\n&\\vec{w}\n\\end{aligned}\\right]\n\\end{equation}\n$$\n其中$B$是对角块矩阵，每个对角块的维度和相机位姿的维度相同，对角块的个数和位姿的个数相同(李群：9维，李代数：6维)；同理$C$是对角块矩阵，每个对角块的维度和路标的维度(三维)相同，对角块的个数和路标个数相同， 公式$\\ref{eq:Joint_probability_distribution}$可以理解成**联合概率分布**。\n\n**边缘化**即消去路标变量对应的系数矩阵，使方程中只含有位姿变量和其系数矩阵\n$$\n\\begin{equation}\n\\left[\\begin{aligned}\n&I & &-EC^{-1}\\\\\n&[0]_{n\\times m} & &I\n\\end{aligned}\\right]\n\\left[\\begin{aligned}\n&B & &E\\\\\n&E^T & &C\n\\end{aligned}\\right]\n\\left[\\begin{aligned}\n&\\Delta\\vec{x}_c\\\\\n&\\Delta\\vec{x}_p\n\\end{aligned}\\right]\n=\n\\left[\\begin{aligned}\n&I & &-EC^{-1}\\\\\n&[0]_{n\\times m} & &I\n\\end{aligned}\\right]\n\\left[\\begin{aligned}\n&\\vec{v}\\\\\n&\\vec{w}\n\\end{aligned}\\right]\n\\end{equation}\n$$\n\n位姿部分的增量方程为\n$$\n\\begin{equation}\n\\label{eq:marginalization}\n\\left[B-EC^{-1}\\right]\\Delta \\vec{x}_c=\\vec{v}-EC^{-1}\\vec{w}\n\\end{equation}\n$$\n这可以理解成$\\Delta\\vec{x}_c$的**先验函数**，求解$\\Delta\\vec{x}_c$后再求解$\\Delta\\vec{x}_p$，求解$\\Delta\\vec{x}_p$的方程可以理解成**条件概率**（已知$\\Delta\\vec{x}_c$），利用对角块矩阵$C$的逆运算容易求解\n$$\n\\begin{equation}\n\\Delta\\vec{x}_p=C^{-1}(\\vec{w}-E^T\\Delta\\vec{x}_c)\n\\end{equation}\n$$\n\n公式$\\ref{eq:marginalization}$的系数矩阵$\\left[B-EC^{-1}\\right]\\Delta \\vec{x}_c$记作$S$，它的稀疏性是不规则的，如图\n<center><img src=\"image-20221108112428467.png\" alt=\"边缘化系数矩阵稀疏结构\" style=\"zoom: 33%;\" /></center>\n\n<p>如同$H$矩阵的<strong>非对角块处的非零块</strong>对应着<strong>相机位姿</strong>和<strong>路标</strong>的联系，$S$矩阵的<strong>非对角块处的非零块</strong>对应着<strong>相机的两个位姿</strong>之间存在着<strong>共同观测的路标</strong>也称<strong>共视路标</strong></p>\n<h3 id=\"鲁棒核函数\"><a href=\"#鲁棒核函数\" class=\"headerlink\" title=\"鲁棒核函数\"></a>鲁棒核函数</h3><p>目标函数采用最小化误差项的二范数平方和的方式，<strong>存在的问题</strong>是误匹配导致优化时对误匹配的误差进行了优化从而忽略了对正确匹配的误差的优化，这是由于误差很大时二范数增长太快。<strong>解决方法</strong>采用核函数，保证每个数据误差不会因为大得没边而掩盖其它数据误差。核函数的性质：1、增长不快，2、函数光滑（可导），例如Huber核函数：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{equation}\nH(e)=\\left\\{\\begin{aligned}\n&\\frac{1}{2}e^2 & &当|e|\\le\\delta,\\\\\n&\\delta(|e|-\\frac{1}{2}\\delta) & &其它\n\\end{aligned}\\right.\n\\end{equation}</script><p><img src=\"image-20221108141628283.png\" alt=\"huber核函数\" style=\"zoom:33%;\" /></p>\n"},{"typora-root-url":"./cmake","title":"cmake","mathjax":true,"date":"2022-11-15T05:43:56.000Z","_content":"\n# CMake\n\n## CMakeLists.txt\n\n1. 指定语言版本\n```cmake\nset(CMAKE_CXX_STANDARD 11)\n```\nCMAKE_开头的变量都是CMAKE内置变量，CMAKE保留变量\n\n2. 配置编译选项\n```cmake\nadd_compile_options(-Wall -Wextra -pedantic -Werror)\n#或者\nset(CMAKE_CXX_FLAGS \"\"${CMAKE_CXX_FLAGS} -pipe -std=c++11\"\")\n```\n\n3. 配置编译类型\n类型可设置为：Debug, Release, RelWithDebInfo, MinSizeRel。可以针对不同的编译类型设置不同的编译选项\n```cmake\nset(CMAKE_BUILD_TYPE Debug)\n#-g开启调试信息 -o0不进行代码优化\nset(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} -g -O0\")\n```\n4. 全局宏定义\n通过判断不同的宏定义在命令行cmake -DDEBUG=1 -DREAL_COOL_ENGINEER=0实现.cpp源代码中宏的流程控制\n```cmake\nadd_definitions(-DDEBUG -DREAL_COOL_ENGINEER)\n```\n5. 添加include目录\n```cmake\ninclude_directories(src/include)\n```","source":"_posts/cmake.md","raw":"---\ntypora-root-url: ./cmake\ntitle: cmake\nmathjax: true\ndate: 2022-11-15 13:43:56\ncategories:\n- 编程语言\n- c++\n- cmake\ntags:\n- cmake\n---\n\n# CMake\n\n## CMakeLists.txt\n\n1. 指定语言版本\n```cmake\nset(CMAKE_CXX_STANDARD 11)\n```\nCMAKE_开头的变量都是CMAKE内置变量，CMAKE保留变量\n\n2. 配置编译选项\n```cmake\nadd_compile_options(-Wall -Wextra -pedantic -Werror)\n#或者\nset(CMAKE_CXX_FLAGS \"\"${CMAKE_CXX_FLAGS} -pipe -std=c++11\"\")\n```\n\n3. 配置编译类型\n类型可设置为：Debug, Release, RelWithDebInfo, MinSizeRel。可以针对不同的编译类型设置不同的编译选项\n```cmake\nset(CMAKE_BUILD_TYPE Debug)\n#-g开启调试信息 -o0不进行代码优化\nset(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} -g -O0\")\n```\n4. 全局宏定义\n通过判断不同的宏定义在命令行cmake -DDEBUG=1 -DREAL_COOL_ENGINEER=0实现.cpp源代码中宏的流程控制\n```cmake\nadd_definitions(-DDEBUG -DREAL_COOL_ENGINEER)\n```\n5. 添加include目录\n```cmake\ninclude_directories(src/include)\n```","slug":"cmake","published":1,"updated":"2022-11-15T06:15:10.648Z","comments":1,"layout":"post","photos":[],"link":"","_id":"claugokr40000ccfyaarqamt8","content":"<h1 id=\"CMake\"><a href=\"#CMake\" class=\"headerlink\" title=\"CMake\"></a>CMake</h1><h2 id=\"CMakeLists-txt\"><a href=\"#CMakeLists-txt\" class=\"headerlink\" title=\"CMakeLists.txt\"></a>CMakeLists.txt</h2><ol>\n<li><p>指定语言版本</p>\n<figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">set</span>(CMAKE_CXX_STANDARD <span class=\"number\">11</span>)</span><br></pre></td></tr></table></figure>\n<p>CMAKE_开头的变量都是CMAKE内置变量，CMAKE保留变量</p>\n</li>\n<li><p>配置编译选项</p>\n<figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">add_compile_options</span>(-Wall -Wextra -pedantic -Werror)</span><br><span class=\"line\"><span class=\"comment\">#或者</span></span><br><span class=\"line\"><span class=\"keyword\">set</span>(CMAKE_CXX_FLAGS <span class=\"string\">&quot;&quot;</span><span class=\"variable\">$&#123;CMAKE_CXX_FLAGS&#125;</span> -pipe -std=c++<span class=\"number\">11</span><span class=\"string\">&quot;&quot;</span>)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>配置编译类型<br>类型可设置为：Debug, Release, RelWithDebInfo, MinSizeRel。可以针对不同的编译类型设置不同的编译选项</p>\n<figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">set</span>(CMAKE_BUILD_TYPE Debug)</span><br><span class=\"line\"><span class=\"comment\">#-g开启调试信息 -o0不进行代码优化</span></span><br><span class=\"line\"><span class=\"keyword\">set</span>(CMAKE_CXX_FLAGS_DEBUG <span class=\"string\">&quot;$&#123;CMAKE_CXX_FLAGS_DEBUG&#125; -g -O0&quot;</span>)</span><br></pre></td></tr></table></figure></li>\n<li>全局宏定义<br>通过判断不同的宏定义在命令行cmake -DDEBUG=1 -DREAL_COOL_ENGINEER=0实现.cpp源代码中宏的流程控制<figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">add_definitions</span>(-DDEBUG -DREAL_COOL_ENGINEER)</span><br></pre></td></tr></table></figure></li>\n<li>添加include目录<figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">include_directories</span>(src/<span class=\"keyword\">include</span>)</span><br></pre></td></tr></table></figure></li>\n</ol>\n","site":{"data":{}},"cover":"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/cover5.JPG","excerpt":"","more":"<h1 id=\"CMake\"><a href=\"#CMake\" class=\"headerlink\" title=\"CMake\"></a>CMake</h1><h2 id=\"CMakeLists-txt\"><a href=\"#CMakeLists-txt\" class=\"headerlink\" title=\"CMakeLists.txt\"></a>CMakeLists.txt</h2><ol>\n<li><p>指定语言版本</p>\n<figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">set</span>(CMAKE_CXX_STANDARD <span class=\"number\">11</span>)</span><br></pre></td></tr></table></figure>\n<p>CMAKE_开头的变量都是CMAKE内置变量，CMAKE保留变量</p>\n</li>\n<li><p>配置编译选项</p>\n<figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">add_compile_options</span>(-Wall -Wextra -pedantic -Werror)</span><br><span class=\"line\"><span class=\"comment\">#或者</span></span><br><span class=\"line\"><span class=\"keyword\">set</span>(CMAKE_CXX_FLAGS <span class=\"string\">&quot;&quot;</span><span class=\"variable\">$&#123;CMAKE_CXX_FLAGS&#125;</span> -pipe -std=c++<span class=\"number\">11</span><span class=\"string\">&quot;&quot;</span>)</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>配置编译类型<br>类型可设置为：Debug, Release, RelWithDebInfo, MinSizeRel。可以针对不同的编译类型设置不同的编译选项</p>\n<figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">set</span>(CMAKE_BUILD_TYPE Debug)</span><br><span class=\"line\"><span class=\"comment\">#-g开启调试信息 -o0不进行代码优化</span></span><br><span class=\"line\"><span class=\"keyword\">set</span>(CMAKE_CXX_FLAGS_DEBUG <span class=\"string\">&quot;$&#123;CMAKE_CXX_FLAGS_DEBUG&#125; -g -O0&quot;</span>)</span><br></pre></td></tr></table></figure></li>\n<li>全局宏定义<br>通过判断不同的宏定义在命令行cmake -DDEBUG=1 -DREAL_COOL_ENGINEER=0实现.cpp源代码中宏的流程控制<figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">add_definitions</span>(-DDEBUG -DREAL_COOL_ENGINEER)</span><br></pre></td></tr></table></figure></li>\n<li>添加include目录<figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">include_directories</span>(src/<span class=\"keyword\">include</span>)</span><br></pre></td></tr></table></figure></li>\n</ol>\n"},{"typora-root-url":"./g2o","title":"g2o","mathjax":true,"date":"2022-11-11T03:05:21.000Z","_content":"\n# g2o：c++图优化库\n\n![库关系](20210103152500545.png)\n\n- g2o提供鲁棒核函数抑制误差特别大的点，g2o::RobustKernelHuber","source":"_posts/g2o.md","raw":"---\ntypora-root-url: ./g2o\ntitle: g2o\nmathjax: true\ndate: 2022-11-11 11:05:21\ncategories:\n- 编程语言\n- c++\ntags:\n- slam14讲\n---\n\n# g2o：c++图优化库\n\n![库关系](20210103152500545.png)\n\n- g2o提供鲁棒核函数抑制误差特别大的点，g2o::RobustKernelHuber","slug":"g2o","published":1,"updated":"2022-11-11T03:24:50.095Z","comments":1,"layout":"post","photos":[],"link":"","_id":"claugokrd0007ccfy627y2ju3","content":"<h1 id=\"g2o：c-图优化库\"><a href=\"#g2o：c-图优化库\" class=\"headerlink\" title=\"g2o：c++图优化库\"></a>g2o：c++图优化库</h1><p><img src=\"20210103152500545.png\" alt=\"库关系\"></p>\n<ul>\n<li>g2o提供鲁棒核函数抑制误差特别大的点，g2o::RobustKernelHuber</li>\n</ul>\n","site":{"data":{}},"cover":"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/cover4.jpg","excerpt":"","more":"<h1 id=\"g2o：c-图优化库\"><a href=\"#g2o：c-图优化库\" class=\"headerlink\" title=\"g2o：c++图优化库\"></a>g2o：c++图优化库</h1><p><img src=\"20210103152500545.png\" alt=\"库关系\"></p>\n<ul>\n<li>g2o提供鲁棒核函数抑制误差特别大的点，g2o::RobustKernelHuber</li>\n</ul>\n"},{"typora-root-url":"./目标识别","title":"目标识别","mathjax":true,"date":"2022-11-22T01:13:23.000Z","_content":"\n[TOC]\n\n# 基于深度学习的视觉目标检测技术综述\n\n作者：曹家乐，2022\n\n## 发展历程\n\n1. 基于手工设计特征的方法\n    1. 支持向量机\n    2. AdaBoost\n    3. Haar特征(Viola, 2004)\n    4. 方向梯度直方图histograms of oriented gradients(Dalal, 2005)\n2. 深度学习\n    1. 区域卷积神经网络region-based convolutional neural network, R-CNN (Girshick, 2014)\n    2. 单次检测single shot detector, SSD (Liu, 2016)\n    3. yolo (Redmon, 2016)\n    4. detection transformer, DETR (Carion, 2020)\n\n深度网络模型：\n1. AlexNet(Krizhevsky,2012)\n2. GoogLeNet(Szegedy,2015)\n3. VGGNet(Simonyan,2015)\n4. ResNet(He,2016)\n5. DenseNet(Huang,2017)\n6. Mobilenet(Howard,2018)\n7. ShuffleNet(Zhang,2018)\n8. SENet(Hu, 2018)\n9. EfficientNet(Tan,2019)\n10. ViT(Dosovitskiy,2021)\n11. Swin(Liu,2022)\n\n目标检测方法\n1. DetectorNet(Szegedy,2014)\n2. R-CNN(Girshick,2014)\n3. OvearFeat(Sermanet,2014)\n4. SPPNet(He,2015)\n5. Fast R-CNN(Girshick,2016; Ren,2016)\n6. YOLO(Redmon,2016)\n7. SSD(Liu,2016)\n8. R-FCN(Dai,2017)\n9. FPN(Lin,2017)\n10. YOLOv2(Redmon,2017)\n11. Mask RCNN(He,2018)\n12. DCN(Dai,2018)\n13. RetinaNet(Lin,2018)\n14. Cascade RCNN(Cai,2018)\n15. YOLOv3(Redmon,2019)\n16. CornerNet(Law,2019)\n17. FCOS(Tian,2020)\n18. CenterNet(Zhou,2020)\n19. EfficientDet(Tan,2020)\n20. ATSS(Zhang,2020)\n21. MoCo(He,2020)\n22. YOLOv4(Bochkovskiy,2021)\n23. Deformable DETR(Zhu,2021)\n24. DETR(Carion,2021)\n25. YOLOv5(Jocher,2021)\n26. UP-DETR(Dai,2021)\n27. Pix2seq(Chen,2022)\n\n\n## 检测设备\n\n1. 单目相机\n2. 双目相机 （提供三维信息）\n\n\n## 基于单目相机流程及其涵盖的方法\n\n1. 数据预处理\n    - 翻转\n    - 放缩\n    - 均值归一化\n    - 色调变化\n    - 剪切、擦除、分区(DeVries, 2017; Zhong, 2020b; Singh, 2017; Chen, 2020a)\n    - 混合(Mixup: Zhang, 2018; CutMix: Yun, 2019; Fang, 2019; Mosaic: Bochkovskiy, 2020; Montage: Zhou, 2020; dynamic scale training: Chen, 2020b)\n\n2. 检测网络\n    - 基础骨架\n        - AlexNet(Krizhevsky, 2012)\n        - VGGNet(Simonyan, 2014) \n        - ResNet(He, 2016)\n        - DenseNet(Huang, 2017)\n        - Transformer(Vaswani, 2017), ViT(Dosovitskiy, 2021; Beal, 2020), Swin(Liu, 2021c), PVT(Wang, 2021c)\n    - 特征融合\n        - 特征金字塔(Lin, 2017a) \n    - 预测网络(分类回归任务)\n        - 两阶段目标检测：全连接\n        - 单阶段目标检测：全卷积\n\n3. 标签分配与损失计算\n    - 标签分配准则\n        - 交并比准则\n            - 基于锚点框与真实框的交并比\n        - 距离准则\n            - 基于无锚点框 ，点到物体中心的距离\n        - 似然估计准则\n            - 分类、回归\n        - 二分匹配准则\n            - 分类、回归\n\n     - 损失函数\n         - 交叉熵函数\n         - 聚焦损失函数(Lin, 2017b)\n         - 回归损失函数: L1损失函数、平滑L1损失函数、IoU损失函数、GIoU损失函数(Reztofighi, 2019)、CIoU损失函数(Zheng, 2020b)\n\n4. 后处理：为每个物体保留一个检测结果，去除冗余结果\n\t- 非极大值抑制NMS\n\t- soft-NMS(Bodla, 2017)\n\t- IoUNet(Jiang, 2018)\n\t- 定位方差(He, 2018)\n\t- 上下文推理(Pato, 2020)\n\n### 基于锚点框方法\n\n描述：为空间每个位置设定多个矩形框（框的尺度和长宽比），尽可能的涵盖图像中的物体\n分类：\n1. 两阶段目标检测\n\t1. 提取k个候选框\n\t2. 对候选框分类和回归\n2. 单阶段目标检测\n\t1. 直接对锚点框分类和回归\n\n### 基于无锚点框方法\n\n分类：\n1. 基于关键点目标检测：多个关键点集成到物体上\n2. 基于内部点目标检测：物体内部点到物体边界的上下左右偏移量\n\n## 基于双目相机流程及其涵盖的方法\n\n方法流程与单目相同\n\n### 基于直接视锥空间\n\n描述：直接使用基础骨干提取的两个单目特征构造双目特征。\n方法：\n1. 串接特征构造\n\t不改变原单目特征的坐标空间\n2. 平面扫描构造\n\t通过逐视差平面或者深度平面地扫描一对2维特征，所得三维特征即是匹配代价体\n\n### 基于显式逆投影空间\n\n描述：将存在尺度变化和遮挡问题的视锥空间图像逆投影到尺度均匀、不存在重叠遮挡的3维空间，从而缓解视锥投影产生的问题。\n方法：\n1. 基于原始图像视差的逆投影方法\n    先利用双目视差估计算法预测每个像素的视差，将像素的视差逆投影到三维空间生成电云，最后利用点云的3维检测方法进行目标检测\n2. 基于特征体的逆投影方法\n    通过插值和采样将平面扫描得到的匹配代价体变换到3维空间，利用了图像特征提供的颜色和纹理信息。\n3. 基于候选像素视差的逆投影方法\n    仅聚焦感兴趣目标区域的三维空间，先利用实例分割方案得到目标的前景像素，然后生成仅含前景区域的3维空间。\n    $$逆投影策略\\left\\{\\begin{align*}\n    \t& 前景共享3维空间\\\\\n    \t& 每个实例生成相互独立的3维子空间\n    \\end{align*}\\right.$$\n    \n## 发展趋势\n1. 高效的端到端目标检测transform，加快收敛，减少计算资源。\n2. 基于自监督学习的目标检测，目标检测任务存在数量和尺度不确定的物体。\n3. 长尾分布目标检测，现实世界物体类别数量庞大且不同类别的物体数量存在极度不平衡。\n4. 小样本、0样本目标检测能力的提高\n5. 大规模双目目标检测数据集少，需要标注物体的2维和3维信息以及相机标注视差和相机参数，还需完善评价体系和开放测试平台\n6. 弱监督双目目标检测\n\n\n# YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors\n\n作者：Chien-Yao Wang，2022\n\n## 解决的问题\n发现的问题:\n1. 模型再参数化\n2. 用动态标签分配技术后，如何将动态标签分配给模型的不同输出层\n\n本篇文章解决的问题：\n1. 最高推理精度56.8%AP和最快推理速度160FPS,都达到最高水平，参与对比的模型有：YOLOv5、YOLOX、Scaled-YOLOv4、YOLOR、PPYOLOE、DETR、Deformable DETR、DINO-5scale-R50、ViT-Adapter-B、SWIN-L Cascade-Mask R-CNN、ConvNeXt-XL Cascade-Mask R-CNN\n2. 支持移动GPU以及边缘端和云端GPU\n3. 设计trainable bag-of-freebies方法，既可增强训练代价提高检测准确度又不增加推理代价\n4. 提出planned re-parameterized model\n5. 提出新的标签分配方法，coarse-to-fine lead guided label assigment\n6. 提出extend and compound scaling方法，减少40%的模型参数和50%计算时间\n\n其他模型的优点和不足：\n1. YOLOX和YOLOR只改进各种GPU推理速度\n2. 基于MobileNet, ShuffleNet, GhostNet针对CPU设计\n3. ResNet, DarkNet, DLA, CSPNet针对GPU设计\n4. YOLO和FCOS具有：1、快而强壮的网络架构，2、高效的特征集成方法，3、鲁棒的损失函数，4、高效的标签分配方法，5、准确的检测方法，6、高效训练方法\n\n\n\n## 当前的不足\n\n\n## 使用的方法\n\n### 模型再参数化\n模型再参数化：融合多个计算模块于一体，可是为组装技术\n\n$$分类\\left\\{\\begin{align*}\n  & 模块间组合\\\\\n  & 模型间组合\n\\end{align*}\\right.$$\n1. 模型间组合方法\n\t1. 在不同训练集中训练多个相同模型，然后再平均模型的参数\n\t2. 在不同的迭代次数间进行模型参数均值化\n2. 模块间组合方法\n\t在训练期间将一个模块分解成多个分支模块，在推理时将多个分支模块整合成一个完整模块\n\t\n### 模型缩放\n模型放缩可以增大和缩小模型使它适合不同计算能力的设备，满足不同的推理速度。\n\n$$放缩因子\\left\\{\\begin{align*}\n  & 分辨率resolution（输出图像尺度）\\\\\n  & 深度depth（隐藏层层数）\\\\\n  & 宽度width（通道数） \\\\\n  & 阶段stage（特征金字塔层数）\n\\end{align*}\\right.$$\n\n放缩方法：网络架构搜索Network architecture search(NAS)，折中了网络参数大小、计算时间、推理速度和精确性\n\n放缩因子的影响：\n1. 对基于非连接的网络架构，在进行模型放缩时由于每个隐藏层的入度和出度不被改变因此可以独立分析每个放缩因子对模型参数数量和计算速度的影响\n2. 对基于连接的网络架构，在进行模型隐藏层深度放大或缩小时紧跟在计算模块后的转移/转化模块的入度会减小或增大，不能独立分析单个尺度因子的影响必须一起分析\n3. 文章提出compound scaling method合成尺度方法既可保持原有模型的的性质又可保持最优结构\n\n### 架构\n选取架构主要考虑1、模型参数数量，2、计算量，3、计算密度\n采用Extended-ELAN(E-ELAN)扩展高效层聚合网络架构，该架构使用扩展基数层、清洗基数层、合并基数层增强网络学习能力\n\n基于早期版本的YOLO框架和YOLOR框架作为基本框架\n\n### 可训练赠品袋trainable bag-of-freebies\n- 计划再参数化卷积\n\t1. 如何将再参数化卷积和不同的网络结合？\n\t2. 提出planned re-parameterized convolution\n\t3. 提出无identity connection的RePConv构造planned re-parameterized convolution\n\t4. 用RepConvN网络层替换3堆叠ELAN架构中不同位置处的3x3卷积层\n\t\n- 以粗为辅以精为主的损失值\n\t1. 深度监督是在网络的中间层添加额外的辅助头，将带有损失值信息的浅层网络权重作为引导方式\n\t2. 将负责最后输出的头称为主头，将用于协助训练的头称为辅头\n\t3. 采用软标签即使用网络预测输出的性质和分布和考虑实际标签，使用一些计算和优化方式生成可靠的标签\n\t4. 如何分配软标签到主头和辅头？\n\t\t1. 分别计算主头和辅头预测结果，使用各自的分配器结合实际结果制作各自标签，再通过各自标签和头计算损失\n\t\t2. 文章提出经分配器用主头和实际结果制作由粗到精的等级标签，再将这些等级标签用在主头和辅头上计算损失值\n\t\n- 批归一化放到conv-bn-activation 拓扑结构中\n\n- YOLOR的隐性知识以串行和并行方式结合到卷积特征图中\n\n- EMA模型\n\n# AN IMAGE IS WORTH 16x16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\n作者：Alexey Dosovitskiy, 2021\n\n## 解决的问题\n根据Transformer计算的效率和可扩展性以及借鉴Transformer在自然语言方面取得的成功将其应用于图像上\n\n1. 证明在大样本上14M-300M图，Transformer胜过CNN\n2. 可以处理中等分辨率图像\n3. 在更大规模数据集而非ImageNet数据集，探索图像识别\n\n\n\n前人工作优点和不足：\n1. 把CNN框架和自注意力结合(Wang,2018; Carion, 2020)\n2. 用自注意力替换整个卷积网络(Ramachandran, 2019; Wang, 2020a)\n3. ResNet架构在大尺度图片识别上是效果好的(Mahajan, 2018; Xie, 2020; Kolesnikov, 2020)\n4. transformer用于机器翻译(Vaswani, 2017)\n5. 将transformer用到图像处理环境中\n\t- 只将自注意力应用于代查询像素的局部临域中，并非全局应用(Parmar,2018)\n\t- 局部多头点积自注意力块完全替换卷积(Hu,2019; Ramachandran, 2019; Zhao, 2020)\n\t- 稀疏Transformer在全局自注意力中使用放缩近似以适应图片(Child, 2019)\n\t- **(Cordonnier, 2020)提出的模型也是ViT但是没有证明大规模预训练模型可以甚至超过CNN模型，使用的2x2块太小只能适应小分辨率图像**\n6. (Sun, 2017)研究CNN性能如何随数据集大小变化\n7. (Kolesnikov,2020; Djolonga,2020)从大规模的数据集上探索CNN的迁移学习\n\n## 当前的不足\n1. Transformer和CNN相比缺少偏移量无法实现平移等变映射和无法进行局部化，因此在小样本中泛化能力弱\n2. **应用ViT到其它计算机视觉任务，例如目标检测和分割**\n3. 持续开发自监督与训练方法\n\n## 使用的方法\n- 分割图片成若干块，给这些块提供顺序线性嵌入体，并将嵌入体作为Transformer的输入\n- 选用原始Transformer(Vaswani, 2017)\n- 框架\n\t![image-20221125100901770](./image-20221125100901770.png)\n\t标准Transformer接收1维符号嵌入序列，将图像$x\\in\\mathbb{R}^{H\\times W\\times C}$分割成有序排列小块$x_p\\in\\mathbb{R}^{N\\times(P^2\\cdot C)}$，输入$z_0=[x_{class}; x_p^1E; x_p^2E; \\cdots; x_p^NE]+E_{pos}$, $x_{class}\\in\\mathbb{R}^{1\\times D}$, $E\\in \\mathbb{R}^{(P^2\\cdot C)\\times D}$, $E_{pos}\\in\\mathbb{R}^{(N+1)\\times D}$\n\t\n# Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions\n作者：Wenhai Wang, 2021\n\n## 解决的问题\n1. 文章(PVT)解决将transform移植到密集预测任务的问题\n2. 产生高输出分辨率，利用收缩的金字塔减少计算量\n3. PVT继承了CNN和Transformer的优点，成为应对各种视觉任务的无需卷积的统一骨干架构，可以直接替换CNN骨干架构\n4. 提高下游任务的性能，包括：目标检测，实例分割，语意分割\n5. 克服传统transformer问题方式\n\t1. 采用获取精细图像块（每个图像块为4x4像素）作为输入，以学习高分辨率的表示\n\t2. 采用逐渐缩小的金字塔形式减小transformer在深层网络中的序列长度，以减小计算量\n\t3. 采用空间减小注意力层spatial-reduction attention(SRA), 这近一步在学习高分辨率特征时减小资源损耗\n6. 具有的优点\n\t1. CNN的局部接收视野随网络深度的增加而增加，PVT产生的是全局接收视野，这有利于检测和分割任务\n\t2. 借助金字塔结构的优势，易于将PVT插入到许多代表密集预测的管道中，例如RetinaNet和Mask R-CNN\n\t3. 通过结合PVT和其它特殊任务的Transformer解码器可以构建无卷积的管道，例如PVT+DETR用作目标检测\n\n\n前人的工作及优缺点\n1. Vision Transformer(ViT)被用作分类任务\n\n2. Vision Transformer(ViT)是一种柱状机构具有粗糙的输入图像块，不是很适合像素级别的致密预测任务如目标检测和分割任务，原因有：1、输出的特征图是单一尺度，分辨率低；2、高计算和内存占用成本, \n\n3. CNN在视觉上取得巨大成功(Karen,2015; Kaiming,2017; Saining,2017)\n\n4. 将视觉任务建模成具有可学习查询功能的字典查找问题，使用Transformer解码器作为特殊任务的头应用于卷积框架的顶层(Nicolas,2020; Christian,2017; Enze,2021)\n\n5. 网络架构\n\n  6. (Yann, 1998)首次引入CNN分辨手写数字，在整个图像空间共享卷积核参数实现平移等变性\n\n  7. (Alex, 2012; Karen, 2015)在大尺度图片分类数据集中使用堆叠的卷积块\n\n  8. GoogLeNet(Christain,2015)包含了多核路径\n\n  9. 多路径卷积模块的效率在Inception系列网络(Christian,2016;)、ResNeXt(Saining, 2017)、 DPN(Yunpeng, 2017)、MixNet(Wenhai, 2018)、SKNet(Xiang,2019)网络中被验证\n\n  10. (Kaiming,2016)在ResNet网络中引入跳跃式连接卷积模块，这有助于训练更深的网络\n\n  11. (Gao,2017)在DenseNet中引入密集连接拓扑结构，把每个卷积模块同它前面的所有卷积模块相连\n\n12. 密集预测任务\n   1. 目的：在特征图上进行像素级别的分类和回归\n   2. $$分类\\left\\{\\begin{align*}\n       & 目标检测\\left\\{\\begin{array}{l}\n                  单阶段\\left\\{\\begin{array}{l}\n                                 SSD(Wei,2016) \\\\\n                                 RetinaNet(Tsung-Yi\\; Lin,2017) \\\\\n                                 FCOS(Zhi\\; Tian,2019) \\\\\n                                 GFL(Xiang,2020) \\\\\n                                 PolarMask(Enze\\;Xie,2021) \\\\\n                                 OneNet(Peize\\; Sun,2020) \\\\\n                              \\end{array}\\right. \\\\\n                  多阶段\\left\\{\\begin{array}{l}\n                                 Faster R-CNN(Shaoqing\\; Ren,2015) \\\\\n                                 Mask R-CNN(Kaiming\\; He,2017) \\\\\n                                 Cascade R-CNN(Zhaowei\\; Cai,2018) \\\\\n                                 Sparse R-CNN(Peize\\; Sun, 2021) \\\\\n                              \\end{array}\\right. \\\\\n                  结合CNN和Transformer\\; decoder\\left\\{\\begin{array}{l}\n                                  DETR(Nicolas\\; Carion, 2020)\\\\\n                                  deformable DETR(Xizhou\\; Zhu,2021)\\\\\n                               \\end{array}\\right.\n               \\end{array}\\right.\\\\\n       & 语意分割 \\left\\{\\begin{array}{l}\n                      FCN(Jonathan\\; Long,2015)\\\\\n                      deconvolution\\; operation(Hyeonwoo\\; Noh,2015)\\\\\n                      U-Net(Olaf\\; Ronneberger,2015)\\\\\n                      添加金字塔池化(HengShuang\\; Zhao,2017)\\\\\n                      添加FPN头(Alexander\\; Kirillov,2019)\\\\\n                      DeepLab(Liang-Chieh\\; Chen,2017)\\\\\n                   \\end{array}\\right.\\\\\n       & 实例分割\n       \\end{align*}\\right.$$\n\n  13. 自注意力和变换器\n\t \t1. 卷积滤波器权重经过训练后被固定无法动态适应不同的输入，(Xu Jia,2016)使用动态滤波器，(Ashish Vaswani,2017)使用自注意力\n\t\t2. 非局部模块被(Xiaolong Wang,2018)引入解决时间和空间在大尺度上的依赖性，但是代价是计算成本和内存占用成本\n\t\t3. (Zilong Huang,2019)引入十字交叉路径Criss-cross减小注意力机制的复杂度\n\t\t4. \n\n\n\n## 当前的不足\n\n1. 有许多特殊模块和运算方法是专门为CNN设计的因此没有在PVT中引入，例如：Squeeze-and-excitation network(SE)模块, Selective kernel network(SK)模块, 膨胀卷积模块, 模型精简模块，Network architecture search(NAS)模块\n2. 基于Transformer 的模型在视觉应用上起步晚可以应用于OCR, 3D和医疗图像分析\n\n## 使用的方法\n### 框架\n整体框架分成4个阶段，每个阶段都产生不同尺度的特征图，每个阶段都具有相似的结构，包括：1、分块嵌入层，2、若干Transformer编码层\n- 结构（以第一阶段为例）\n\t1. 第一阶段输入图像HxWx3, 并分割成$\\frac{HW}{4^2}$多个图块，每块大小4x4x3\n\t\n\t2. 将分割块展平进行线性投影变换得到嵌入块形状为$\\frac{HW}{4^2}\\times C_1$\n\t\n\t3. 将嵌入块和位置嵌入一起传入具有$L_1$层的Transformer编码器中\n\t\n\t4. 输出特征层$F_1$形状是$\\frac{H}{4}\\times\\frac{W}{4}\\times C_1$\n\t\n\t5. 以此类推，以上一阶段输出作为下一阶段的输入，选取的块的大小相对于原始图像分别是8x8，16x16，32x32像素，即第i阶段的块大小为$P_i$，得到的特征图为$F_i$:$\\{F_2$,$F_3$,$F_4\\}$,特征图尺寸为$\\frac{H}{8}\\times\\frac{W}{8}\\times C_2$,$\\frac{H}{16}\\times\\frac{W}{16}\\times C_3$,$\\frac{H}{32}\\times\\frac{W}{32}\\times C_4$,\n\t\n\t   ![image-20221127151329769](./image-20221127151329769.png)\n\t\n- transformer的特征金字塔\n\t- CNN的特征金字塔是使用不同的卷积跨步来实现，PVT是使用逐步缩小策略实现\n\t\n- transformer编码器\n\t1. Transformer编码器在第i阶段有$L_i$个编码层，每一个编码层又包含：1、注意力层，2、feed-forward层\n\t2. 使用spatial-reduction注意力层(SRA)替换传统多头注意力层(MHA)，为了处理高分辨率特征图(4跨步特征图)\n\t    ![image-20221127153059574](./image-20221127153059574.png)\n\t\tSRA特点：减小Key和Value输入的尺寸：\n\t\t$$\\begin{align*}\n\t\t\t\tSRA(Q,K,V)&=Concat(head_0,\\cdots,head_{N_i})W^O\\\\\n\t\t\t\thead_j&=Attention(QW_j^Q,SR(K)W_j^K,SR(V)W_j^V)\\\\\n\t\t\t\tSR(x)&=Norm(Reshape(x,R_i)W^S)\\\\\n\t\t\t\tAttention(\\vec{q},\\vec{k},\\vec{v})&=Softmax(\\frac{\\vec{q}\\vec{k}^T}{\\sqrt{\\vec{d}_{head}}})\\vec{v}\\\\\n\t\t\\end{align*}$$\n\t\n\t\t符号说明：$Concat(\\cdot)$链接操作；$W_j^Q\\in\\mathbb{R}^{C_i\\times d_{head}}$,$W_j^K\\in\\mathbb{R}^{C_i\\times d_{head}}$,$W_j^V\\in\\mathbb{R}^{C_i\\times d_{head}}$,$W^O\\in\\mathbb{R}^{C_i\\times C_i}$,$W^S\\in\\mathbb{R}^{(R_i^2C_i)\\times C_i}$都是线性投影矩阵；$N_i$注意力的头数；$d_{head}=\\frac{C_i}{N_i}$每个头的大小；$SR(\\cdot)$减小空间尺度操作；$x\\in\\mathbb{R}^{(H_iW_i)\\times C_i}$表示输入序列；$R_i$表示缩减比例；$Reshape(x,R_i)=\\frac{H_iW_i}{R_i^2}\\times(R_i^2C_i)$修改张量形状;$Norm(\\cdot)$层归一化；$Attention(\\cdot,\\cdot,\\cdot)$注意力得分\n\n\n# Deformable DETR: Deformable Transformers For End-to-End Object Detection\n作者：Xinzhou Zhu,2020\n\n## 解决的问题\n1. 改善空间分辨率问题和收敛慢问题\n2. Deformable DETR在检测小目标上优于DETR\n3. 结合了deformable convolution的稀疏空间采样和transformer的相关性建模能力\n\n前人的工作以及优缺点：\n1. DETR(Nicolas Carion,2020)用于剔除目标检测中辅助成分的需求（例如，非极大值抑制），缺点是收敛慢、有限的特征空间分辨率，该模型结合了CNN模型和Transformer编解码模型，小目标效果差，优点就是具有元素间的相关性\n2. 目标检测使用了一些辅助成分(Li Liu,2020),例如锚点生成，基于规则的训练目标分配，非极大抑制\n3. deformable convolution(Jifeng Dai,2017)用于稀疏空间定位，因此高效、收敛快，缺点就是缺少元素间的相关性\n4. (Ashish Vaswani,2017)在transformer中引入自注意力和交叉注意力机制,缺点是时间成本和内存占用高\n5. 解决时间和内存的方式有三种\n\t1. 对关键点使用预定义(冻结参数)的稀疏注意力模式\n\t\t- 将注意力模式限制在固定局部窗口中使用(Peter J Liu,2018a;Niki Parmar,2018;Rewon Child,2019;Zilong Huang;2019),优点是减少复杂度，缺点是丢失全局信息\n\t\t- 以固定间隔方式设置关键点(Jonathan Ho,2019)，优点：增加接受视野\n\t\t- 允许少许特殊符号可以访问所有关键点(Iz Beltagy,2020),增加全局信息\n\t2. 学习依赖数据的稀疏注意力\n\t\t- 基于注意力机制的局部敏感哈希映射LSH(Nikita Kitaev,2020),将查询元素和关键字元素映射到不同的区域\n\t\t- (Aurko Roy,2020)用k-means聚类找到最相关的关键字元素\n\t\t- (Yi Tay,2020a)对逐块的稀疏注意力学习块交换\n\t3. 研究自注意力机制的低级别性质\n\t\t- 通过线性投影减少关键点数量(Sinong Wang,2020)\n\t\t- 通过核函数近似计算自注意力(Krzysztof Choromanski,2020)\n6. 多尺度特征表示\n\t- FPN(Tsung-Yi Lin,2017a),自上而下生成多尺度特征图\n\t- PANet(Shu Liu,2018b)，自下而上生成多尺度特征图\n\t- 从全局自注意力中提取所有尺度上的特征(Tao Kong,2018)\n\t- U-shape模块融合多尺度特征(Qijie Zhao,2019)\n\t- NAS-FPN(Golnaz Ghiasi,2019)、Auto-FPN(Hang Xu,2019)通过神经架构搜索自动进行交叉尺度连接\n\t- BiFPN(Mingxing Tan,2020)\n\n\n\n\n## 当前的不足\n1. 简单且高效的可迭代的边界框调优机制进一步改善性能\n2. 可将Deformable DETR应用到2阶段目标识别中，先生成推荐区域，再将推荐区域作为目标query送入解码器\n\n\n\n## 使用的方法\n1. 应用在若干的采样位置点处，这些点作为特征图中关键特征点\n2. 使用图像尺度放缩而不是特征金字塔应用于deformable注意力模型\n\n### 经典多头注意力结构\nquery元素代表了要输出的句子的目标单词，key元素代表输入句子中的单词，多头注意力模块根据测量的query-key对的相似性权重因子汇聚这些key。用$q\\in\\Omega_q$索引具有表达特征$z_q\\in\\mathbb{R}^C的$query元素；用$k\\in\\Omega_k$索引具有表达特征$x_k\\in\\mathbb{R}^C的$key元素,$C$是特征维度，$\\Omega_q$和$\\Omega_k$给出了query和key的元素总数;多头注意力特征计算：\n\n$$MultiHeadAttn(z_q,x)=\\sum\\limits_{m=1}^MW_m[\\sum\\limits_{k\\in\\Omega_k}A_{mqk}\\cdot W_m^\\prime x_k]$$\n\n这里$m$索引各个注意力头总共有M个注意力头，$W_m^\\prime\\in\\mathbb{C_v\\times C}$和$W_m\\in\\mathbb{C\\times C_v}$是第m头待学习的矩阵，$C_v=C/M$,注意力权重$A_{mqk}\\propto \\exp\\{\\frac{z_q^TU_m^TV_mx_k}{\\sqrt{C_v}}\\}$满足归一化$\\sum\\limits_{k\\in\\Omega_k}A_{mqk}=1$, 这里$U_m\\in\\mathbb{R}^{C_v\\times C}$和$V_m\\in\\mathbb{R}^{C_v\\times C}$同样是待学习的矩阵。为了消除不同空间位置的奇异性，表达的查询和关键字特征$z_q$和$x_k$需要和位置嵌入体做结合。\n\n### DETR Transformer编解码架构\n采用Hungarian(匈牙利)损失函数借助二分(双边)匹配实现对每一个真实边框都有唯一预测值。用$y$表示ground truth集合，$\\hat{y}=\\{\\hat{y}_i\\}_{i=1}^N$表示有N个预测值的集合，当$N$远大于图像中物体个数时，可以认为$y$集合也是由N个真实结果构成没有目标的结果被符号$\\phi$填充。搜索N个元素$\\sigma\\in\\mathfrak{G}_N$的一个置换$\\hat{\\sigma}=\\mathop{\\arg\\min}\\limits_{\\sigma\\in\\mathfrak{G}_N}\\sum\\limits_i^N\\mathcal{L}_{match}(y_i,\\hat{y}_{\\sigma(i)})$, （1）$\\mathcal{L}_{match}(y_i,\\hat{y}_{\\sigma(i)})$是真值$y_i$和具有索引$\\sigma(i)$的预测值之间的逐对匹配代价函数；（2）第i个真值元素可以看成$y_i=(c_i,b_i)$,$c_i$是目标类别标签，符号$\\phi$用N/A表示，$b_i=\\{b_x,b_y,b_w,b_h\\}\\in[0,1]^4$是向量记录了中心坐标和相对图像大小的高度和宽度；（3）对索引$\\sigma(i)$的预测值，定义类$c_i$的概率为$\\hat{p}_{\\sigma(i)}(c_i)$,预测边框为$\\hat{b}_{\\sigma(i)}$，定义：\n\n$$\\mathcal{L}_{match}(y_i,\\hat{y}_{\\sigma(i)})=-1_{\\{c_i\\neq\\phi\\}}\\hat{p}_{\\sigma(i)}(c_i)+1_{\\{c_i\\neq\\phi\\}}\\mathcal{L}_{box}(b_i,\\hat{b}_{\\sigma(i)})$$\n$$\\mathcal{L}_{box}(b_i,\\hat{b}_{\\sigma(i)})=\\lambda_{iou}\\mathcal{L}_{iou}(b_i,\\hat{b}_{\\sigma(i)})+\\lambda_{L1}||b_i-\\hat{b}_{\\sigma(i)}||_1$$\n\n这是真实框集合中的每一框和预测框集合中的每一个框匹配，损失值最小的预测框为该真实框的最佳匹配框，得到唯一匹配。匈牙利损失函数定义如下：\n\n$$\\mathcal{L}_{Hungarian}(y,\\hat{y})=\\sum\\limits_{i=1}^N\\left[-1_{\\{c_i\\neq\\phi\\}}\\hat{p}_{\\hat{\\sigma}(i)}(c_i)+1_{\\{c_i\\neq\\phi\\}}\\mathcal{L}_{box}(b_i,\\hat{b}_{\\hat{\\sigma}(i)})\\right]$$\n\nDETR主要使用Transformer编码解码架构将特征图像映射成一组待查目标对象的特征，feed-forward网络FFN用做回归分支预测边框坐标，线性投影用作分类分支预测目标类别。编码部分输入ResNet的生成的特征图，取query和key元素，它们都是特征图的每一个像素；解码部分输入包含来自编码部分的特征图和N个目标查询，有两个注意力模型：1、cross-attention,2、self-attention。在cross-attention中查询元素是从特征图中提取特征的N个目标查询，关键字元素来自于编码输出的特征图；在self-attention中，查询元素要获取他们的关系因此查询元素和关键字元素都来自于N个目标查询\n\n### Deformable transformer\n1. 单尺度Deformable attention\n\t通过对每个查询元素只分配少许固定数量的关键字元素，这些关键字元素取自一个参考点附近的点，无需对所有关键字进行匹配，所以无视特征图尺度，已知特征图$x\\in\\mathbb{R}^{C\\times H\\times W}$,$q$是查询元素特征$z_q$索引以及分配的一个2维参考点$p_q$索引\n\t$$DeformAttn(z_q,p_q,x)=\\sum\\limits_{m=1}^MW_m\\left[\\sum\\limits_{k=1}^KA_{mqk}\\cdot W_m^\\prime x(p_q+\\Delta p_{mqk})\\right]$$\n\t这里m是注意力头的索引，k是采样到的关键字元素索引，K是采样关键字总数，显然$K\\ll HW$。$\\Delta p_{mqk}$和$A_{mqk}$表示对第m注意力头、第k个关键字元素采样点的采样邻域半径和注意力权重，注：标量注意力权重$A_{mqk}\\in [0,1]$要归一化处理, $\\Delta p_{mqk}\\in \\mathbb{R}^2$没有约束范围，当$p_q+\\Delta p_{mqk}$是小数时双线性插值可以使用，$\\Delta p_{mqk}$和$A_{mqk}$是通过对查询元素特征$z_q$线性投影计算得到\n\n2. 多尺度Deformable attention\n\t设$\\{x^l\\}_{l=1}^L$是用于输入的多尺度特征图，$x^l\\in\\mathbb{R}^{C\\times H_l\\times W_l}$, $\\hat{p}_q\\in[0,1]^2$是每个查询元素q对应参考点的归一化二维坐标，坐标的归一化操作是对每一个尺度特征图进行, 模型公式为\n\t$$MSDeformAttn(z_q,\\hat{p}_q,\\{x^l\\}_{l=1}^L)=\\sum\\limits_{m=1}^MW_m\\left[\\sum\\limits_{l=1}^L\\sum\\limits_{k=1}^KA_{mlqk}\\cdot W_m^\\prime x^l(\\phi_l(\\hat{p}_q)+\\Delta p_{mlqk})\\right]$$\n\tm是注意力头索引，l是输入特征图的尺度等级索引，k是采样点的索引，$\\Delta p_{mlqk}$和$A_{mlqk}$表示在l层尺度上的特征图、对第m注意力头、第k个关键字元素采样点的采样邻域半径和注意力权重，$\\phi_l(\\hat{p}_q)$将归一化的坐标缩放回尺度为l的特征图中的坐标\n\n3. 编码器\n\t编码器的输入输出都是具有相同分辨率的多尺度特征图，编码器的多尺度特征图$\\{x^l\\}_{l=1}^{L-1},L=4$取自ResNet的输出特征图$C_3$到$C5$分辨率分别为$H/2^3$到$H/2^5$,而最低分辨率的特征图$x^4$取自$C_5$特征图进行3x3步长2的卷积后得到的特征图$C_6$,所有的输入特征图的通道数都是256：\n\t![image-20221130151059959](./image-20221130151059959.png)\n\tquery和key元素都是多尺度特征图的像素，每个query像素的参考点就是其自身，为了识别每个query像素在哪个尺度图上，除了添加位置嵌入体外，需要添加尺度嵌入体$e_l$到特征表达中，区别是位置嵌入体是固定编码，尺度嵌入体$\\{e_l\\}_{l=1}^L$需要连同网络一起训练获取。\n\n4. 解码器\n\t包含cross-attention和self-attention,query元素在两类注意力机制中都是目标query，目标query在cross-attention中取自特征图，而key元素是编码器的输出特征图；在self-attention中，key元素是目标query。每个目标query参考点的二维归一化坐标需要经过线性投影和激活函数从目标query的嵌入体中给出\n\t![image-20221130160631440](./image-20221130160631440.png)\n\t模型提取的图像特征都是在参考点周围的点，所以我们预测的边框都是相对于参考点的偏移量，参考点初始值都是边框的中心点\n\n\n# End-to-End Object Detection with Transformers\n作者：Nicolas Carion,2020\n\n## 解决的问题\n1. 提出目标检测新方法DEtection TRansformer(DETR)：将目标检测看作单向集合预测问题\n2. 不再使用辅助设计成分，例如非极大值抑制抑制，锚点，是完全端到端的理念，流水化检测过程直接预测具有相对输入图像的绝对边框的检测集合\n3. 结合了二分匹配损失和具有并行解码(非自动回归)的transformer，损失函数对预测目标交换具有不变性，因此可以并行执行\n4. 对大目标具有更好的性能，可能是由于transformer的非局部计算\n\n\n前人的工作及其优缺点：\n1. 以间接方式解决预测边框和种类任务，通过对大量推荐边框(Ren,S.,2015; Cai,Z.,2019)、锚点(Lin,T.Y.,2017)和窗口中心(Zhou,X.,2019;Tian,Z.,2019)定义回归和分类问题,通过后处理步骤削减近邻重复预测\n2. (Stewart,R.J.,2015;Hosang,J.H.,2017;Bodla,N.,2017,Rezatofighi,S.H.,2018)要么添加了其他形式的先验内容，要么在有挑战的基准样本上没有证明有强大竞争力\n3. (Stewart,R.J.,2015;Romera-Paraedes,B.,2015;Park,E.,2015)关注具有RNN自动回归解码\n4. 集合预测\n\t基础的集合预测任务是多标签分类(Rezatofighi,S.H.,2017;Pineda,L.,2019)而目标检测要考虑元素间暗含的结构即识别近邻边框，因此不能用one-vs-rest方式处理问题。使用后处理方式例如非极大抑制处理近邻重叠问题；利用全局的对所有预测结果建立相互影响的推理方案规避这种冗余问题。集合预测可以使用致密的全连接网络(Erhan,D.,2014)、自动回归序列模型(recurrent neural network:Vinyals,O.,2016)。损失函数可以基于Hungarian算法(Kuhn,H.W.,1955)保证交换不变性和每个目标元素都有唯一匹配\n5. transformer和并行解码\n\tTransformer是一个基于注意力机制的搭建模块用于机器翻译(Vaswani,A.,2017)；注意力机制是一个神经网络层，可以从整个输入序列中汇集信息(Bahdanau,D.,2015)；Transformer用在自动回归模型，根据之前的句子到句子模型(Sutskever,I.,2014)生成一个接一个的输出符号,这种方法缺点是推理成本(正比于输出长度)和很难批处理，所以需要开发并行序列生成，可以应用在语音(Oord,A.v.d.,2017)、机器翻译(Gu,J.,2018;Ghazvininejad,M.,2019)、单词表达学习(Devlin,J.,2019)、语音识别(Chan,W.,2020)。将transformer和并行解码结合权衡了计算成本和集合预测需要的全局计算能力\n6. 目标检测\n\t目标检测根据一些初始猜测做预测，两阶段目标检测是根据推荐框(Ren,S.,2015;Cai,Z.,2019)做预测，单阶段目标检测是根据锚点(Lin,T.Y.,2017)或可能的目标中心网格(Zhou,X.,2019;Tian,Z.,2019)做预测,这类目标检测严重依赖初始猜测设置方式(Zhang,S.2019)。\n\t- 基于集合损失\n\t\t早期的深度学习模型只用卷积和全连接层建模不同预测的关系并用辅助后处理改善性能；最近有些检测器使用真值和预测值之间的非唯一分配规则并结合NMS(Ren,S.,2015;Lin,T.Y.,2017;Zhou,X.,2019);可学习NMS方法(Hosang,J.H.,2017;Bodla,N.,2017)和关系网络(Hu,H.,2018)利用注意力显式建模不同预测间的关系，直接使用集合损失不在需要后处理步骤，但是也需要上下文特征例如推荐框坐标。\n\t- 循环探测\n\t\t端到端方法(Stewart,R.J.,2015;Romera-Paredes,B.,2015;Park,E.,2015;Ren,M.,2017)使用二分匹配损失以及基于CNN的编解码框架直接产生一组边界框，这些方法只在小样本数据中评估，基于的是自动回归模型没有利用具有并行解码的transformer。\n\n\n## 当前的问题\n1. 对小目标性能不佳，可以用FPN进行改善\n2. 长时间的训练\n\n\n## 使用的方法\n基于一组全局损失，通过二分匹配给出唯一预测，使用transformer编解码架构，需要一组固定数量的学习好的目标query, DETR会推理目标和全局图像的关系然后输出最终的预测集合\n![image-20221201110746341](./image-20221201110746341.png)\n\n![image-20221202090850052](./image-20221202090850052.png)\n\n### 目标检测集合预测损失\n\nDETR推理N个固定大小的预测结果集合，N明显多于图像中典型的目标数量，给出预测目标（类别，位置，大小）同真值比较后的好坏。用$y$表示ground truth目标集合，$\\hat{y}=\\{\\hat{y}_i\\}_{i=1}^N$表示有N个预测值的集合，当$N$远大于图像中物体个数时，可以认为$y$集合也是由N个真实结果构成没有目标的结果被符号$\\phi$填充。两个集合之间进行二分匹配，搜索N个元素$\\sigma\\in\\mathfrak{G}_N$的一个置换以得到最小代价$\\hat{\\sigma}=\\mathop{\\arg\\min}\\limits_{\\sigma\\in\\mathfrak{G}_N}\\sum\\limits_i^N\\mathcal{L}_{match}(y_i,\\hat{y}_{\\sigma(i)})$, （1）$\\mathcal{L}_{match}(y_i,\\hat{y}_{\\sigma(i)})$是真值$y_i$和具有索引$\\sigma(i)$的预测值之间的逐对匹配代价函数；（2）第i个真值元素可以看成$y_i=(c_i,b_i)$,$c_i$是目标类别标签，符号$\\phi$用N/A表示，$b_i=\\{b_x,b_y,b_w,b_h\\}\\in[0,1]^4$是向量记录了中心坐标和相对图像大小的高度和宽度；（3）对索引$\\sigma(i)$的预测值，定义类$c_i$的概率为$\\hat{p}_{\\sigma(i)}(c_i)$,预测边框为$\\hat{b}_{\\sigma(i)}$，定义：\n\n$$\\mathcal{L}_{match}(y_i,\\hat{y}_{\\sigma(i)})=-1_{\\{c_i\\neq\\phi\\}}\\hat{p}_{\\sigma(i)}(c_i)+1_{\\{c_i\\neq\\phi\\}}\\mathcal{L}_{box}(b_i,\\hat{b}_{\\sigma(i)})$$\n$$\\mathcal{L}_{box}(b_i,\\hat{b}_{\\sigma(i)})=\\lambda_{iou}\\mathcal{L}_{iou}(b_i,\\hat{b}_{\\sigma(i)})+\\lambda_{L1}||b_i-\\hat{b}_{\\sigma(i)}||_1$$\n\n这是真实框集合中的每一框和预测框集合中的每一个框匹配，损失值最小的预测框为该真实框的最佳匹配框，得到唯一匹配。作用类似于用于匹配的启发式分配规则（匹配推荐框、锚点）区别只是一一匹配。根据二分匹配损失计算匈牙利损失函数(类别的负对数似然函数损失和边框损失线性组合)，定义如下：\n$$\\mathcal{L}_{Hungarian}(y,\\hat{y})=\\sum\\limits_{i=1}^N\\left[-\\log\\,\\hat{p}_{\\hat{\\sigma}(i)}(c_i)+1_{\\{c_i\\neq\\phi\\}}\\mathcal{L}_{box}(b_i,\\hat{b}_{\\hat{\\sigma}(i)})\\right]$$\n$\\hat{\\sigma}$是二分分配损失计算得到的最优预测值索引，注：$c_i=\\phi$的样本通常比目标类别多很多，属于不平衡样本集，所以对于无目标类别将对数几率的权重减少到$1/10$。别的目标检测器对边界框的预测是基于和初始猜测边框的偏差，这里是直接生成边框预测，$\\ell_1$损失的缺点就是有损失值的相对尺度问题，即大目标的大边框和小目标的小边框都可能产生相似的$\\ell_1$损失，因此无法通过该损失区分大小尺寸，需要结合具有尺度不变性的交并比损失，注两个损失值还需要在批中样本上做归一化处理。\n\n### DETR框架\n\n架构三部分组成：1、CNN骨架（提取出致密特征表达），2、transformer编解码，3、feed forward network,FFN(生成最终的预测)。\n\n#### CNN\n初始图片$x_{img}\\in\\mathbb{R}^{3\\times H_0\\times W_0}$, CNN输出低分辨率的激活图$f\\in\\mathbb{R}^{C\\times H\\times W}$,$C=2048$,$H,W=\\frac{H_0}{32},\\frac{W_0}{32}$\n\n#### transformer编码器\n1. 用1x1卷积核减少高层激活图$f$的通道维度从$C$到$d$，新的特征图为$z_0\\in\\mathbb{R}^{d\\times H\\times W}$\n2. 编码器需要输入特征序列，将$z_0$的空间维度塌缩到一个维度$z_0\\in\\mathbb{R}^{d\\times HW}$\n3. 编码器包含：1、多头注意力模块，2、feed forward网络层，编码器具有输入序列的交换不变性，因此需要向序列中添加固定位置嵌入体\n\n#### transformer解码器\n- 使用标准的多头自注意力和编解码注意力机制的transformer架构，但是区别是解码器每层并行解码N个目标，原来的模型是使用自动回归模型一次预测输出序列中的一个元素。\n- 由于解码器也是对输入序列具有交换不变性，因此也需要让N个输入嵌入是不同的以生成不同结果，这N个输入嵌入体就是位置编码也称作N个目标查询，将他们添加到每一个注意力层的输入部分。\n- N个目标查询被解码器转换成输出嵌入体\n\n\n#### FFN网络层\n- 具有ReLU激活函数的3层感知网络、线性投影层\n- FFN将N个输出嵌入体独立分解成N个边框坐标和类别标签，即最终预测结果\n- 边框坐标是根据输入图像做归一化\n- 类别标签使用softmax函数给出\n\n####补充多头注意力层\n\n**多头注意力**\n有M个注意力头，每个头的维度是d，多头注意力是一个函数函数的输入输出如下，注：$d^\\prime=\\frac{d}{M}$，在花括号中列出了矩阵/张量的大小\n\n$$\\operatorname{mh-attn}: \\underbrace{X_q}\\limits_{d\\times N_q},\\underbrace{X_{kv}}\\limits_{d\\times N_{kv}},\\underbrace{T}\\limits_{M\\times 3\\times d^\\prime\\times d},\\underbrace{L}\\limits_{d\\times d}\\;\\mapsto\\;\\underbrace{\\tilde{X}_q}\\limits_{d\\times N_q}$$\n\n展开为每个单头计算结果在特征阶次上的连接\n$$\\operatorname{mh-attn}(X,X,T,L)=L\\left[attn(X_q,X_{kv},T_1);\\cdots;attn(X_q,X_{kv},T_M)\\right]$$\n\n$X_q$是长度为$N_q$的查询序列，$X_{kv}$是长度为$N_{kv}$的键-值序列，$T$是权重张量用于计算所谓的查询、键值、得分嵌入体，$L$是投影矩阵。多头自注意力是$X_q=X_{kv}$的特殊情况\n\n$$\\operatorname{mh-s-attn}(X,T,L)=\\operatorname{mh-attn}(X,X,T,L)$$\n\n一般在得到$\\tilde{X}_q$后还要在进行残差连接、dropout和层归一化处理\n$$\\hat{X}_q=layernorm(X_q+dropout(\\tilde{X}_q))$$\n\n**单头注意力**\n单头注意力的权重张量$T^\\prime\\in\\mathbb{R}^{3\\times d^\\prime\\times d}$，在计算单头注意力时还要引入位置编码嵌入体$P_q\\in\\mathbb{R}^{d\\times N_q}$和$P_{kv}\\in\\mathbb{R}^{d\\times N_{kv}}$。\n首先计算添加位置编码后的查询、键值、得分嵌入体，计算复杂度$\\mathcal{O}(d^\\prime N_q\\cdot d +d^\\prime N_{kv} \\cdot d)$，$Q_i,K_i:\\mathcal{O}(d^\\prime d)$\n$$\\left[Q;K;V\\right]=\\left[T_1^\\prime(X_q+P_q);T_2^\\prime(X_{kv}+P_{kv});T_3^\\prime X_{kv}\\right]$$\n$T^\\prime$就是$T_1^\\prime,T_2^\\prime,T_3^\\prime$在第一阶次上的连接\n\n再计算注意力权重$\\alpha$,通过query嵌入体和键值嵌入体点积运算和softmax运算\n因此可以得到查询序列中的每个查询嵌入体和键值序列中的每个键值嵌入体之间的权重（相似性），计算复杂度$\\alpha_{i,j}:\\mathcal{O}(d^\\prime)$：\n$$\\alpha_{i,j}=\\frac{e^{\\frac{1}{\\sqrt{d^\\prime}}Q_i^TK_j}}{Z_i}\\quad\\quad Z_i=\\sum\\limits_{j=1}^{N_{kv}}e^{\\frac{1}{\\sqrt{d^\\prime}}Q_i^TK_j}$$\n\n最后计算带注意力权重$\\alpha$的第i个嵌入体在特征维度上的加权得分分布,计算复杂度$\\mathcal{O}(N_{kv}\\cdot d^\\prime)$\n$$attn(X_{q_i},X_{kv},T^\\prime)=\\sum\\limits_{j=1}^{N_{kv}}\\alpha_{i,j}V_j$$\n在查询序列阶次上连接,计算复杂度$\\mathcal{O}(N_qN_{kv}\\cdot d^\\prime)$\n$$attn(X_q,X_{kv},T^\\prime)=[attn(X_{q_1},X_{kv},T^\\prime);\\cdots;attn(X_{q_{N_q}},X_{kv},T^\\prime)]$$\n\n![image-20221203145853415](./image-20221203145853415.png)\n","source":"_posts/目标识别.md","raw":"---\ntypora-root-url: ./目标识别\ntitle: 目标识别\nmathjax: true\ndate: 2022-11-22 09:13:23\ncategories:\n- 文献\n- 目标识别\ntags:\n- 目标识别\n---\n\n[TOC]\n\n# 基于深度学习的视觉目标检测技术综述\n\n作者：曹家乐，2022\n\n## 发展历程\n\n1. 基于手工设计特征的方法\n    1. 支持向量机\n    2. AdaBoost\n    3. Haar特征(Viola, 2004)\n    4. 方向梯度直方图histograms of oriented gradients(Dalal, 2005)\n2. 深度学习\n    1. 区域卷积神经网络region-based convolutional neural network, R-CNN (Girshick, 2014)\n    2. 单次检测single shot detector, SSD (Liu, 2016)\n    3. yolo (Redmon, 2016)\n    4. detection transformer, DETR (Carion, 2020)\n\n深度网络模型：\n1. AlexNet(Krizhevsky,2012)\n2. GoogLeNet(Szegedy,2015)\n3. VGGNet(Simonyan,2015)\n4. ResNet(He,2016)\n5. DenseNet(Huang,2017)\n6. Mobilenet(Howard,2018)\n7. ShuffleNet(Zhang,2018)\n8. SENet(Hu, 2018)\n9. EfficientNet(Tan,2019)\n10. ViT(Dosovitskiy,2021)\n11. Swin(Liu,2022)\n\n目标检测方法\n1. DetectorNet(Szegedy,2014)\n2. R-CNN(Girshick,2014)\n3. OvearFeat(Sermanet,2014)\n4. SPPNet(He,2015)\n5. Fast R-CNN(Girshick,2016; Ren,2016)\n6. YOLO(Redmon,2016)\n7. SSD(Liu,2016)\n8. R-FCN(Dai,2017)\n9. FPN(Lin,2017)\n10. YOLOv2(Redmon,2017)\n11. Mask RCNN(He,2018)\n12. DCN(Dai,2018)\n13. RetinaNet(Lin,2018)\n14. Cascade RCNN(Cai,2018)\n15. YOLOv3(Redmon,2019)\n16. CornerNet(Law,2019)\n17. FCOS(Tian,2020)\n18. CenterNet(Zhou,2020)\n19. EfficientDet(Tan,2020)\n20. ATSS(Zhang,2020)\n21. MoCo(He,2020)\n22. YOLOv4(Bochkovskiy,2021)\n23. Deformable DETR(Zhu,2021)\n24. DETR(Carion,2021)\n25. YOLOv5(Jocher,2021)\n26. UP-DETR(Dai,2021)\n27. Pix2seq(Chen,2022)\n\n\n## 检测设备\n\n1. 单目相机\n2. 双目相机 （提供三维信息）\n\n\n## 基于单目相机流程及其涵盖的方法\n\n1. 数据预处理\n    - 翻转\n    - 放缩\n    - 均值归一化\n    - 色调变化\n    - 剪切、擦除、分区(DeVries, 2017; Zhong, 2020b; Singh, 2017; Chen, 2020a)\n    - 混合(Mixup: Zhang, 2018; CutMix: Yun, 2019; Fang, 2019; Mosaic: Bochkovskiy, 2020; Montage: Zhou, 2020; dynamic scale training: Chen, 2020b)\n\n2. 检测网络\n    - 基础骨架\n        - AlexNet(Krizhevsky, 2012)\n        - VGGNet(Simonyan, 2014) \n        - ResNet(He, 2016)\n        - DenseNet(Huang, 2017)\n        - Transformer(Vaswani, 2017), ViT(Dosovitskiy, 2021; Beal, 2020), Swin(Liu, 2021c), PVT(Wang, 2021c)\n    - 特征融合\n        - 特征金字塔(Lin, 2017a) \n    - 预测网络(分类回归任务)\n        - 两阶段目标检测：全连接\n        - 单阶段目标检测：全卷积\n\n3. 标签分配与损失计算\n    - 标签分配准则\n        - 交并比准则\n            - 基于锚点框与真实框的交并比\n        - 距离准则\n            - 基于无锚点框 ，点到物体中心的距离\n        - 似然估计准则\n            - 分类、回归\n        - 二分匹配准则\n            - 分类、回归\n\n     - 损失函数\n         - 交叉熵函数\n         - 聚焦损失函数(Lin, 2017b)\n         - 回归损失函数: L1损失函数、平滑L1损失函数、IoU损失函数、GIoU损失函数(Reztofighi, 2019)、CIoU损失函数(Zheng, 2020b)\n\n4. 后处理：为每个物体保留一个检测结果，去除冗余结果\n\t- 非极大值抑制NMS\n\t- soft-NMS(Bodla, 2017)\n\t- IoUNet(Jiang, 2018)\n\t- 定位方差(He, 2018)\n\t- 上下文推理(Pato, 2020)\n\n### 基于锚点框方法\n\n描述：为空间每个位置设定多个矩形框（框的尺度和长宽比），尽可能的涵盖图像中的物体\n分类：\n1. 两阶段目标检测\n\t1. 提取k个候选框\n\t2. 对候选框分类和回归\n2. 单阶段目标检测\n\t1. 直接对锚点框分类和回归\n\n### 基于无锚点框方法\n\n分类：\n1. 基于关键点目标检测：多个关键点集成到物体上\n2. 基于内部点目标检测：物体内部点到物体边界的上下左右偏移量\n\n## 基于双目相机流程及其涵盖的方法\n\n方法流程与单目相同\n\n### 基于直接视锥空间\n\n描述：直接使用基础骨干提取的两个单目特征构造双目特征。\n方法：\n1. 串接特征构造\n\t不改变原单目特征的坐标空间\n2. 平面扫描构造\n\t通过逐视差平面或者深度平面地扫描一对2维特征，所得三维特征即是匹配代价体\n\n### 基于显式逆投影空间\n\n描述：将存在尺度变化和遮挡问题的视锥空间图像逆投影到尺度均匀、不存在重叠遮挡的3维空间，从而缓解视锥投影产生的问题。\n方法：\n1. 基于原始图像视差的逆投影方法\n    先利用双目视差估计算法预测每个像素的视差，将像素的视差逆投影到三维空间生成电云，最后利用点云的3维检测方法进行目标检测\n2. 基于特征体的逆投影方法\n    通过插值和采样将平面扫描得到的匹配代价体变换到3维空间，利用了图像特征提供的颜色和纹理信息。\n3. 基于候选像素视差的逆投影方法\n    仅聚焦感兴趣目标区域的三维空间，先利用实例分割方案得到目标的前景像素，然后生成仅含前景区域的3维空间。\n    $$逆投影策略\\left\\{\\begin{align*}\n    \t& 前景共享3维空间\\\\\n    \t& 每个实例生成相互独立的3维子空间\n    \\end{align*}\\right.$$\n    \n## 发展趋势\n1. 高效的端到端目标检测transform，加快收敛，减少计算资源。\n2. 基于自监督学习的目标检测，目标检测任务存在数量和尺度不确定的物体。\n3. 长尾分布目标检测，现实世界物体类别数量庞大且不同类别的物体数量存在极度不平衡。\n4. 小样本、0样本目标检测能力的提高\n5. 大规模双目目标检测数据集少，需要标注物体的2维和3维信息以及相机标注视差和相机参数，还需完善评价体系和开放测试平台\n6. 弱监督双目目标检测\n\n\n# YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors\n\n作者：Chien-Yao Wang，2022\n\n## 解决的问题\n发现的问题:\n1. 模型再参数化\n2. 用动态标签分配技术后，如何将动态标签分配给模型的不同输出层\n\n本篇文章解决的问题：\n1. 最高推理精度56.8%AP和最快推理速度160FPS,都达到最高水平，参与对比的模型有：YOLOv5、YOLOX、Scaled-YOLOv4、YOLOR、PPYOLOE、DETR、Deformable DETR、DINO-5scale-R50、ViT-Adapter-B、SWIN-L Cascade-Mask R-CNN、ConvNeXt-XL Cascade-Mask R-CNN\n2. 支持移动GPU以及边缘端和云端GPU\n3. 设计trainable bag-of-freebies方法，既可增强训练代价提高检测准确度又不增加推理代价\n4. 提出planned re-parameterized model\n5. 提出新的标签分配方法，coarse-to-fine lead guided label assigment\n6. 提出extend and compound scaling方法，减少40%的模型参数和50%计算时间\n\n其他模型的优点和不足：\n1. YOLOX和YOLOR只改进各种GPU推理速度\n2. 基于MobileNet, ShuffleNet, GhostNet针对CPU设计\n3. ResNet, DarkNet, DLA, CSPNet针对GPU设计\n4. YOLO和FCOS具有：1、快而强壮的网络架构，2、高效的特征集成方法，3、鲁棒的损失函数，4、高效的标签分配方法，5、准确的检测方法，6、高效训练方法\n\n\n\n## 当前的不足\n\n\n## 使用的方法\n\n### 模型再参数化\n模型再参数化：融合多个计算模块于一体，可是为组装技术\n\n$$分类\\left\\{\\begin{align*}\n  & 模块间组合\\\\\n  & 模型间组合\n\\end{align*}\\right.$$\n1. 模型间组合方法\n\t1. 在不同训练集中训练多个相同模型，然后再平均模型的参数\n\t2. 在不同的迭代次数间进行模型参数均值化\n2. 模块间组合方法\n\t在训练期间将一个模块分解成多个分支模块，在推理时将多个分支模块整合成一个完整模块\n\t\n### 模型缩放\n模型放缩可以增大和缩小模型使它适合不同计算能力的设备，满足不同的推理速度。\n\n$$放缩因子\\left\\{\\begin{align*}\n  & 分辨率resolution（输出图像尺度）\\\\\n  & 深度depth（隐藏层层数）\\\\\n  & 宽度width（通道数） \\\\\n  & 阶段stage（特征金字塔层数）\n\\end{align*}\\right.$$\n\n放缩方法：网络架构搜索Network architecture search(NAS)，折中了网络参数大小、计算时间、推理速度和精确性\n\n放缩因子的影响：\n1. 对基于非连接的网络架构，在进行模型放缩时由于每个隐藏层的入度和出度不被改变因此可以独立分析每个放缩因子对模型参数数量和计算速度的影响\n2. 对基于连接的网络架构，在进行模型隐藏层深度放大或缩小时紧跟在计算模块后的转移/转化模块的入度会减小或增大，不能独立分析单个尺度因子的影响必须一起分析\n3. 文章提出compound scaling method合成尺度方法既可保持原有模型的的性质又可保持最优结构\n\n### 架构\n选取架构主要考虑1、模型参数数量，2、计算量，3、计算密度\n采用Extended-ELAN(E-ELAN)扩展高效层聚合网络架构，该架构使用扩展基数层、清洗基数层、合并基数层增强网络学习能力\n\n基于早期版本的YOLO框架和YOLOR框架作为基本框架\n\n### 可训练赠品袋trainable bag-of-freebies\n- 计划再参数化卷积\n\t1. 如何将再参数化卷积和不同的网络结合？\n\t2. 提出planned re-parameterized convolution\n\t3. 提出无identity connection的RePConv构造planned re-parameterized convolution\n\t4. 用RepConvN网络层替换3堆叠ELAN架构中不同位置处的3x3卷积层\n\t\n- 以粗为辅以精为主的损失值\n\t1. 深度监督是在网络的中间层添加额外的辅助头，将带有损失值信息的浅层网络权重作为引导方式\n\t2. 将负责最后输出的头称为主头，将用于协助训练的头称为辅头\n\t3. 采用软标签即使用网络预测输出的性质和分布和考虑实际标签，使用一些计算和优化方式生成可靠的标签\n\t4. 如何分配软标签到主头和辅头？\n\t\t1. 分别计算主头和辅头预测结果，使用各自的分配器结合实际结果制作各自标签，再通过各自标签和头计算损失\n\t\t2. 文章提出经分配器用主头和实际结果制作由粗到精的等级标签，再将这些等级标签用在主头和辅头上计算损失值\n\t\n- 批归一化放到conv-bn-activation 拓扑结构中\n\n- YOLOR的隐性知识以串行和并行方式结合到卷积特征图中\n\n- EMA模型\n\n# AN IMAGE IS WORTH 16x16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\n作者：Alexey Dosovitskiy, 2021\n\n## 解决的问题\n根据Transformer计算的效率和可扩展性以及借鉴Transformer在自然语言方面取得的成功将其应用于图像上\n\n1. 证明在大样本上14M-300M图，Transformer胜过CNN\n2. 可以处理中等分辨率图像\n3. 在更大规模数据集而非ImageNet数据集，探索图像识别\n\n\n\n前人工作优点和不足：\n1. 把CNN框架和自注意力结合(Wang,2018; Carion, 2020)\n2. 用自注意力替换整个卷积网络(Ramachandran, 2019; Wang, 2020a)\n3. ResNet架构在大尺度图片识别上是效果好的(Mahajan, 2018; Xie, 2020; Kolesnikov, 2020)\n4. transformer用于机器翻译(Vaswani, 2017)\n5. 将transformer用到图像处理环境中\n\t- 只将自注意力应用于代查询像素的局部临域中，并非全局应用(Parmar,2018)\n\t- 局部多头点积自注意力块完全替换卷积(Hu,2019; Ramachandran, 2019; Zhao, 2020)\n\t- 稀疏Transformer在全局自注意力中使用放缩近似以适应图片(Child, 2019)\n\t- **(Cordonnier, 2020)提出的模型也是ViT但是没有证明大规模预训练模型可以甚至超过CNN模型，使用的2x2块太小只能适应小分辨率图像**\n6. (Sun, 2017)研究CNN性能如何随数据集大小变化\n7. (Kolesnikov,2020; Djolonga,2020)从大规模的数据集上探索CNN的迁移学习\n\n## 当前的不足\n1. Transformer和CNN相比缺少偏移量无法实现平移等变映射和无法进行局部化，因此在小样本中泛化能力弱\n2. **应用ViT到其它计算机视觉任务，例如目标检测和分割**\n3. 持续开发自监督与训练方法\n\n## 使用的方法\n- 分割图片成若干块，给这些块提供顺序线性嵌入体，并将嵌入体作为Transformer的输入\n- 选用原始Transformer(Vaswani, 2017)\n- 框架\n\t![image-20221125100901770](./image-20221125100901770.png)\n\t标准Transformer接收1维符号嵌入序列，将图像$x\\in\\mathbb{R}^{H\\times W\\times C}$分割成有序排列小块$x_p\\in\\mathbb{R}^{N\\times(P^2\\cdot C)}$，输入$z_0=[x_{class}; x_p^1E; x_p^2E; \\cdots; x_p^NE]+E_{pos}$, $x_{class}\\in\\mathbb{R}^{1\\times D}$, $E\\in \\mathbb{R}^{(P^2\\cdot C)\\times D}$, $E_{pos}\\in\\mathbb{R}^{(N+1)\\times D}$\n\t\n# Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions\n作者：Wenhai Wang, 2021\n\n## 解决的问题\n1. 文章(PVT)解决将transform移植到密集预测任务的问题\n2. 产生高输出分辨率，利用收缩的金字塔减少计算量\n3. PVT继承了CNN和Transformer的优点，成为应对各种视觉任务的无需卷积的统一骨干架构，可以直接替换CNN骨干架构\n4. 提高下游任务的性能，包括：目标检测，实例分割，语意分割\n5. 克服传统transformer问题方式\n\t1. 采用获取精细图像块（每个图像块为4x4像素）作为输入，以学习高分辨率的表示\n\t2. 采用逐渐缩小的金字塔形式减小transformer在深层网络中的序列长度，以减小计算量\n\t3. 采用空间减小注意力层spatial-reduction attention(SRA), 这近一步在学习高分辨率特征时减小资源损耗\n6. 具有的优点\n\t1. CNN的局部接收视野随网络深度的增加而增加，PVT产生的是全局接收视野，这有利于检测和分割任务\n\t2. 借助金字塔结构的优势，易于将PVT插入到许多代表密集预测的管道中，例如RetinaNet和Mask R-CNN\n\t3. 通过结合PVT和其它特殊任务的Transformer解码器可以构建无卷积的管道，例如PVT+DETR用作目标检测\n\n\n前人的工作及优缺点\n1. Vision Transformer(ViT)被用作分类任务\n\n2. Vision Transformer(ViT)是一种柱状机构具有粗糙的输入图像块，不是很适合像素级别的致密预测任务如目标检测和分割任务，原因有：1、输出的特征图是单一尺度，分辨率低；2、高计算和内存占用成本, \n\n3. CNN在视觉上取得巨大成功(Karen,2015; Kaiming,2017; Saining,2017)\n\n4. 将视觉任务建模成具有可学习查询功能的字典查找问题，使用Transformer解码器作为特殊任务的头应用于卷积框架的顶层(Nicolas,2020; Christian,2017; Enze,2021)\n\n5. 网络架构\n\n  6. (Yann, 1998)首次引入CNN分辨手写数字，在整个图像空间共享卷积核参数实现平移等变性\n\n  7. (Alex, 2012; Karen, 2015)在大尺度图片分类数据集中使用堆叠的卷积块\n\n  8. GoogLeNet(Christain,2015)包含了多核路径\n\n  9. 多路径卷积模块的效率在Inception系列网络(Christian,2016;)、ResNeXt(Saining, 2017)、 DPN(Yunpeng, 2017)、MixNet(Wenhai, 2018)、SKNet(Xiang,2019)网络中被验证\n\n  10. (Kaiming,2016)在ResNet网络中引入跳跃式连接卷积模块，这有助于训练更深的网络\n\n  11. (Gao,2017)在DenseNet中引入密集连接拓扑结构，把每个卷积模块同它前面的所有卷积模块相连\n\n12. 密集预测任务\n   1. 目的：在特征图上进行像素级别的分类和回归\n   2. $$分类\\left\\{\\begin{align*}\n       & 目标检测\\left\\{\\begin{array}{l}\n                  单阶段\\left\\{\\begin{array}{l}\n                                 SSD(Wei,2016) \\\\\n                                 RetinaNet(Tsung-Yi\\; Lin,2017) \\\\\n                                 FCOS(Zhi\\; Tian,2019) \\\\\n                                 GFL(Xiang,2020) \\\\\n                                 PolarMask(Enze\\;Xie,2021) \\\\\n                                 OneNet(Peize\\; Sun,2020) \\\\\n                              \\end{array}\\right. \\\\\n                  多阶段\\left\\{\\begin{array}{l}\n                                 Faster R-CNN(Shaoqing\\; Ren,2015) \\\\\n                                 Mask R-CNN(Kaiming\\; He,2017) \\\\\n                                 Cascade R-CNN(Zhaowei\\; Cai,2018) \\\\\n                                 Sparse R-CNN(Peize\\; Sun, 2021) \\\\\n                              \\end{array}\\right. \\\\\n                  结合CNN和Transformer\\; decoder\\left\\{\\begin{array}{l}\n                                  DETR(Nicolas\\; Carion, 2020)\\\\\n                                  deformable DETR(Xizhou\\; Zhu,2021)\\\\\n                               \\end{array}\\right.\n               \\end{array}\\right.\\\\\n       & 语意分割 \\left\\{\\begin{array}{l}\n                      FCN(Jonathan\\; Long,2015)\\\\\n                      deconvolution\\; operation(Hyeonwoo\\; Noh,2015)\\\\\n                      U-Net(Olaf\\; Ronneberger,2015)\\\\\n                      添加金字塔池化(HengShuang\\; Zhao,2017)\\\\\n                      添加FPN头(Alexander\\; Kirillov,2019)\\\\\n                      DeepLab(Liang-Chieh\\; Chen,2017)\\\\\n                   \\end{array}\\right.\\\\\n       & 实例分割\n       \\end{align*}\\right.$$\n\n  13. 自注意力和变换器\n\t \t1. 卷积滤波器权重经过训练后被固定无法动态适应不同的输入，(Xu Jia,2016)使用动态滤波器，(Ashish Vaswani,2017)使用自注意力\n\t\t2. 非局部模块被(Xiaolong Wang,2018)引入解决时间和空间在大尺度上的依赖性，但是代价是计算成本和内存占用成本\n\t\t3. (Zilong Huang,2019)引入十字交叉路径Criss-cross减小注意力机制的复杂度\n\t\t4. \n\n\n\n## 当前的不足\n\n1. 有许多特殊模块和运算方法是专门为CNN设计的因此没有在PVT中引入，例如：Squeeze-and-excitation network(SE)模块, Selective kernel network(SK)模块, 膨胀卷积模块, 模型精简模块，Network architecture search(NAS)模块\n2. 基于Transformer 的模型在视觉应用上起步晚可以应用于OCR, 3D和医疗图像分析\n\n## 使用的方法\n### 框架\n整体框架分成4个阶段，每个阶段都产生不同尺度的特征图，每个阶段都具有相似的结构，包括：1、分块嵌入层，2、若干Transformer编码层\n- 结构（以第一阶段为例）\n\t1. 第一阶段输入图像HxWx3, 并分割成$\\frac{HW}{4^2}$多个图块，每块大小4x4x3\n\t\n\t2. 将分割块展平进行线性投影变换得到嵌入块形状为$\\frac{HW}{4^2}\\times C_1$\n\t\n\t3. 将嵌入块和位置嵌入一起传入具有$L_1$层的Transformer编码器中\n\t\n\t4. 输出特征层$F_1$形状是$\\frac{H}{4}\\times\\frac{W}{4}\\times C_1$\n\t\n\t5. 以此类推，以上一阶段输出作为下一阶段的输入，选取的块的大小相对于原始图像分别是8x8，16x16，32x32像素，即第i阶段的块大小为$P_i$，得到的特征图为$F_i$:$\\{F_2$,$F_3$,$F_4\\}$,特征图尺寸为$\\frac{H}{8}\\times\\frac{W}{8}\\times C_2$,$\\frac{H}{16}\\times\\frac{W}{16}\\times C_3$,$\\frac{H}{32}\\times\\frac{W}{32}\\times C_4$,\n\t\n\t   ![image-20221127151329769](./image-20221127151329769.png)\n\t\n- transformer的特征金字塔\n\t- CNN的特征金字塔是使用不同的卷积跨步来实现，PVT是使用逐步缩小策略实现\n\t\n- transformer编码器\n\t1. Transformer编码器在第i阶段有$L_i$个编码层，每一个编码层又包含：1、注意力层，2、feed-forward层\n\t2. 使用spatial-reduction注意力层(SRA)替换传统多头注意力层(MHA)，为了处理高分辨率特征图(4跨步特征图)\n\t    ![image-20221127153059574](./image-20221127153059574.png)\n\t\tSRA特点：减小Key和Value输入的尺寸：\n\t\t$$\\begin{align*}\n\t\t\t\tSRA(Q,K,V)&=Concat(head_0,\\cdots,head_{N_i})W^O\\\\\n\t\t\t\thead_j&=Attention(QW_j^Q,SR(K)W_j^K,SR(V)W_j^V)\\\\\n\t\t\t\tSR(x)&=Norm(Reshape(x,R_i)W^S)\\\\\n\t\t\t\tAttention(\\vec{q},\\vec{k},\\vec{v})&=Softmax(\\frac{\\vec{q}\\vec{k}^T}{\\sqrt{\\vec{d}_{head}}})\\vec{v}\\\\\n\t\t\\end{align*}$$\n\t\n\t\t符号说明：$Concat(\\cdot)$链接操作；$W_j^Q\\in\\mathbb{R}^{C_i\\times d_{head}}$,$W_j^K\\in\\mathbb{R}^{C_i\\times d_{head}}$,$W_j^V\\in\\mathbb{R}^{C_i\\times d_{head}}$,$W^O\\in\\mathbb{R}^{C_i\\times C_i}$,$W^S\\in\\mathbb{R}^{(R_i^2C_i)\\times C_i}$都是线性投影矩阵；$N_i$注意力的头数；$d_{head}=\\frac{C_i}{N_i}$每个头的大小；$SR(\\cdot)$减小空间尺度操作；$x\\in\\mathbb{R}^{(H_iW_i)\\times C_i}$表示输入序列；$R_i$表示缩减比例；$Reshape(x,R_i)=\\frac{H_iW_i}{R_i^2}\\times(R_i^2C_i)$修改张量形状;$Norm(\\cdot)$层归一化；$Attention(\\cdot,\\cdot,\\cdot)$注意力得分\n\n\n# Deformable DETR: Deformable Transformers For End-to-End Object Detection\n作者：Xinzhou Zhu,2020\n\n## 解决的问题\n1. 改善空间分辨率问题和收敛慢问题\n2. Deformable DETR在检测小目标上优于DETR\n3. 结合了deformable convolution的稀疏空间采样和transformer的相关性建模能力\n\n前人的工作以及优缺点：\n1. DETR(Nicolas Carion,2020)用于剔除目标检测中辅助成分的需求（例如，非极大值抑制），缺点是收敛慢、有限的特征空间分辨率，该模型结合了CNN模型和Transformer编解码模型，小目标效果差，优点就是具有元素间的相关性\n2. 目标检测使用了一些辅助成分(Li Liu,2020),例如锚点生成，基于规则的训练目标分配，非极大抑制\n3. deformable convolution(Jifeng Dai,2017)用于稀疏空间定位，因此高效、收敛快，缺点就是缺少元素间的相关性\n4. (Ashish Vaswani,2017)在transformer中引入自注意力和交叉注意力机制,缺点是时间成本和内存占用高\n5. 解决时间和内存的方式有三种\n\t1. 对关键点使用预定义(冻结参数)的稀疏注意力模式\n\t\t- 将注意力模式限制在固定局部窗口中使用(Peter J Liu,2018a;Niki Parmar,2018;Rewon Child,2019;Zilong Huang;2019),优点是减少复杂度，缺点是丢失全局信息\n\t\t- 以固定间隔方式设置关键点(Jonathan Ho,2019)，优点：增加接受视野\n\t\t- 允许少许特殊符号可以访问所有关键点(Iz Beltagy,2020),增加全局信息\n\t2. 学习依赖数据的稀疏注意力\n\t\t- 基于注意力机制的局部敏感哈希映射LSH(Nikita Kitaev,2020),将查询元素和关键字元素映射到不同的区域\n\t\t- (Aurko Roy,2020)用k-means聚类找到最相关的关键字元素\n\t\t- (Yi Tay,2020a)对逐块的稀疏注意力学习块交换\n\t3. 研究自注意力机制的低级别性质\n\t\t- 通过线性投影减少关键点数量(Sinong Wang,2020)\n\t\t- 通过核函数近似计算自注意力(Krzysztof Choromanski,2020)\n6. 多尺度特征表示\n\t- FPN(Tsung-Yi Lin,2017a),自上而下生成多尺度特征图\n\t- PANet(Shu Liu,2018b)，自下而上生成多尺度特征图\n\t- 从全局自注意力中提取所有尺度上的特征(Tao Kong,2018)\n\t- U-shape模块融合多尺度特征(Qijie Zhao,2019)\n\t- NAS-FPN(Golnaz Ghiasi,2019)、Auto-FPN(Hang Xu,2019)通过神经架构搜索自动进行交叉尺度连接\n\t- BiFPN(Mingxing Tan,2020)\n\n\n\n\n## 当前的不足\n1. 简单且高效的可迭代的边界框调优机制进一步改善性能\n2. 可将Deformable DETR应用到2阶段目标识别中，先生成推荐区域，再将推荐区域作为目标query送入解码器\n\n\n\n## 使用的方法\n1. 应用在若干的采样位置点处，这些点作为特征图中关键特征点\n2. 使用图像尺度放缩而不是特征金字塔应用于deformable注意力模型\n\n### 经典多头注意力结构\nquery元素代表了要输出的句子的目标单词，key元素代表输入句子中的单词，多头注意力模块根据测量的query-key对的相似性权重因子汇聚这些key。用$q\\in\\Omega_q$索引具有表达特征$z_q\\in\\mathbb{R}^C的$query元素；用$k\\in\\Omega_k$索引具有表达特征$x_k\\in\\mathbb{R}^C的$key元素,$C$是特征维度，$\\Omega_q$和$\\Omega_k$给出了query和key的元素总数;多头注意力特征计算：\n\n$$MultiHeadAttn(z_q,x)=\\sum\\limits_{m=1}^MW_m[\\sum\\limits_{k\\in\\Omega_k}A_{mqk}\\cdot W_m^\\prime x_k]$$\n\n这里$m$索引各个注意力头总共有M个注意力头，$W_m^\\prime\\in\\mathbb{C_v\\times C}$和$W_m\\in\\mathbb{C\\times C_v}$是第m头待学习的矩阵，$C_v=C/M$,注意力权重$A_{mqk}\\propto \\exp\\{\\frac{z_q^TU_m^TV_mx_k}{\\sqrt{C_v}}\\}$满足归一化$\\sum\\limits_{k\\in\\Omega_k}A_{mqk}=1$, 这里$U_m\\in\\mathbb{R}^{C_v\\times C}$和$V_m\\in\\mathbb{R}^{C_v\\times C}$同样是待学习的矩阵。为了消除不同空间位置的奇异性，表达的查询和关键字特征$z_q$和$x_k$需要和位置嵌入体做结合。\n\n### DETR Transformer编解码架构\n采用Hungarian(匈牙利)损失函数借助二分(双边)匹配实现对每一个真实边框都有唯一预测值。用$y$表示ground truth集合，$\\hat{y}=\\{\\hat{y}_i\\}_{i=1}^N$表示有N个预测值的集合，当$N$远大于图像中物体个数时，可以认为$y$集合也是由N个真实结果构成没有目标的结果被符号$\\phi$填充。搜索N个元素$\\sigma\\in\\mathfrak{G}_N$的一个置换$\\hat{\\sigma}=\\mathop{\\arg\\min}\\limits_{\\sigma\\in\\mathfrak{G}_N}\\sum\\limits_i^N\\mathcal{L}_{match}(y_i,\\hat{y}_{\\sigma(i)})$, （1）$\\mathcal{L}_{match}(y_i,\\hat{y}_{\\sigma(i)})$是真值$y_i$和具有索引$\\sigma(i)$的预测值之间的逐对匹配代价函数；（2）第i个真值元素可以看成$y_i=(c_i,b_i)$,$c_i$是目标类别标签，符号$\\phi$用N/A表示，$b_i=\\{b_x,b_y,b_w,b_h\\}\\in[0,1]^4$是向量记录了中心坐标和相对图像大小的高度和宽度；（3）对索引$\\sigma(i)$的预测值，定义类$c_i$的概率为$\\hat{p}_{\\sigma(i)}(c_i)$,预测边框为$\\hat{b}_{\\sigma(i)}$，定义：\n\n$$\\mathcal{L}_{match}(y_i,\\hat{y}_{\\sigma(i)})=-1_{\\{c_i\\neq\\phi\\}}\\hat{p}_{\\sigma(i)}(c_i)+1_{\\{c_i\\neq\\phi\\}}\\mathcal{L}_{box}(b_i,\\hat{b}_{\\sigma(i)})$$\n$$\\mathcal{L}_{box}(b_i,\\hat{b}_{\\sigma(i)})=\\lambda_{iou}\\mathcal{L}_{iou}(b_i,\\hat{b}_{\\sigma(i)})+\\lambda_{L1}||b_i-\\hat{b}_{\\sigma(i)}||_1$$\n\n这是真实框集合中的每一框和预测框集合中的每一个框匹配，损失值最小的预测框为该真实框的最佳匹配框，得到唯一匹配。匈牙利损失函数定义如下：\n\n$$\\mathcal{L}_{Hungarian}(y,\\hat{y})=\\sum\\limits_{i=1}^N\\left[-1_{\\{c_i\\neq\\phi\\}}\\hat{p}_{\\hat{\\sigma}(i)}(c_i)+1_{\\{c_i\\neq\\phi\\}}\\mathcal{L}_{box}(b_i,\\hat{b}_{\\hat{\\sigma}(i)})\\right]$$\n\nDETR主要使用Transformer编码解码架构将特征图像映射成一组待查目标对象的特征，feed-forward网络FFN用做回归分支预测边框坐标，线性投影用作分类分支预测目标类别。编码部分输入ResNet的生成的特征图，取query和key元素，它们都是特征图的每一个像素；解码部分输入包含来自编码部分的特征图和N个目标查询，有两个注意力模型：1、cross-attention,2、self-attention。在cross-attention中查询元素是从特征图中提取特征的N个目标查询，关键字元素来自于编码输出的特征图；在self-attention中，查询元素要获取他们的关系因此查询元素和关键字元素都来自于N个目标查询\n\n### Deformable transformer\n1. 单尺度Deformable attention\n\t通过对每个查询元素只分配少许固定数量的关键字元素，这些关键字元素取自一个参考点附近的点，无需对所有关键字进行匹配，所以无视特征图尺度，已知特征图$x\\in\\mathbb{R}^{C\\times H\\times W}$,$q$是查询元素特征$z_q$索引以及分配的一个2维参考点$p_q$索引\n\t$$DeformAttn(z_q,p_q,x)=\\sum\\limits_{m=1}^MW_m\\left[\\sum\\limits_{k=1}^KA_{mqk}\\cdot W_m^\\prime x(p_q+\\Delta p_{mqk})\\right]$$\n\t这里m是注意力头的索引，k是采样到的关键字元素索引，K是采样关键字总数，显然$K\\ll HW$。$\\Delta p_{mqk}$和$A_{mqk}$表示对第m注意力头、第k个关键字元素采样点的采样邻域半径和注意力权重，注：标量注意力权重$A_{mqk}\\in [0,1]$要归一化处理, $\\Delta p_{mqk}\\in \\mathbb{R}^2$没有约束范围，当$p_q+\\Delta p_{mqk}$是小数时双线性插值可以使用，$\\Delta p_{mqk}$和$A_{mqk}$是通过对查询元素特征$z_q$线性投影计算得到\n\n2. 多尺度Deformable attention\n\t设$\\{x^l\\}_{l=1}^L$是用于输入的多尺度特征图，$x^l\\in\\mathbb{R}^{C\\times H_l\\times W_l}$, $\\hat{p}_q\\in[0,1]^2$是每个查询元素q对应参考点的归一化二维坐标，坐标的归一化操作是对每一个尺度特征图进行, 模型公式为\n\t$$MSDeformAttn(z_q,\\hat{p}_q,\\{x^l\\}_{l=1}^L)=\\sum\\limits_{m=1}^MW_m\\left[\\sum\\limits_{l=1}^L\\sum\\limits_{k=1}^KA_{mlqk}\\cdot W_m^\\prime x^l(\\phi_l(\\hat{p}_q)+\\Delta p_{mlqk})\\right]$$\n\tm是注意力头索引，l是输入特征图的尺度等级索引，k是采样点的索引，$\\Delta p_{mlqk}$和$A_{mlqk}$表示在l层尺度上的特征图、对第m注意力头、第k个关键字元素采样点的采样邻域半径和注意力权重，$\\phi_l(\\hat{p}_q)$将归一化的坐标缩放回尺度为l的特征图中的坐标\n\n3. 编码器\n\t编码器的输入输出都是具有相同分辨率的多尺度特征图，编码器的多尺度特征图$\\{x^l\\}_{l=1}^{L-1},L=4$取自ResNet的输出特征图$C_3$到$C5$分辨率分别为$H/2^3$到$H/2^5$,而最低分辨率的特征图$x^4$取自$C_5$特征图进行3x3步长2的卷积后得到的特征图$C_6$,所有的输入特征图的通道数都是256：\n\t![image-20221130151059959](./image-20221130151059959.png)\n\tquery和key元素都是多尺度特征图的像素，每个query像素的参考点就是其自身，为了识别每个query像素在哪个尺度图上，除了添加位置嵌入体外，需要添加尺度嵌入体$e_l$到特征表达中，区别是位置嵌入体是固定编码，尺度嵌入体$\\{e_l\\}_{l=1}^L$需要连同网络一起训练获取。\n\n4. 解码器\n\t包含cross-attention和self-attention,query元素在两类注意力机制中都是目标query，目标query在cross-attention中取自特征图，而key元素是编码器的输出特征图；在self-attention中，key元素是目标query。每个目标query参考点的二维归一化坐标需要经过线性投影和激活函数从目标query的嵌入体中给出\n\t![image-20221130160631440](./image-20221130160631440.png)\n\t模型提取的图像特征都是在参考点周围的点，所以我们预测的边框都是相对于参考点的偏移量，参考点初始值都是边框的中心点\n\n\n# End-to-End Object Detection with Transformers\n作者：Nicolas Carion,2020\n\n## 解决的问题\n1. 提出目标检测新方法DEtection TRansformer(DETR)：将目标检测看作单向集合预测问题\n2. 不再使用辅助设计成分，例如非极大值抑制抑制，锚点，是完全端到端的理念，流水化检测过程直接预测具有相对输入图像的绝对边框的检测集合\n3. 结合了二分匹配损失和具有并行解码(非自动回归)的transformer，损失函数对预测目标交换具有不变性，因此可以并行执行\n4. 对大目标具有更好的性能，可能是由于transformer的非局部计算\n\n\n前人的工作及其优缺点：\n1. 以间接方式解决预测边框和种类任务，通过对大量推荐边框(Ren,S.,2015; Cai,Z.,2019)、锚点(Lin,T.Y.,2017)和窗口中心(Zhou,X.,2019;Tian,Z.,2019)定义回归和分类问题,通过后处理步骤削减近邻重复预测\n2. (Stewart,R.J.,2015;Hosang,J.H.,2017;Bodla,N.,2017,Rezatofighi,S.H.,2018)要么添加了其他形式的先验内容，要么在有挑战的基准样本上没有证明有强大竞争力\n3. (Stewart,R.J.,2015;Romera-Paraedes,B.,2015;Park,E.,2015)关注具有RNN自动回归解码\n4. 集合预测\n\t基础的集合预测任务是多标签分类(Rezatofighi,S.H.,2017;Pineda,L.,2019)而目标检测要考虑元素间暗含的结构即识别近邻边框，因此不能用one-vs-rest方式处理问题。使用后处理方式例如非极大抑制处理近邻重叠问题；利用全局的对所有预测结果建立相互影响的推理方案规避这种冗余问题。集合预测可以使用致密的全连接网络(Erhan,D.,2014)、自动回归序列模型(recurrent neural network:Vinyals,O.,2016)。损失函数可以基于Hungarian算法(Kuhn,H.W.,1955)保证交换不变性和每个目标元素都有唯一匹配\n5. transformer和并行解码\n\tTransformer是一个基于注意力机制的搭建模块用于机器翻译(Vaswani,A.,2017)；注意力机制是一个神经网络层，可以从整个输入序列中汇集信息(Bahdanau,D.,2015)；Transformer用在自动回归模型，根据之前的句子到句子模型(Sutskever,I.,2014)生成一个接一个的输出符号,这种方法缺点是推理成本(正比于输出长度)和很难批处理，所以需要开发并行序列生成，可以应用在语音(Oord,A.v.d.,2017)、机器翻译(Gu,J.,2018;Ghazvininejad,M.,2019)、单词表达学习(Devlin,J.,2019)、语音识别(Chan,W.,2020)。将transformer和并行解码结合权衡了计算成本和集合预测需要的全局计算能力\n6. 目标检测\n\t目标检测根据一些初始猜测做预测，两阶段目标检测是根据推荐框(Ren,S.,2015;Cai,Z.,2019)做预测，单阶段目标检测是根据锚点(Lin,T.Y.,2017)或可能的目标中心网格(Zhou,X.,2019;Tian,Z.,2019)做预测,这类目标检测严重依赖初始猜测设置方式(Zhang,S.2019)。\n\t- 基于集合损失\n\t\t早期的深度学习模型只用卷积和全连接层建模不同预测的关系并用辅助后处理改善性能；最近有些检测器使用真值和预测值之间的非唯一分配规则并结合NMS(Ren,S.,2015;Lin,T.Y.,2017;Zhou,X.,2019);可学习NMS方法(Hosang,J.H.,2017;Bodla,N.,2017)和关系网络(Hu,H.,2018)利用注意力显式建模不同预测间的关系，直接使用集合损失不在需要后处理步骤，但是也需要上下文特征例如推荐框坐标。\n\t- 循环探测\n\t\t端到端方法(Stewart,R.J.,2015;Romera-Paredes,B.,2015;Park,E.,2015;Ren,M.,2017)使用二分匹配损失以及基于CNN的编解码框架直接产生一组边界框，这些方法只在小样本数据中评估，基于的是自动回归模型没有利用具有并行解码的transformer。\n\n\n## 当前的问题\n1. 对小目标性能不佳，可以用FPN进行改善\n2. 长时间的训练\n\n\n## 使用的方法\n基于一组全局损失，通过二分匹配给出唯一预测，使用transformer编解码架构，需要一组固定数量的学习好的目标query, DETR会推理目标和全局图像的关系然后输出最终的预测集合\n![image-20221201110746341](./image-20221201110746341.png)\n\n![image-20221202090850052](./image-20221202090850052.png)\n\n### 目标检测集合预测损失\n\nDETR推理N个固定大小的预测结果集合，N明显多于图像中典型的目标数量，给出预测目标（类别，位置，大小）同真值比较后的好坏。用$y$表示ground truth目标集合，$\\hat{y}=\\{\\hat{y}_i\\}_{i=1}^N$表示有N个预测值的集合，当$N$远大于图像中物体个数时，可以认为$y$集合也是由N个真实结果构成没有目标的结果被符号$\\phi$填充。两个集合之间进行二分匹配，搜索N个元素$\\sigma\\in\\mathfrak{G}_N$的一个置换以得到最小代价$\\hat{\\sigma}=\\mathop{\\arg\\min}\\limits_{\\sigma\\in\\mathfrak{G}_N}\\sum\\limits_i^N\\mathcal{L}_{match}(y_i,\\hat{y}_{\\sigma(i)})$, （1）$\\mathcal{L}_{match}(y_i,\\hat{y}_{\\sigma(i)})$是真值$y_i$和具有索引$\\sigma(i)$的预测值之间的逐对匹配代价函数；（2）第i个真值元素可以看成$y_i=(c_i,b_i)$,$c_i$是目标类别标签，符号$\\phi$用N/A表示，$b_i=\\{b_x,b_y,b_w,b_h\\}\\in[0,1]^4$是向量记录了中心坐标和相对图像大小的高度和宽度；（3）对索引$\\sigma(i)$的预测值，定义类$c_i$的概率为$\\hat{p}_{\\sigma(i)}(c_i)$,预测边框为$\\hat{b}_{\\sigma(i)}$，定义：\n\n$$\\mathcal{L}_{match}(y_i,\\hat{y}_{\\sigma(i)})=-1_{\\{c_i\\neq\\phi\\}}\\hat{p}_{\\sigma(i)}(c_i)+1_{\\{c_i\\neq\\phi\\}}\\mathcal{L}_{box}(b_i,\\hat{b}_{\\sigma(i)})$$\n$$\\mathcal{L}_{box}(b_i,\\hat{b}_{\\sigma(i)})=\\lambda_{iou}\\mathcal{L}_{iou}(b_i,\\hat{b}_{\\sigma(i)})+\\lambda_{L1}||b_i-\\hat{b}_{\\sigma(i)}||_1$$\n\n这是真实框集合中的每一框和预测框集合中的每一个框匹配，损失值最小的预测框为该真实框的最佳匹配框，得到唯一匹配。作用类似于用于匹配的启发式分配规则（匹配推荐框、锚点）区别只是一一匹配。根据二分匹配损失计算匈牙利损失函数(类别的负对数似然函数损失和边框损失线性组合)，定义如下：\n$$\\mathcal{L}_{Hungarian}(y,\\hat{y})=\\sum\\limits_{i=1}^N\\left[-\\log\\,\\hat{p}_{\\hat{\\sigma}(i)}(c_i)+1_{\\{c_i\\neq\\phi\\}}\\mathcal{L}_{box}(b_i,\\hat{b}_{\\hat{\\sigma}(i)})\\right]$$\n$\\hat{\\sigma}$是二分分配损失计算得到的最优预测值索引，注：$c_i=\\phi$的样本通常比目标类别多很多，属于不平衡样本集，所以对于无目标类别将对数几率的权重减少到$1/10$。别的目标检测器对边界框的预测是基于和初始猜测边框的偏差，这里是直接生成边框预测，$\\ell_1$损失的缺点就是有损失值的相对尺度问题，即大目标的大边框和小目标的小边框都可能产生相似的$\\ell_1$损失，因此无法通过该损失区分大小尺寸，需要结合具有尺度不变性的交并比损失，注两个损失值还需要在批中样本上做归一化处理。\n\n### DETR框架\n\n架构三部分组成：1、CNN骨架（提取出致密特征表达），2、transformer编解码，3、feed forward network,FFN(生成最终的预测)。\n\n#### CNN\n初始图片$x_{img}\\in\\mathbb{R}^{3\\times H_0\\times W_0}$, CNN输出低分辨率的激活图$f\\in\\mathbb{R}^{C\\times H\\times W}$,$C=2048$,$H,W=\\frac{H_0}{32},\\frac{W_0}{32}$\n\n#### transformer编码器\n1. 用1x1卷积核减少高层激活图$f$的通道维度从$C$到$d$，新的特征图为$z_0\\in\\mathbb{R}^{d\\times H\\times W}$\n2. 编码器需要输入特征序列，将$z_0$的空间维度塌缩到一个维度$z_0\\in\\mathbb{R}^{d\\times HW}$\n3. 编码器包含：1、多头注意力模块，2、feed forward网络层，编码器具有输入序列的交换不变性，因此需要向序列中添加固定位置嵌入体\n\n#### transformer解码器\n- 使用标准的多头自注意力和编解码注意力机制的transformer架构，但是区别是解码器每层并行解码N个目标，原来的模型是使用自动回归模型一次预测输出序列中的一个元素。\n- 由于解码器也是对输入序列具有交换不变性，因此也需要让N个输入嵌入是不同的以生成不同结果，这N个输入嵌入体就是位置编码也称作N个目标查询，将他们添加到每一个注意力层的输入部分。\n- N个目标查询被解码器转换成输出嵌入体\n\n\n#### FFN网络层\n- 具有ReLU激活函数的3层感知网络、线性投影层\n- FFN将N个输出嵌入体独立分解成N个边框坐标和类别标签，即最终预测结果\n- 边框坐标是根据输入图像做归一化\n- 类别标签使用softmax函数给出\n\n####补充多头注意力层\n\n**多头注意力**\n有M个注意力头，每个头的维度是d，多头注意力是一个函数函数的输入输出如下，注：$d^\\prime=\\frac{d}{M}$，在花括号中列出了矩阵/张量的大小\n\n$$\\operatorname{mh-attn}: \\underbrace{X_q}\\limits_{d\\times N_q},\\underbrace{X_{kv}}\\limits_{d\\times N_{kv}},\\underbrace{T}\\limits_{M\\times 3\\times d^\\prime\\times d},\\underbrace{L}\\limits_{d\\times d}\\;\\mapsto\\;\\underbrace{\\tilde{X}_q}\\limits_{d\\times N_q}$$\n\n展开为每个单头计算结果在特征阶次上的连接\n$$\\operatorname{mh-attn}(X,X,T,L)=L\\left[attn(X_q,X_{kv},T_1);\\cdots;attn(X_q,X_{kv},T_M)\\right]$$\n\n$X_q$是长度为$N_q$的查询序列，$X_{kv}$是长度为$N_{kv}$的键-值序列，$T$是权重张量用于计算所谓的查询、键值、得分嵌入体，$L$是投影矩阵。多头自注意力是$X_q=X_{kv}$的特殊情况\n\n$$\\operatorname{mh-s-attn}(X,T,L)=\\operatorname{mh-attn}(X,X,T,L)$$\n\n一般在得到$\\tilde{X}_q$后还要在进行残差连接、dropout和层归一化处理\n$$\\hat{X}_q=layernorm(X_q+dropout(\\tilde{X}_q))$$\n\n**单头注意力**\n单头注意力的权重张量$T^\\prime\\in\\mathbb{R}^{3\\times d^\\prime\\times d}$，在计算单头注意力时还要引入位置编码嵌入体$P_q\\in\\mathbb{R}^{d\\times N_q}$和$P_{kv}\\in\\mathbb{R}^{d\\times N_{kv}}$。\n首先计算添加位置编码后的查询、键值、得分嵌入体，计算复杂度$\\mathcal{O}(d^\\prime N_q\\cdot d +d^\\prime N_{kv} \\cdot d)$，$Q_i,K_i:\\mathcal{O}(d^\\prime d)$\n$$\\left[Q;K;V\\right]=\\left[T_1^\\prime(X_q+P_q);T_2^\\prime(X_{kv}+P_{kv});T_3^\\prime X_{kv}\\right]$$\n$T^\\prime$就是$T_1^\\prime,T_2^\\prime,T_3^\\prime$在第一阶次上的连接\n\n再计算注意力权重$\\alpha$,通过query嵌入体和键值嵌入体点积运算和softmax运算\n因此可以得到查询序列中的每个查询嵌入体和键值序列中的每个键值嵌入体之间的权重（相似性），计算复杂度$\\alpha_{i,j}:\\mathcal{O}(d^\\prime)$：\n$$\\alpha_{i,j}=\\frac{e^{\\frac{1}{\\sqrt{d^\\prime}}Q_i^TK_j}}{Z_i}\\quad\\quad Z_i=\\sum\\limits_{j=1}^{N_{kv}}e^{\\frac{1}{\\sqrt{d^\\prime}}Q_i^TK_j}$$\n\n最后计算带注意力权重$\\alpha$的第i个嵌入体在特征维度上的加权得分分布,计算复杂度$\\mathcal{O}(N_{kv}\\cdot d^\\prime)$\n$$attn(X_{q_i},X_{kv},T^\\prime)=\\sum\\limits_{j=1}^{N_{kv}}\\alpha_{i,j}V_j$$\n在查询序列阶次上连接,计算复杂度$\\mathcal{O}(N_qN_{kv}\\cdot d^\\prime)$\n$$attn(X_q,X_{kv},T^\\prime)=[attn(X_{q_1},X_{kv},T^\\prime);\\cdots;attn(X_{q_{N_q}},X_{kv},T^\\prime)]$$\n\n![image-20221203145853415](./image-20221203145853415.png)\n","slug":"目标识别","published":1,"updated":"2022-12-03T07:39:09.241Z","_id":"claugokre0008ccfyfe7b9of7","comments":1,"layout":"post","photos":[],"link":"","content":"<p>[TOC]</p>\n<h1 id=\"基于深度学习的视觉目标检测技术综述\"><a href=\"#基于深度学习的视觉目标检测技术综述\" class=\"headerlink\" title=\"基于深度学习的视觉目标检测技术综述\"></a>基于深度学习的视觉目标检测技术综述</h1><p>作者：曹家乐，2022</p>\n<h2 id=\"发展历程\"><a href=\"#发展历程\" class=\"headerlink\" title=\"发展历程\"></a>发展历程</h2><ol>\n<li>基于手工设计特征的方法<ol>\n<li>支持向量机</li>\n<li>AdaBoost</li>\n<li>Haar特征(Viola, 2004)</li>\n<li>方向梯度直方图histograms of oriented gradients(Dalal, 2005)</li>\n</ol>\n</li>\n<li>深度学习<ol>\n<li>区域卷积神经网络region-based convolutional neural network, R-CNN (Girshick, 2014)</li>\n<li>单次检测single shot detector, SSD (Liu, 2016)</li>\n<li>yolo (Redmon, 2016)</li>\n<li>detection transformer, DETR (Carion, 2020)</li>\n</ol>\n</li>\n</ol>\n<p>深度网络模型：</p>\n<ol>\n<li>AlexNet(Krizhevsky,2012)</li>\n<li>GoogLeNet(Szegedy,2015)</li>\n<li>VGGNet(Simonyan,2015)</li>\n<li>ResNet(He,2016)</li>\n<li>DenseNet(Huang,2017)</li>\n<li>Mobilenet(Howard,2018)</li>\n<li>ShuffleNet(Zhang,2018)</li>\n<li>SENet(Hu, 2018)</li>\n<li>EfficientNet(Tan,2019)</li>\n<li>ViT(Dosovitskiy,2021)</li>\n<li>Swin(Liu,2022)</li>\n</ol>\n<p>目标检测方法</p>\n<ol>\n<li>DetectorNet(Szegedy,2014)</li>\n<li>R-CNN(Girshick,2014)</li>\n<li>OvearFeat(Sermanet,2014)</li>\n<li>SPPNet(He,2015)</li>\n<li>Fast R-CNN(Girshick,2016; Ren,2016)</li>\n<li>YOLO(Redmon,2016)</li>\n<li>SSD(Liu,2016)</li>\n<li>R-FCN(Dai,2017)</li>\n<li>FPN(Lin,2017)</li>\n<li>YOLOv2(Redmon,2017)</li>\n<li>Mask RCNN(He,2018)</li>\n<li>DCN(Dai,2018)</li>\n<li>RetinaNet(Lin,2018)</li>\n<li>Cascade RCNN(Cai,2018)</li>\n<li>YOLOv3(Redmon,2019)</li>\n<li>CornerNet(Law,2019)</li>\n<li>FCOS(Tian,2020)</li>\n<li>CenterNet(Zhou,2020)</li>\n<li>EfficientDet(Tan,2020)</li>\n<li>ATSS(Zhang,2020)</li>\n<li>MoCo(He,2020)</li>\n<li>YOLOv4(Bochkovskiy,2021)</li>\n<li>Deformable DETR(Zhu,2021)</li>\n<li>DETR(Carion,2021)</li>\n<li>YOLOv5(Jocher,2021)</li>\n<li>UP-DETR(Dai,2021)</li>\n<li>Pix2seq(Chen,2022)</li>\n</ol>\n<h2 id=\"检测设备\"><a href=\"#检测设备\" class=\"headerlink\" title=\"检测设备\"></a>检测设备</h2><ol>\n<li>单目相机</li>\n<li>双目相机 （提供三维信息）</li>\n</ol>\n<h2 id=\"基于单目相机流程及其涵盖的方法\"><a href=\"#基于单目相机流程及其涵盖的方法\" class=\"headerlink\" title=\"基于单目相机流程及其涵盖的方法\"></a>基于单目相机流程及其涵盖的方法</h2><ol>\n<li><p>数据预处理</p>\n<ul>\n<li>翻转</li>\n<li>放缩</li>\n<li>均值归一化</li>\n<li>色调变化</li>\n<li>剪切、擦除、分区(DeVries, 2017; Zhong, 2020b; Singh, 2017; Chen, 2020a)</li>\n<li>混合(Mixup: Zhang, 2018; CutMix: Yun, 2019; Fang, 2019; Mosaic: Bochkovskiy, 2020; Montage: Zhou, 2020; dynamic scale training: Chen, 2020b)</li>\n</ul>\n</li>\n<li><p>检测网络</p>\n<ul>\n<li>基础骨架<ul>\n<li>AlexNet(Krizhevsky, 2012)</li>\n<li>VGGNet(Simonyan, 2014) </li>\n<li>ResNet(He, 2016)</li>\n<li>DenseNet(Huang, 2017)</li>\n<li>Transformer(Vaswani, 2017), ViT(Dosovitskiy, 2021; Beal, 2020), Swin(Liu, 2021c), PVT(Wang, 2021c)</li>\n</ul>\n</li>\n<li>特征融合<ul>\n<li>特征金字塔(Lin, 2017a) </li>\n</ul>\n</li>\n<li>预测网络(分类回归任务)<ul>\n<li>两阶段目标检测：全连接</li>\n<li>单阶段目标检测：全卷积</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>标签分配与损失计算</p>\n<ul>\n<li><p>标签分配准则</p>\n<ul>\n<li>交并比准则<ul>\n<li>基于锚点框与真实框的交并比</li>\n</ul>\n</li>\n<li>距离准则<ul>\n<li>基于无锚点框 ，点到物体中心的距离</li>\n</ul>\n</li>\n<li>似然估计准则<ul>\n<li>分类、回归</li>\n</ul>\n</li>\n<li>二分匹配准则<ul>\n<li>分类、回归</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>损失函数<ul>\n<li>交叉熵函数</li>\n<li>聚焦损失函数(Lin, 2017b)</li>\n<li>回归损失函数: L1损失函数、平滑L1损失函数、IoU损失函数、GIoU损失函数(Reztofighi, 2019)、CIoU损失函数(Zheng, 2020b)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>后处理：为每个物体保留一个检测结果，去除冗余结果</p>\n<ul>\n<li>非极大值抑制NMS</li>\n<li>soft-NMS(Bodla, 2017)</li>\n<li>IoUNet(Jiang, 2018)</li>\n<li>定位方差(He, 2018)</li>\n<li>上下文推理(Pato, 2020)</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"基于锚点框方法\"><a href=\"#基于锚点框方法\" class=\"headerlink\" title=\"基于锚点框方法\"></a>基于锚点框方法</h3><p>描述：为空间每个位置设定多个矩形框（框的尺度和长宽比），尽可能的涵盖图像中的物体<br>分类：</p>\n<ol>\n<li>两阶段目标检测<ol>\n<li>提取k个候选框</li>\n<li>对候选框分类和回归</li>\n</ol>\n</li>\n<li>单阶段目标检测<ol>\n<li>直接对锚点框分类和回归</li>\n</ol>\n</li>\n</ol>\n<h3 id=\"基于无锚点框方法\"><a href=\"#基于无锚点框方法\" class=\"headerlink\" title=\"基于无锚点框方法\"></a>基于无锚点框方法</h3><p>分类：</p>\n<ol>\n<li>基于关键点目标检测：多个关键点集成到物体上</li>\n<li>基于内部点目标检测：物体内部点到物体边界的上下左右偏移量</li>\n</ol>\n<h2 id=\"基于双目相机流程及其涵盖的方法\"><a href=\"#基于双目相机流程及其涵盖的方法\" class=\"headerlink\" title=\"基于双目相机流程及其涵盖的方法\"></a>基于双目相机流程及其涵盖的方法</h2><p>方法流程与单目相同</p>\n<h3 id=\"基于直接视锥空间\"><a href=\"#基于直接视锥空间\" class=\"headerlink\" title=\"基于直接视锥空间\"></a>基于直接视锥空间</h3><p>描述：直接使用基础骨干提取的两个单目特征构造双目特征。<br>方法：</p>\n<ol>\n<li>串接特征构造<br> 不改变原单目特征的坐标空间</li>\n<li>平面扫描构造<br> 通过逐视差平面或者深度平面地扫描一对2维特征，所得三维特征即是匹配代价体</li>\n</ol>\n<h3 id=\"基于显式逆投影空间\"><a href=\"#基于显式逆投影空间\" class=\"headerlink\" title=\"基于显式逆投影空间\"></a>基于显式逆投影空间</h3><p>描述：将存在尺度变化和遮挡问题的视锥空间图像逆投影到尺度均匀、不存在重叠遮挡的3维空间，从而缓解视锥投影产生的问题。<br>方法：</p>\n<ol>\n<li>基于原始图像视差的逆投影方法<br> 先利用双目视差估计算法预测每个像素的视差，将像素的视差逆投影到三维空间生成电云，最后利用点云的3维检测方法进行目标检测</li>\n<li>基于特征体的逆投影方法<br> 通过插值和采样将平面扫描得到的匹配代价体变换到3维空间，利用了图像特征提供的颜色和纹理信息。</li>\n<li>基于候选像素视差的逆投影方法<br> 仅聚焦感兴趣目标区域的三维空间，先利用实例分割方案得到目标的前景像素，然后生成仅含前景区域的3维空间。<script type=\"math/tex; mode=display\">逆投影策略\\left\\{\\begin{align*}\n     & 前景共享3维空间\\\\\n     & 每个实例生成相互独立的3维子空间\n \\end{align*}\\right.</script></li>\n</ol>\n<h2 id=\"发展趋势\"><a href=\"#发展趋势\" class=\"headerlink\" title=\"发展趋势\"></a>发展趋势</h2><ol>\n<li>高效的端到端目标检测transform，加快收敛，减少计算资源。</li>\n<li>基于自监督学习的目标检测，目标检测任务存在数量和尺度不确定的物体。</li>\n<li>长尾分布目标检测，现实世界物体类别数量庞大且不同类别的物体数量存在极度不平衡。</li>\n<li>小样本、0样本目标检测能力的提高</li>\n<li>大规模双目目标检测数据集少，需要标注物体的2维和3维信息以及相机标注视差和相机参数，还需完善评价体系和开放测试平台</li>\n<li>弱监督双目目标检测</li>\n</ol>\n<h1 id=\"YOLOv7-Trainable-bag-of-freebies-sets-new-state-of-the-art-for-real-time-object-detectors\"><a href=\"#YOLOv7-Trainable-bag-of-freebies-sets-new-state-of-the-art-for-real-time-object-detectors\" class=\"headerlink\" title=\"YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors\"></a>YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</h1><p>作者：Chien-Yao Wang，2022</p>\n<h2 id=\"解决的问题\"><a href=\"#解决的问题\" class=\"headerlink\" title=\"解决的问题\"></a>解决的问题</h2><p>发现的问题:</p>\n<ol>\n<li>模型再参数化</li>\n<li>用动态标签分配技术后，如何将动态标签分配给模型的不同输出层</li>\n</ol>\n<p>本篇文章解决的问题：</p>\n<ol>\n<li>最高推理精度56.8%AP和最快推理速度160FPS,都达到最高水平，参与对比的模型有：YOLOv5、YOLOX、Scaled-YOLOv4、YOLOR、PPYOLOE、DETR、Deformable DETR、DINO-5scale-R50、ViT-Adapter-B、SWIN-L Cascade-Mask R-CNN、ConvNeXt-XL Cascade-Mask R-CNN</li>\n<li>支持移动GPU以及边缘端和云端GPU</li>\n<li>设计trainable bag-of-freebies方法，既可增强训练代价提高检测准确度又不增加推理代价</li>\n<li>提出planned re-parameterized model</li>\n<li>提出新的标签分配方法，coarse-to-fine lead guided label assigment</li>\n<li>提出extend and compound scaling方法，减少40%的模型参数和50%计算时间</li>\n</ol>\n<p>其他模型的优点和不足：</p>\n<ol>\n<li>YOLOX和YOLOR只改进各种GPU推理速度</li>\n<li>基于MobileNet, ShuffleNet, GhostNet针对CPU设计</li>\n<li>ResNet, DarkNet, DLA, CSPNet针对GPU设计</li>\n<li>YOLO和FCOS具有：1、快而强壮的网络架构，2、高效的特征集成方法，3、鲁棒的损失函数，4、高效的标签分配方法，5、准确的检测方法，6、高效训练方法</li>\n</ol>\n<h2 id=\"当前的不足\"><a href=\"#当前的不足\" class=\"headerlink\" title=\"当前的不足\"></a>当前的不足</h2><h2 id=\"使用的方法\"><a href=\"#使用的方法\" class=\"headerlink\" title=\"使用的方法\"></a>使用的方法</h2><h3 id=\"模型再参数化\"><a href=\"#模型再参数化\" class=\"headerlink\" title=\"模型再参数化\"></a>模型再参数化</h3><p>模型再参数化：融合多个计算模块于一体，可是为组装技术</p>\n<script type=\"math/tex; mode=display\">分类\\left\\{\\begin{align*}\n  & 模块间组合\\\\\n  & 模型间组合\n\\end{align*}\\right.</script><ol>\n<li>模型间组合方法<ol>\n<li>在不同训练集中训练多个相同模型，然后再平均模型的参数</li>\n<li>在不同的迭代次数间进行模型参数均值化</li>\n</ol>\n</li>\n<li>模块间组合方法<br> 在训练期间将一个模块分解成多个分支模块，在推理时将多个分支模块整合成一个完整模块</li>\n</ol>\n<h3 id=\"模型缩放\"><a href=\"#模型缩放\" class=\"headerlink\" title=\"模型缩放\"></a>模型缩放</h3><p>模型放缩可以增大和缩小模型使它适合不同计算能力的设备，满足不同的推理速度。</p>\n<script type=\"math/tex; mode=display\">放缩因子\\left\\{\\begin{align*}\n  & 分辨率resolution（输出图像尺度）\\\\\n  & 深度depth（隐藏层层数）\\\\\n  & 宽度width（通道数） \\\\\n  & 阶段stage（特征金字塔层数）\n\\end{align*}\\right.</script><p>放缩方法：网络架构搜索Network architecture search(NAS)，折中了网络参数大小、计算时间、推理速度和精确性</p>\n<p>放缩因子的影响：</p>\n<ol>\n<li>对基于非连接的网络架构，在进行模型放缩时由于每个隐藏层的入度和出度不被改变因此可以独立分析每个放缩因子对模型参数数量和计算速度的影响</li>\n<li>对基于连接的网络架构，在进行模型隐藏层深度放大或缩小时紧跟在计算模块后的转移/转化模块的入度会减小或增大，不能独立分析单个尺度因子的影响必须一起分析</li>\n<li>文章提出compound scaling method合成尺度方法既可保持原有模型的的性质又可保持最优结构</li>\n</ol>\n<h3 id=\"架构\"><a href=\"#架构\" class=\"headerlink\" title=\"架构\"></a>架构</h3><p>选取架构主要考虑1、模型参数数量，2、计算量，3、计算密度<br>采用Extended-ELAN(E-ELAN)扩展高效层聚合网络架构，该架构使用扩展基数层、清洗基数层、合并基数层增强网络学习能力</p>\n<p>基于早期版本的YOLO框架和YOLOR框架作为基本框架</p>\n<h3 id=\"可训练赠品袋trainable-bag-of-freebies\"><a href=\"#可训练赠品袋trainable-bag-of-freebies\" class=\"headerlink\" title=\"可训练赠品袋trainable bag-of-freebies\"></a>可训练赠品袋trainable bag-of-freebies</h3><ul>\n<li><p>计划再参数化卷积</p>\n<ol>\n<li>如何将再参数化卷积和不同的网络结合？</li>\n<li>提出planned re-parameterized convolution</li>\n<li>提出无identity connection的RePConv构造planned re-parameterized convolution</li>\n<li>用RepConvN网络层替换3堆叠ELAN架构中不同位置处的3x3卷积层</li>\n</ol>\n</li>\n<li><p>以粗为辅以精为主的损失值</p>\n<ol>\n<li>深度监督是在网络的中间层添加额外的辅助头，将带有损失值信息的浅层网络权重作为引导方式</li>\n<li>将负责最后输出的头称为主头，将用于协助训练的头称为辅头</li>\n<li>采用软标签即使用网络预测输出的性质和分布和考虑实际标签，使用一些计算和优化方式生成可靠的标签</li>\n<li>如何分配软标签到主头和辅头？<ol>\n<li>分别计算主头和辅头预测结果，使用各自的分配器结合实际结果制作各自标签，再通过各自标签和头计算损失</li>\n<li>文章提出经分配器用主头和实际结果制作由粗到精的等级标签，再将这些等级标签用在主头和辅头上计算损失值</li>\n</ol>\n</li>\n</ol>\n</li>\n<li><p>批归一化放到conv-bn-activation 拓扑结构中</p>\n</li>\n<li><p>YOLOR的隐性知识以串行和并行方式结合到卷积特征图中</p>\n</li>\n<li><p>EMA模型</p>\n</li>\n</ul>\n<h1 id=\"AN-IMAGE-IS-WORTH-16x16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE\"><a href=\"#AN-IMAGE-IS-WORTH-16x16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE\" class=\"headerlink\" title=\"AN IMAGE IS WORTH 16x16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\"></a>AN IMAGE IS WORTH 16x16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</h1><p>作者：Alexey Dosovitskiy, 2021</p>\n<h2 id=\"解决的问题-1\"><a href=\"#解决的问题-1\" class=\"headerlink\" title=\"解决的问题\"></a>解决的问题</h2><p>根据Transformer计算的效率和可扩展性以及借鉴Transformer在自然语言方面取得的成功将其应用于图像上</p>\n<ol>\n<li>证明在大样本上14M-300M图，Transformer胜过CNN</li>\n<li>可以处理中等分辨率图像</li>\n<li>在更大规模数据集而非ImageNet数据集，探索图像识别</li>\n</ol>\n<p>前人工作优点和不足：</p>\n<ol>\n<li>把CNN框架和自注意力结合(Wang,2018; Carion, 2020)</li>\n<li>用自注意力替换整个卷积网络(Ramachandran, 2019; Wang, 2020a)</li>\n<li>ResNet架构在大尺度图片识别上是效果好的(Mahajan, 2018; Xie, 2020; Kolesnikov, 2020)</li>\n<li>transformer用于机器翻译(Vaswani, 2017)</li>\n<li>将transformer用到图像处理环境中<ul>\n<li>只将自注意力应用于代查询像素的局部临域中，并非全局应用(Parmar,2018)</li>\n<li>局部多头点积自注意力块完全替换卷积(Hu,2019; Ramachandran, 2019; Zhao, 2020)</li>\n<li>稀疏Transformer在全局自注意力中使用放缩近似以适应图片(Child, 2019)</li>\n<li><strong>(Cordonnier, 2020)提出的模型也是ViT但是没有证明大规模预训练模型可以甚至超过CNN模型，使用的2x2块太小只能适应小分辨率图像</strong></li>\n</ul>\n</li>\n<li>(Sun, 2017)研究CNN性能如何随数据集大小变化</li>\n<li>(Kolesnikov,2020; Djolonga,2020)从大规模的数据集上探索CNN的迁移学习</li>\n</ol>\n<h2 id=\"当前的不足-1\"><a href=\"#当前的不足-1\" class=\"headerlink\" title=\"当前的不足\"></a>当前的不足</h2><ol>\n<li>Transformer和CNN相比缺少偏移量无法实现平移等变映射和无法进行局部化，因此在小样本中泛化能力弱</li>\n<li><strong>应用ViT到其它计算机视觉任务，例如目标检测和分割</strong></li>\n<li>持续开发自监督与训练方法</li>\n</ol>\n<h2 id=\"使用的方法-1\"><a href=\"#使用的方法-1\" class=\"headerlink\" title=\"使用的方法\"></a>使用的方法</h2><ul>\n<li>分割图片成若干块，给这些块提供顺序线性嵌入体，并将嵌入体作为Transformer的输入</li>\n<li>选用原始Transformer(Vaswani, 2017)</li>\n<li>框架<br>  <img src=\"./image-20221125100901770.png\" alt=\"image-20221125100901770\"><br>  标准Transformer接收1维符号嵌入序列，将图像$x\\in\\mathbb{R}^{H\\times W\\times C}$分割成有序排列小块$x_p\\in\\mathbb{R}^{N\\times(P^2\\cdot C)}$，输入$z_0=[x_{class}; x_p^1E; x_p^2E; \\cdots; x_p^NE]+E_{pos}$, $x_{class}\\in\\mathbb{R}^{1\\times D}$, $E\\in \\mathbb{R}^{(P^2\\cdot C)\\times D}$, $E_{pos}\\in\\mathbb{R}^{(N+1)\\times D}$</li>\n</ul>\n<h1 id=\"Pyramid-Vision-Transformer-A-Versatile-Backbone-for-Dense-Prediction-without-Convolutions\"><a href=\"#Pyramid-Vision-Transformer-A-Versatile-Backbone-for-Dense-Prediction-without-Convolutions\" class=\"headerlink\" title=\"Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions\"></a>Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions</h1><p>作者：Wenhai Wang, 2021</p>\n<h2 id=\"解决的问题-2\"><a href=\"#解决的问题-2\" class=\"headerlink\" title=\"解决的问题\"></a>解决的问题</h2><ol>\n<li>文章(PVT)解决将transform移植到密集预测任务的问题</li>\n<li>产生高输出分辨率，利用收缩的金字塔减少计算量</li>\n<li>PVT继承了CNN和Transformer的优点，成为应对各种视觉任务的无需卷积的统一骨干架构，可以直接替换CNN骨干架构</li>\n<li>提高下游任务的性能，包括：目标检测，实例分割，语意分割</li>\n<li>克服传统transformer问题方式<ol>\n<li>采用获取精细图像块（每个图像块为4x4像素）作为输入，以学习高分辨率的表示</li>\n<li>采用逐渐缩小的金字塔形式减小transformer在深层网络中的序列长度，以减小计算量</li>\n<li>采用空间减小注意力层spatial-reduction attention(SRA), 这近一步在学习高分辨率特征时减小资源损耗</li>\n</ol>\n</li>\n<li>具有的优点<ol>\n<li>CNN的局部接收视野随网络深度的增加而增加，PVT产生的是全局接收视野，这有利于检测和分割任务</li>\n<li>借助金字塔结构的优势，易于将PVT插入到许多代表密集预测的管道中，例如RetinaNet和Mask R-CNN</li>\n<li>通过结合PVT和其它特殊任务的Transformer解码器可以构建无卷积的管道，例如PVT+DETR用作目标检测</li>\n</ol>\n</li>\n</ol>\n<p>前人的工作及优缺点</p>\n<ol>\n<li><p>Vision Transformer(ViT)被用作分类任务</p>\n</li>\n<li><p>Vision Transformer(ViT)是一种柱状机构具有粗糙的输入图像块，不是很适合像素级别的致密预测任务如目标检测和分割任务，原因有：1、输出的特征图是单一尺度，分辨率低；2、高计算和内存占用成本, </p>\n</li>\n<li><p>CNN在视觉上取得巨大成功(Karen,2015; Kaiming,2017; Saining,2017)</p>\n</li>\n<li><p>将视觉任务建模成具有可学习查询功能的字典查找问题，使用Transformer解码器作为特殊任务的头应用于卷积框架的顶层(Nicolas,2020; Christian,2017; Enze,2021)</p>\n</li>\n<li><p>网络架构</p>\n<ol>\n<li><p>(Yann, 1998)首次引入CNN分辨手写数字，在整个图像空间共享卷积核参数实现平移等变性</p>\n</li>\n<li><p>(Alex, 2012; Karen, 2015)在大尺度图片分类数据集中使用堆叠的卷积块</p>\n</li>\n<li><p>GoogLeNet(Christain,2015)包含了多核路径</p>\n</li>\n<li><p>多路径卷积模块的效率在Inception系列网络(Christian,2016;)、ResNeXt(Saining, 2017)、 DPN(Yunpeng, 2017)、MixNet(Wenhai, 2018)、SKNet(Xiang,2019)网络中被验证</p>\n</li>\n<li><p>(Kaiming,2016)在ResNet网络中引入跳跃式连接卷积模块，这有助于训练更深的网络</p>\n</li>\n<li><p>(Gao,2017)在DenseNet中引入密集连接拓扑结构，把每个卷积模块同它前面的所有卷积模块相连</p>\n</li>\n</ol>\n</li>\n<li><p>密集预测任务</p>\n<ol>\n<li>目的：在特征图上进行像素级别的分类和回归</li>\n<li><script type=\"math/tex; mode=display\">分类\\left\\{\\begin{align*}\n& 目标检测\\left\\{\\begin{array}{l}\n           单阶段\\left\\{\\begin{array}{l}\n                          SSD(Wei,2016) \\\\\n                          RetinaNet(Tsung-Yi\\; Lin,2017) \\\\\n                          FCOS(Zhi\\; Tian,2019) \\\\\n                          GFL(Xiang,2020) \\\\\n                          PolarMask(Enze\\;Xie,2021) \\\\\n                          OneNet(Peize\\; Sun,2020) \\\\\n                       \\end{array}\\right. \\\\\n           多阶段\\left\\{\\begin{array}{l}\n                          Faster R-CNN(Shaoqing\\; Ren,2015) \\\\\n                          Mask R-CNN(Kaiming\\; He,2017) \\\\\n                          Cascade R-CNN(Zhaowei\\; Cai,2018) \\\\\n                          Sparse R-CNN(Peize\\; Sun, 2021) \\\\\n                       \\end{array}\\right. \\\\\n           结合CNN和Transformer\\; decoder\\left\\{\\begin{array}{l}\n                           DETR(Nicolas\\; Carion, 2020)\\\\\n                           deformable DETR(Xizhou\\; Zhu,2021)\\\\\n                        \\end{array}\\right.\n        \\end{array}\\right.\\\\\n& 语意分割 \\left\\{\\begin{array}{l}\n               FCN(Jonathan\\; Long,2015)\\\\\n               deconvolution\\; operation(Hyeonwoo\\; Noh,2015)\\\\\n               U-Net(Olaf\\; Ronneberger,2015)\\\\\n               添加金字塔池化(HengShuang\\; Zhao,2017)\\\\\n               添加FPN头(Alexander\\; Kirillov,2019)\\\\\n               DeepLab(Liang-Chieh\\; Chen,2017)\\\\\n            \\end{array}\\right.\\\\\n& 实例分割\n\\end{align*}\\right.</script></li>\n<li><p>自注意力和变换器</p>\n<ol>\n<li>卷积滤波器权重经过训练后被固定无法动态适应不同的输入，(Xu Jia,2016)使用动态滤波器，(Ashish Vaswani,2017)使用自注意力<ol>\n<li>非局部模块被(Xiaolong Wang,2018)引入解决时间和空间在大尺度上的依赖性，但是代价是计算成本和内存占用成本</li>\n<li>(Zilong Huang,2019)引入十字交叉路径Criss-cross减小注意力机制的复杂度</li>\n<li></li>\n</ol>\n</li>\n</ol>\n</li>\n</ol>\n</li>\n</ol>\n<h2 id=\"当前的不足-2\"><a href=\"#当前的不足-2\" class=\"headerlink\" title=\"当前的不足\"></a>当前的不足</h2><ol>\n<li>有许多特殊模块和运算方法是专门为CNN设计的因此没有在PVT中引入，例如：Squeeze-and-excitation network(SE)模块, Selective kernel network(SK)模块, 膨胀卷积模块, 模型精简模块，Network architecture search(NAS)模块</li>\n<li>基于Transformer 的模型在视觉应用上起步晚可以应用于OCR, 3D和医疗图像分析</li>\n</ol>\n<h2 id=\"使用的方法-2\"><a href=\"#使用的方法-2\" class=\"headerlink\" title=\"使用的方法\"></a>使用的方法</h2><h3 id=\"框架\"><a href=\"#框架\" class=\"headerlink\" title=\"框架\"></a>框架</h3><p>整体框架分成4个阶段，每个阶段都产生不同尺度的特征图，每个阶段都具有相似的结构，包括：1、分块嵌入层，2、若干Transformer编码层</p>\n<ul>\n<li><p>结构（以第一阶段为例）</p>\n<ol>\n<li><p>第一阶段输入图像HxWx3, 并分割成$\\frac{HW}{4^2}$多个图块，每块大小4x4x3</p>\n</li>\n<li><p>将分割块展平进行线性投影变换得到嵌入块形状为$\\frac{HW}{4^2}\\times C_1$</p>\n</li>\n<li><p>将嵌入块和位置嵌入一起传入具有$L_1$层的Transformer编码器中</p>\n</li>\n<li><p>输出特征层$F_1$形状是$\\frac{H}{4}\\times\\frac{W}{4}\\times C_1$</p>\n</li>\n<li><p>以此类推，以上一阶段输出作为下一阶段的输入，选取的块的大小相对于原始图像分别是8x8，16x16，32x32像素，即第i阶段的块大小为$P_i$，得到的特征图为$F_i$:$\\{F_2$,$F_3$,$F_4\\}$,特征图尺寸为$\\frac{H}{8}\\times\\frac{W}{8}\\times C_2$,$\\frac{H}{16}\\times\\frac{W}{16}\\times C_3$,$\\frac{H}{32}\\times\\frac{W}{32}\\times C_4$,</p>\n<p><img src=\"./image-20221127151329769.png\" alt=\"image-20221127151329769\"></p>\n</li>\n</ol>\n</li>\n<li><p>transformer的特征金字塔</p>\n<ul>\n<li>CNN的特征金字塔是使用不同的卷积跨步来实现，PVT是使用逐步缩小策略实现</li>\n</ul>\n</li>\n<li><p>transformer编码器</p>\n<ol>\n<li>Transformer编码器在第i阶段有$L_i$个编码层，每一个编码层又包含：1、注意力层，2、feed-forward层</li>\n<li><p>使用spatial-reduction注意力层(SRA)替换传统多头注意力层(MHA)，为了处理高分辨率特征图(4跨步特征图)<br> <img src=\"./image-20221127153059574.png\" alt=\"image-20221127153059574\"><br> SRA特点：减小Key和Value输入的尺寸：</p>\n<script type=\"math/tex; mode=display\">\\begin{align*}\n         SRA(Q,K,V)&=Concat(head_0,\\cdots,head_{N_i})W^O\\\\\n         head_j&=Attention(QW_j^Q,SR(K)W_j^K,SR(V)W_j^V)\\\\\n         SR(x)&=Norm(Reshape(x,R_i)W^S)\\\\\n         Attention(\\vec{q},\\vec{k},\\vec{v})&=Softmax(\\frac{\\vec{q}\\vec{k}^T}{\\sqrt{\\vec{d}_{head}}})\\vec{v}\\\\\n \\end{align*}</script><p> 符号说明：$Concat(\\cdot)$链接操作；$W_j^Q\\in\\mathbb{R}^{C_i\\times d_{head}}$,$W_j^K\\in\\mathbb{R}^{C_i\\times d_{head}}$,$W_j^V\\in\\mathbb{R}^{C_i\\times d_{head}}$,$W^O\\in\\mathbb{R}^{C_i\\times C_i}$,$W^S\\in\\mathbb{R}^{(R_i^2C_i)\\times C_i}$都是线性投影矩阵；$N_i$注意力的头数；$d_{head}=\\frac{C_i}{N_i}$每个头的大小；$SR(\\cdot)$减小空间尺度操作；$x\\in\\mathbb{R}^{(H_iW_i)\\times C_i}$表示输入序列；$R_i$表示缩减比例；$Reshape(x,R_i)=\\frac{H_iW_i}{R_i^2}\\times(R_i^2C_i)$修改张量形状;$Norm(\\cdot)$层归一化；$Attention(\\cdot,\\cdot,\\cdot)$注意力得分</p>\n</li>\n</ol>\n</li>\n</ul>\n<h1 id=\"Deformable-DETR-Deformable-Transformers-For-End-to-End-Object-Detection\"><a href=\"#Deformable-DETR-Deformable-Transformers-For-End-to-End-Object-Detection\" class=\"headerlink\" title=\"Deformable DETR: Deformable Transformers For End-to-End Object Detection\"></a>Deformable DETR: Deformable Transformers For End-to-End Object Detection</h1><p>作者：Xinzhou Zhu,2020</p>\n<h2 id=\"解决的问题-3\"><a href=\"#解决的问题-3\" class=\"headerlink\" title=\"解决的问题\"></a>解决的问题</h2><ol>\n<li>改善空间分辨率问题和收敛慢问题</li>\n<li>Deformable DETR在检测小目标上优于DETR</li>\n<li>结合了deformable convolution的稀疏空间采样和transformer的相关性建模能力</li>\n</ol>\n<p>前人的工作以及优缺点：</p>\n<ol>\n<li>DETR(Nicolas Carion,2020)用于剔除目标检测中辅助成分的需求（例如，非极大值抑制），缺点是收敛慢、有限的特征空间分辨率，该模型结合了CNN模型和Transformer编解码模型，小目标效果差，优点就是具有元素间的相关性</li>\n<li>目标检测使用了一些辅助成分(Li Liu,2020),例如锚点生成，基于规则的训练目标分配，非极大抑制</li>\n<li>deformable convolution(Jifeng Dai,2017)用于稀疏空间定位，因此高效、收敛快，缺点就是缺少元素间的相关性</li>\n<li>(Ashish Vaswani,2017)在transformer中引入自注意力和交叉注意力机制,缺点是时间成本和内存占用高</li>\n<li>解决时间和内存的方式有三种<ol>\n<li>对关键点使用预定义(冻结参数)的稀疏注意力模式<ul>\n<li>将注意力模式限制在固定局部窗口中使用(Peter J Liu,2018a;Niki Parmar,2018;Rewon Child,2019;Zilong Huang;2019),优点是减少复杂度，缺点是丢失全局信息</li>\n<li>以固定间隔方式设置关键点(Jonathan Ho,2019)，优点：增加接受视野</li>\n<li>允许少许特殊符号可以访问所有关键点(Iz Beltagy,2020),增加全局信息</li>\n</ul>\n</li>\n<li>学习依赖数据的稀疏注意力<ul>\n<li>基于注意力机制的局部敏感哈希映射LSH(Nikita Kitaev,2020),将查询元素和关键字元素映射到不同的区域</li>\n<li>(Aurko Roy,2020)用k-means聚类找到最相关的关键字元素</li>\n<li>(Yi Tay,2020a)对逐块的稀疏注意力学习块交换</li>\n</ul>\n</li>\n<li>研究自注意力机制的低级别性质<ul>\n<li>通过线性投影减少关键点数量(Sinong Wang,2020)</li>\n<li>通过核函数近似计算自注意力(Krzysztof Choromanski,2020)</li>\n</ul>\n</li>\n</ol>\n</li>\n<li>多尺度特征表示<ul>\n<li>FPN(Tsung-Yi Lin,2017a),自上而下生成多尺度特征图</li>\n<li>PANet(Shu Liu,2018b)，自下而上生成多尺度特征图</li>\n<li>从全局自注意力中提取所有尺度上的特征(Tao Kong,2018)</li>\n<li>U-shape模块融合多尺度特征(Qijie Zhao,2019)</li>\n<li>NAS-FPN(Golnaz Ghiasi,2019)、Auto-FPN(Hang Xu,2019)通过神经架构搜索自动进行交叉尺度连接</li>\n<li>BiFPN(Mingxing Tan,2020)</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"当前的不足-3\"><a href=\"#当前的不足-3\" class=\"headerlink\" title=\"当前的不足\"></a>当前的不足</h2><ol>\n<li>简单且高效的可迭代的边界框调优机制进一步改善性能</li>\n<li>可将Deformable DETR应用到2阶段目标识别中，先生成推荐区域，再将推荐区域作为目标query送入解码器</li>\n</ol>\n<h2 id=\"使用的方法-3\"><a href=\"#使用的方法-3\" class=\"headerlink\" title=\"使用的方法\"></a>使用的方法</h2><ol>\n<li>应用在若干的采样位置点处，这些点作为特征图中关键特征点</li>\n<li>使用图像尺度放缩而不是特征金字塔应用于deformable注意力模型</li>\n</ol>\n<h3 id=\"经典多头注意力结构\"><a href=\"#经典多头注意力结构\" class=\"headerlink\" title=\"经典多头注意力结构\"></a>经典多头注意力结构</h3><p>query元素代表了要输出的句子的目标单词，key元素代表输入句子中的单词，多头注意力模块根据测量的query-key对的相似性权重因子汇聚这些key。用$q\\in\\Omega_q$索引具有表达特征$z_q\\in\\mathbb{R}^C的$query元素；用$k\\in\\Omega_k$索引具有表达特征$x_k\\in\\mathbb{R}^C的$key元素,$C$是特征维度，$\\Omega_q$和$\\Omega_k$给出了query和key的元素总数;多头注意力特征计算：</p>\n<script type=\"math/tex; mode=display\">MultiHeadAttn(z_q,x)=\\sum\\limits_{m=1}^MW_m[\\sum\\limits_{k\\in\\Omega_k}A_{mqk}\\cdot W_m^\\prime x_k]</script><p>这里$m$索引各个注意力头总共有M个注意力头，$W_m^\\prime\\in\\mathbb{C_v\\times C}$和$W_m\\in\\mathbb{C\\times C_v}$是第m头待学习的矩阵，$C_v=C/M$,注意力权重$A_{mqk}\\propto \\exp\\{\\frac{z_q^TU_m^TV_mx_k}{\\sqrt{C_v}}\\}$满足归一化$\\sum\\limits_{k\\in\\Omega_k}A_{mqk}=1$, 这里$U_m\\in\\mathbb{R}^{C_v\\times C}$和$V_m\\in\\mathbb{R}^{C_v\\times C}$同样是待学习的矩阵。为了消除不同空间位置的奇异性，表达的查询和关键字特征$z_q$和$x_k$需要和位置嵌入体做结合。</p>\n<h3 id=\"DETR-Transformer编解码架构\"><a href=\"#DETR-Transformer编解码架构\" class=\"headerlink\" title=\"DETR Transformer编解码架构\"></a>DETR Transformer编解码架构</h3><p>采用Hungarian(匈牙利)损失函数借助二分(双边)匹配实现对每一个真实边框都有唯一预测值。用$y$表示ground truth集合，$\\hat{y}=\\{\\hat{y}_i\\}_{i=1}^N$表示有N个预测值的集合，当$N$远大于图像中物体个数时，可以认为$y$集合也是由N个真实结果构成没有目标的结果被符号$\\phi$填充。搜索N个元素$\\sigma\\in\\mathfrak{G}_N$的一个置换$\\hat{\\sigma}=\\mathop{\\arg\\min}\\limits_{\\sigma\\in\\mathfrak{G}_N}\\sum\\limits_i^N\\mathcal{L}_{match}(y_i,\\hat{y}_{\\sigma(i)})$, （1）$\\mathcal{L}_{match}(y_i,\\hat{y}_{\\sigma(i)})$是真值$y_i$和具有索引$\\sigma(i)$的预测值之间的逐对匹配代价函数；（2）第i个真值元素可以看成$y_i=(c_i,b_i)$,$c_i$是目标类别标签，符号$\\phi$用N/A表示，$b_i=\\{b_x,b_y,b_w,b_h\\}\\in[0,1]^4$是向量记录了中心坐标和相对图像大小的高度和宽度；（3）对索引$\\sigma(i)$的预测值，定义类$c_i$的概率为$\\hat{p}_{\\sigma(i)}(c_i)$,预测边框为$\\hat{b}_{\\sigma(i)}$，定义：</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}_{match}(y_i,\\hat{y}_{\\sigma(i)})=-1_{\\{c_i\\neq\\phi\\}}\\hat{p}_{\\sigma(i)}(c_i)+1_{\\{c_i\\neq\\phi\\}}\\mathcal{L}_{box}(b_i,\\hat{b}_{\\sigma(i)})</script><script type=\"math/tex; mode=display\">\\mathcal{L}_{box}(b_i,\\hat{b}_{\\sigma(i)})=\\lambda_{iou}\\mathcal{L}_{iou}(b_i,\\hat{b}_{\\sigma(i)})+\\lambda_{L1}||b_i-\\hat{b}_{\\sigma(i)}||_1</script><p>这是真实框集合中的每一框和预测框集合中的每一个框匹配，损失值最小的预测框为该真实框的最佳匹配框，得到唯一匹配。匈牙利损失函数定义如下：</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}_{Hungarian}(y,\\hat{y})=\\sum\\limits_{i=1}^N\\left[-1_{\\{c_i\\neq\\phi\\}}\\hat{p}_{\\hat{\\sigma}(i)}(c_i)+1_{\\{c_i\\neq\\phi\\}}\\mathcal{L}_{box}(b_i,\\hat{b}_{\\hat{\\sigma}(i)})\\right]</script><p>DETR主要使用Transformer编码解码架构将特征图像映射成一组待查目标对象的特征，feed-forward网络FFN用做回归分支预测边框坐标，线性投影用作分类分支预测目标类别。编码部分输入ResNet的生成的特征图，取query和key元素，它们都是特征图的每一个像素；解码部分输入包含来自编码部分的特征图和N个目标查询，有两个注意力模型：1、cross-attention,2、self-attention。在cross-attention中查询元素是从特征图中提取特征的N个目标查询，关键字元素来自于编码输出的特征图；在self-attention中，查询元素要获取他们的关系因此查询元素和关键字元素都来自于N个目标查询</p>\n<h3 id=\"Deformable-transformer\"><a href=\"#Deformable-transformer\" class=\"headerlink\" title=\"Deformable transformer\"></a>Deformable transformer</h3><ol>\n<li><p>单尺度Deformable attention<br> 通过对每个查询元素只分配少许固定数量的关键字元素，这些关键字元素取自一个参考点附近的点，无需对所有关键字进行匹配，所以无视特征图尺度，已知特征图$x\\in\\mathbb{R}^{C\\times H\\times W}$,$q$是查询元素特征$z_q$索引以及分配的一个2维参考点$p_q$索引</p>\n<script type=\"math/tex; mode=display\">DeformAttn(z_q,p_q,x)=\\sum\\limits_{m=1}^MW_m\\left[\\sum\\limits_{k=1}^KA_{mqk}\\cdot W_m^\\prime x(p_q+\\Delta p_{mqk})\\right]</script><p> 这里m是注意力头的索引，k是采样到的关键字元素索引，K是采样关键字总数，显然$K\\ll HW$。$\\Delta p_{mqk}$和$A_{mqk}$表示对第m注意力头、第k个关键字元素采样点的采样邻域半径和注意力权重，注：标量注意力权重$A_{mqk}\\in [0,1]$要归一化处理, $\\Delta p_{mqk}\\in \\mathbb{R}^2$没有约束范围，当$p_q+\\Delta p_{mqk}$是小数时双线性插值可以使用，$\\Delta p_{mqk}$和$A_{mqk}$是通过对查询元素特征$z_q$线性投影计算得到</p>\n</li>\n<li><p>多尺度Deformable attention<br> 设$\\{x^l\\}_{l=1}^L$是用于输入的多尺度特征图，$x^l\\in\\mathbb{R}^{C\\times H_l\\times W_l}$, $\\hat{p}_q\\in[0,1]^2$是每个查询元素q对应参考点的归一化二维坐标，坐标的归一化操作是对每一个尺度特征图进行, 模型公式为</p>\n<script type=\"math/tex; mode=display\">MSDeformAttn(z_q,\\hat{p}_q,\\{x^l\\}_{l=1}^L)=\\sum\\limits_{m=1}^MW_m\\left[\\sum\\limits_{l=1}^L\\sum\\limits_{k=1}^KA_{mlqk}\\cdot W_m^\\prime x^l(\\phi_l(\\hat{p}_q)+\\Delta p_{mlqk})\\right]</script><p> m是注意力头索引，l是输入特征图的尺度等级索引，k是采样点的索引，$\\Delta p_{mlqk}$和$A_{mlqk}$表示在l层尺度上的特征图、对第m注意力头、第k个关键字元素采样点的采样邻域半径和注意力权重，$\\phi_l(\\hat{p}_q)$将归一化的坐标缩放回尺度为l的特征图中的坐标</p>\n</li>\n<li><p>编码器<br> 编码器的输入输出都是具有相同分辨率的多尺度特征图，编码器的多尺度特征图$\\{x^l\\}_{l=1}^{L-1},L=4$取自ResNet的输出特征图$C_3$到$C5$分辨率分别为$H/2^3$到$H/2^5$,而最低分辨率的特征图$x^4$取自$C_5$特征图进行3x3步长2的卷积后得到的特征图$C_6$,所有的输入特征图的通道数都是256：<br> <img src=\"./image-20221130151059959.png\" alt=\"image-20221130151059959\"><br> query和key元素都是多尺度特征图的像素，每个query像素的参考点就是其自身，为了识别每个query像素在哪个尺度图上，除了添加位置嵌入体外，需要添加尺度嵌入体$e_l$到特征表达中，区别是位置嵌入体是固定编码，尺度嵌入体$\\{e_l\\}_{l=1}^L$需要连同网络一起训练获取。</p>\n</li>\n<li><p>解码器<br> 包含cross-attention和self-attention,query元素在两类注意力机制中都是目标query，目标query在cross-attention中取自特征图，而key元素是编码器的输出特征图；在self-attention中，key元素是目标query。每个目标query参考点的二维归一化坐标需要经过线性投影和激活函数从目标query的嵌入体中给出<br> <img src=\"./image-20221130160631440.png\" alt=\"image-20221130160631440\"><br> 模型提取的图像特征都是在参考点周围的点，所以我们预测的边框都是相对于参考点的偏移量，参考点初始值都是边框的中心点</p>\n</li>\n</ol>\n<h1 id=\"End-to-End-Object-Detection-with-Transformers\"><a href=\"#End-to-End-Object-Detection-with-Transformers\" class=\"headerlink\" title=\"End-to-End Object Detection with Transformers\"></a>End-to-End Object Detection with Transformers</h1><p>作者：Nicolas Carion,2020</p>\n<h2 id=\"解决的问题-4\"><a href=\"#解决的问题-4\" class=\"headerlink\" title=\"解决的问题\"></a>解决的问题</h2><ol>\n<li>提出目标检测新方法DEtection TRansformer(DETR)：将目标检测看作单向集合预测问题</li>\n<li>不再使用辅助设计成分，例如非极大值抑制抑制，锚点，是完全端到端的理念，流水化检测过程直接预测具有相对输入图像的绝对边框的检测集合</li>\n<li>结合了二分匹配损失和具有并行解码(非自动回归)的transformer，损失函数对预测目标交换具有不变性，因此可以并行执行</li>\n<li>对大目标具有更好的性能，可能是由于transformer的非局部计算</li>\n</ol>\n<p>前人的工作及其优缺点：</p>\n<ol>\n<li>以间接方式解决预测边框和种类任务，通过对大量推荐边框(Ren,S.,2015; Cai,Z.,2019)、锚点(Lin,T.Y.,2017)和窗口中心(Zhou,X.,2019;Tian,Z.,2019)定义回归和分类问题,通过后处理步骤削减近邻重复预测</li>\n<li>(Stewart,R.J.,2015;Hosang,J.H.,2017;Bodla,N.,2017,Rezatofighi,S.H.,2018)要么添加了其他形式的先验内容，要么在有挑战的基准样本上没有证明有强大竞争力</li>\n<li>(Stewart,R.J.,2015;Romera-Paraedes,B.,2015;Park,E.,2015)关注具有RNN自动回归解码</li>\n<li>集合预测<br> 基础的集合预测任务是多标签分类(Rezatofighi,S.H.,2017;Pineda,L.,2019)而目标检测要考虑元素间暗含的结构即识别近邻边框，因此不能用one-vs-rest方式处理问题。使用后处理方式例如非极大抑制处理近邻重叠问题；利用全局的对所有预测结果建立相互影响的推理方案规避这种冗余问题。集合预测可以使用致密的全连接网络(Erhan,D.,2014)、自动回归序列模型(recurrent neural network:Vinyals,O.,2016)。损失函数可以基于Hungarian算法(Kuhn,H.W.,1955)保证交换不变性和每个目标元素都有唯一匹配</li>\n<li>transformer和并行解码<br> Transformer是一个基于注意力机制的搭建模块用于机器翻译(Vaswani,A.,2017)；注意力机制是一个神经网络层，可以从整个输入序列中汇集信息(Bahdanau,D.,2015)；Transformer用在自动回归模型，根据之前的句子到句子模型(Sutskever,I.,2014)生成一个接一个的输出符号,这种方法缺点是推理成本(正比于输出长度)和很难批处理，所以需要开发并行序列生成，可以应用在语音(Oord,A.v.d.,2017)、机器翻译(Gu,J.,2018;Ghazvininejad,M.,2019)、单词表达学习(Devlin,J.,2019)、语音识别(Chan,W.,2020)。将transformer和并行解码结合权衡了计算成本和集合预测需要的全局计算能力</li>\n<li>目标检测<br> 目标检测根据一些初始猜测做预测，两阶段目标检测是根据推荐框(Ren,S.,2015;Cai,Z.,2019)做预测，单阶段目标检测是根据锚点(Lin,T.Y.,2017)或可能的目标中心网格(Zhou,X.,2019;Tian,Z.,2019)做预测,这类目标检测严重依赖初始猜测设置方式(Zhang,S.2019)。<ul>\n<li>基于集合损失<br>  早期的深度学习模型只用卷积和全连接层建模不同预测的关系并用辅助后处理改善性能；最近有些检测器使用真值和预测值之间的非唯一分配规则并结合NMS(Ren,S.,2015;Lin,T.Y.,2017;Zhou,X.,2019);可学习NMS方法(Hosang,J.H.,2017;Bodla,N.,2017)和关系网络(Hu,H.,2018)利用注意力显式建模不同预测间的关系，直接使用集合损失不在需要后处理步骤，但是也需要上下文特征例如推荐框坐标。</li>\n<li>循环探测<br>  端到端方法(Stewart,R.J.,2015;Romera-Paredes,B.,2015;Park,E.,2015;Ren,M.,2017)使用二分匹配损失以及基于CNN的编解码框架直接产生一组边界框，这些方法只在小样本数据中评估，基于的是自动回归模型没有利用具有并行解码的transformer。</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"当前的问题\"><a href=\"#当前的问题\" class=\"headerlink\" title=\"当前的问题\"></a>当前的问题</h2><ol>\n<li>对小目标性能不佳，可以用FPN进行改善</li>\n<li>长时间的训练</li>\n</ol>\n<h2 id=\"使用的方法-4\"><a href=\"#使用的方法-4\" class=\"headerlink\" title=\"使用的方法\"></a>使用的方法</h2><p>基于一组全局损失，通过二分匹配给出唯一预测，使用transformer编解码架构，需要一组固定数量的学习好的目标query, DETR会推理目标和全局图像的关系然后输出最终的预测集合<br><img src=\"./image-20221201110746341.png\" alt=\"image-20221201110746341\"></p>\n<p><img src=\"./image-20221202090850052.png\" alt=\"image-20221202090850052\"></p>\n<h3 id=\"目标检测集合预测损失\"><a href=\"#目标检测集合预测损失\" class=\"headerlink\" title=\"目标检测集合预测损失\"></a>目标检测集合预测损失</h3><p>DETR推理N个固定大小的预测结果集合，N明显多于图像中典型的目标数量，给出预测目标（类别，位置，大小）同真值比较后的好坏。用$y$表示ground truth目标集合，$\\hat{y}=\\{\\hat{y}_i\\}_{i=1}^N$表示有N个预测值的集合，当$N$远大于图像中物体个数时，可以认为$y$集合也是由N个真实结果构成没有目标的结果被符号$\\phi$填充。两个集合之间进行二分匹配，搜索N个元素$\\sigma\\in\\mathfrak{G}_N$的一个置换以得到最小代价$\\hat{\\sigma}=\\mathop{\\arg\\min}\\limits_{\\sigma\\in\\mathfrak{G}_N}\\sum\\limits_i^N\\mathcal{L}_{match}(y_i,\\hat{y}_{\\sigma(i)})$, （1）$\\mathcal{L}_{match}(y_i,\\hat{y}_{\\sigma(i)})$是真值$y_i$和具有索引$\\sigma(i)$的预测值之间的逐对匹配代价函数；（2）第i个真值元素可以看成$y_i=(c_i,b_i)$,$c_i$是目标类别标签，符号$\\phi$用N/A表示，$b_i=\\{b_x,b_y,b_w,b_h\\}\\in[0,1]^4$是向量记录了中心坐标和相对图像大小的高度和宽度；（3）对索引$\\sigma(i)$的预测值，定义类$c_i$的概率为$\\hat{p}_{\\sigma(i)}(c_i)$,预测边框为$\\hat{b}_{\\sigma(i)}$，定义：</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}_{match}(y_i,\\hat{y}_{\\sigma(i)})=-1_{\\{c_i\\neq\\phi\\}}\\hat{p}_{\\sigma(i)}(c_i)+1_{\\{c_i\\neq\\phi\\}}\\mathcal{L}_{box}(b_i,\\hat{b}_{\\sigma(i)})</script><script type=\"math/tex; mode=display\">\\mathcal{L}_{box}(b_i,\\hat{b}_{\\sigma(i)})=\\lambda_{iou}\\mathcal{L}_{iou}(b_i,\\hat{b}_{\\sigma(i)})+\\lambda_{L1}||b_i-\\hat{b}_{\\sigma(i)}||_1</script><p>这是真实框集合中的每一框和预测框集合中的每一个框匹配，损失值最小的预测框为该真实框的最佳匹配框，得到唯一匹配。作用类似于用于匹配的启发式分配规则（匹配推荐框、锚点）区别只是一一匹配。根据二分匹配损失计算匈牙利损失函数(类别的负对数似然函数损失和边框损失线性组合)，定义如下：</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}_{Hungarian}(y,\\hat{y})=\\sum\\limits_{i=1}^N\\left[-\\log\\,\\hat{p}_{\\hat{\\sigma}(i)}(c_i)+1_{\\{c_i\\neq\\phi\\}}\\mathcal{L}_{box}(b_i,\\hat{b}_{\\hat{\\sigma}(i)})\\right]</script><p>$\\hat{\\sigma}$是二分分配损失计算得到的最优预测值索引，注：$c_i=\\phi$的样本通常比目标类别多很多，属于不平衡样本集，所以对于无目标类别将对数几率的权重减少到$1/10$。别的目标检测器对边界框的预测是基于和初始猜测边框的偏差，这里是直接生成边框预测，$\\ell_1$损失的缺点就是有损失值的相对尺度问题，即大目标的大边框和小目标的小边框都可能产生相似的$\\ell_1$损失，因此无法通过该损失区分大小尺寸，需要结合具有尺度不变性的交并比损失，注两个损失值还需要在批中样本上做归一化处理。</p>\n<h3 id=\"DETR框架\"><a href=\"#DETR框架\" class=\"headerlink\" title=\"DETR框架\"></a>DETR框架</h3><p>架构三部分组成：1、CNN骨架（提取出致密特征表达），2、transformer编解码，3、feed forward network,FFN(生成最终的预测)。</p>\n<h4 id=\"CNN\"><a href=\"#CNN\" class=\"headerlink\" title=\"CNN\"></a>CNN</h4><p>初始图片$x_{img}\\in\\mathbb{R}^{3\\times H_0\\times W_0}$, CNN输出低分辨率的激活图$f\\in\\mathbb{R}^{C\\times H\\times W}$,$C=2048$,$H,W=\\frac{H_0}{32},\\frac{W_0}{32}$</p>\n<h4 id=\"transformer编码器\"><a href=\"#transformer编码器\" class=\"headerlink\" title=\"transformer编码器\"></a>transformer编码器</h4><ol>\n<li>用1x1卷积核减少高层激活图$f$的通道维度从$C$到$d$，新的特征图为$z_0\\in\\mathbb{R}^{d\\times H\\times W}$</li>\n<li>编码器需要输入特征序列，将$z_0$的空间维度塌缩到一个维度$z_0\\in\\mathbb{R}^{d\\times HW}$</li>\n<li>编码器包含：1、多头注意力模块，2、feed forward网络层，编码器具有输入序列的交换不变性，因此需要向序列中添加固定位置嵌入体</li>\n</ol>\n<h4 id=\"transformer解码器\"><a href=\"#transformer解码器\" class=\"headerlink\" title=\"transformer解码器\"></a>transformer解码器</h4><ul>\n<li>使用标准的多头自注意力和编解码注意力机制的transformer架构，但是区别是解码器每层并行解码N个目标，原来的模型是使用自动回归模型一次预测输出序列中的一个元素。</li>\n<li>由于解码器也是对输入序列具有交换不变性，因此也需要让N个输入嵌入是不同的以生成不同结果，这N个输入嵌入体就是位置编码也称作N个目标查询，将他们添加到每一个注意力层的输入部分。</li>\n<li>N个目标查询被解码器转换成输出嵌入体</li>\n</ul>\n<h4 id=\"FFN网络层\"><a href=\"#FFN网络层\" class=\"headerlink\" title=\"FFN网络层\"></a>FFN网络层</h4><ul>\n<li>具有ReLU激活函数的3层感知网络、线性投影层</li>\n<li>FFN将N个输出嵌入体独立分解成N个边框坐标和类别标签，即最终预测结果</li>\n<li>边框坐标是根据输入图像做归一化</li>\n<li>类别标签使用softmax函数给出</li>\n</ul>\n<h4 id=\"补充多头注意力层\"><a href=\"#补充多头注意力层\" class=\"headerlink\" title=\"补充多头注意力层\"></a>补充多头注意力层</h4><p><strong>多头注意力</strong><br>有M个注意力头，每个头的维度是d，多头注意力是一个函数函数的输入输出如下，注：$d^\\prime=\\frac{d}{M}$，在花括号中列出了矩阵/张量的大小</p>\n<script type=\"math/tex; mode=display\">\\operatorname{mh-attn}: \\underbrace{X_q}\\limits_{d\\times N_q},\\underbrace{X_{kv}}\\limits_{d\\times N_{kv}},\\underbrace{T}\\limits_{M\\times 3\\times d^\\prime\\times d},\\underbrace{L}\\limits_{d\\times d}\\;\\mapsto\\;\\underbrace{\\tilde{X}_q}\\limits_{d\\times N_q}</script><p>展开为每个单头计算结果在特征阶次上的连接</p>\n<script type=\"math/tex; mode=display\">\\operatorname{mh-attn}(X,X,T,L)=L\\left[attn(X_q,X_{kv},T_1);\\cdots;attn(X_q,X_{kv},T_M)\\right]</script><p>$X_q$是长度为$N_q$的查询序列，$X_{kv}$是长度为$N_{kv}$的键-值序列，$T$是权重张量用于计算所谓的查询、键值、得分嵌入体，$L$是投影矩阵。多头自注意力是$X_q=X_{kv}$的特殊情况</p>\n<script type=\"math/tex; mode=display\">\\operatorname{mh-s-attn}(X,T,L)=\\operatorname{mh-attn}(X,X,T,L)</script><p>一般在得到$\\tilde{X}_q$后还要在进行残差连接、dropout和层归一化处理</p>\n<script type=\"math/tex; mode=display\">\\hat{X}_q=layernorm(X_q+dropout(\\tilde{X}_q))</script><p><strong>单头注意力</strong><br>单头注意力的权重张量$T^\\prime\\in\\mathbb{R}^{3\\times d^\\prime\\times d}$，在计算单头注意力时还要引入位置编码嵌入体$P_q\\in\\mathbb{R}^{d\\times N_q}$和$P_{kv}\\in\\mathbb{R}^{d\\times N_{kv}}$。<br>首先计算添加位置编码后的查询、键值、得分嵌入体，计算复杂度$\\mathcal{O}(d^\\prime N_q\\cdot d +d^\\prime N_{kv} \\cdot d)$，$Q_i,K_i:\\mathcal{O}(d^\\prime d)$</p>\n<script type=\"math/tex; mode=display\">\\left[Q;K;V\\right]=\\left[T_1^\\prime(X_q+P_q);T_2^\\prime(X_{kv}+P_{kv});T_3^\\prime X_{kv}\\right]</script><p>$T^\\prime$就是$T_1^\\prime,T_2^\\prime,T_3^\\prime$在第一阶次上的连接</p>\n<p>再计算注意力权重$\\alpha$,通过query嵌入体和键值嵌入体点积运算和softmax运算<br>因此可以得到查询序列中的每个查询嵌入体和键值序列中的每个键值嵌入体之间的权重（相似性），计算复杂度$\\alpha_{i,j}:\\mathcal{O}(d^\\prime)$：</p>\n<script type=\"math/tex; mode=display\">\\alpha_{i,j}=\\frac{e^{\\frac{1}{\\sqrt{d^\\prime}}Q_i^TK_j}}{Z_i}\\quad\\quad Z_i=\\sum\\limits_{j=1}^{N_{kv}}e^{\\frac{1}{\\sqrt{d^\\prime}}Q_i^TK_j}</script><p>最后计算带注意力权重$\\alpha$的第i个嵌入体在特征维度上的加权得分分布,计算复杂度$\\mathcal{O}(N_{kv}\\cdot d^\\prime)$</p>\n<script type=\"math/tex; mode=display\">attn(X_{q_i},X_{kv},T^\\prime)=\\sum\\limits_{j=1}^{N_{kv}}\\alpha_{i,j}V_j</script><p>在查询序列阶次上连接,计算复杂度$\\mathcal{O}(N_qN_{kv}\\cdot d^\\prime)$</p>\n<script type=\"math/tex; mode=display\">attn(X_q,X_{kv},T^\\prime)=[attn(X_{q_1},X_{kv},T^\\prime);\\cdots;attn(X_{q_{N_q}},X_{kv},T^\\prime)]</script><p><img src=\"./image-20221203145853415.png\" alt=\"image-20221203145853415\"></p>\n","site":{"data":{}},"cover":"https://s1.ax1x.com/2022/11/04/xL0uct.png","excerpt":"","more":"<p>[TOC]</p>\n<h1 id=\"基于深度学习的视觉目标检测技术综述\"><a href=\"#基于深度学习的视觉目标检测技术综述\" class=\"headerlink\" title=\"基于深度学习的视觉目标检测技术综述\"></a>基于深度学习的视觉目标检测技术综述</h1><p>作者：曹家乐，2022</p>\n<h2 id=\"发展历程\"><a href=\"#发展历程\" class=\"headerlink\" title=\"发展历程\"></a>发展历程</h2><ol>\n<li>基于手工设计特征的方法<ol>\n<li>支持向量机</li>\n<li>AdaBoost</li>\n<li>Haar特征(Viola, 2004)</li>\n<li>方向梯度直方图histograms of oriented gradients(Dalal, 2005)</li>\n</ol>\n</li>\n<li>深度学习<ol>\n<li>区域卷积神经网络region-based convolutional neural network, R-CNN (Girshick, 2014)</li>\n<li>单次检测single shot detector, SSD (Liu, 2016)</li>\n<li>yolo (Redmon, 2016)</li>\n<li>detection transformer, DETR (Carion, 2020)</li>\n</ol>\n</li>\n</ol>\n<p>深度网络模型：</p>\n<ol>\n<li>AlexNet(Krizhevsky,2012)</li>\n<li>GoogLeNet(Szegedy,2015)</li>\n<li>VGGNet(Simonyan,2015)</li>\n<li>ResNet(He,2016)</li>\n<li>DenseNet(Huang,2017)</li>\n<li>Mobilenet(Howard,2018)</li>\n<li>ShuffleNet(Zhang,2018)</li>\n<li>SENet(Hu, 2018)</li>\n<li>EfficientNet(Tan,2019)</li>\n<li>ViT(Dosovitskiy,2021)</li>\n<li>Swin(Liu,2022)</li>\n</ol>\n<p>目标检测方法</p>\n<ol>\n<li>DetectorNet(Szegedy,2014)</li>\n<li>R-CNN(Girshick,2014)</li>\n<li>OvearFeat(Sermanet,2014)</li>\n<li>SPPNet(He,2015)</li>\n<li>Fast R-CNN(Girshick,2016; Ren,2016)</li>\n<li>YOLO(Redmon,2016)</li>\n<li>SSD(Liu,2016)</li>\n<li>R-FCN(Dai,2017)</li>\n<li>FPN(Lin,2017)</li>\n<li>YOLOv2(Redmon,2017)</li>\n<li>Mask RCNN(He,2018)</li>\n<li>DCN(Dai,2018)</li>\n<li>RetinaNet(Lin,2018)</li>\n<li>Cascade RCNN(Cai,2018)</li>\n<li>YOLOv3(Redmon,2019)</li>\n<li>CornerNet(Law,2019)</li>\n<li>FCOS(Tian,2020)</li>\n<li>CenterNet(Zhou,2020)</li>\n<li>EfficientDet(Tan,2020)</li>\n<li>ATSS(Zhang,2020)</li>\n<li>MoCo(He,2020)</li>\n<li>YOLOv4(Bochkovskiy,2021)</li>\n<li>Deformable DETR(Zhu,2021)</li>\n<li>DETR(Carion,2021)</li>\n<li>YOLOv5(Jocher,2021)</li>\n<li>UP-DETR(Dai,2021)</li>\n<li>Pix2seq(Chen,2022)</li>\n</ol>\n<h2 id=\"检测设备\"><a href=\"#检测设备\" class=\"headerlink\" title=\"检测设备\"></a>检测设备</h2><ol>\n<li>单目相机</li>\n<li>双目相机 （提供三维信息）</li>\n</ol>\n<h2 id=\"基于单目相机流程及其涵盖的方法\"><a href=\"#基于单目相机流程及其涵盖的方法\" class=\"headerlink\" title=\"基于单目相机流程及其涵盖的方法\"></a>基于单目相机流程及其涵盖的方法</h2><ol>\n<li><p>数据预处理</p>\n<ul>\n<li>翻转</li>\n<li>放缩</li>\n<li>均值归一化</li>\n<li>色调变化</li>\n<li>剪切、擦除、分区(DeVries, 2017; Zhong, 2020b; Singh, 2017; Chen, 2020a)</li>\n<li>混合(Mixup: Zhang, 2018; CutMix: Yun, 2019; Fang, 2019; Mosaic: Bochkovskiy, 2020; Montage: Zhou, 2020; dynamic scale training: Chen, 2020b)</li>\n</ul>\n</li>\n<li><p>检测网络</p>\n<ul>\n<li>基础骨架<ul>\n<li>AlexNet(Krizhevsky, 2012)</li>\n<li>VGGNet(Simonyan, 2014) </li>\n<li>ResNet(He, 2016)</li>\n<li>DenseNet(Huang, 2017)</li>\n<li>Transformer(Vaswani, 2017), ViT(Dosovitskiy, 2021; Beal, 2020), Swin(Liu, 2021c), PVT(Wang, 2021c)</li>\n</ul>\n</li>\n<li>特征融合<ul>\n<li>特征金字塔(Lin, 2017a) </li>\n</ul>\n</li>\n<li>预测网络(分类回归任务)<ul>\n<li>两阶段目标检测：全连接</li>\n<li>单阶段目标检测：全卷积</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>标签分配与损失计算</p>\n<ul>\n<li><p>标签分配准则</p>\n<ul>\n<li>交并比准则<ul>\n<li>基于锚点框与真实框的交并比</li>\n</ul>\n</li>\n<li>距离准则<ul>\n<li>基于无锚点框 ，点到物体中心的距离</li>\n</ul>\n</li>\n<li>似然估计准则<ul>\n<li>分类、回归</li>\n</ul>\n</li>\n<li>二分匹配准则<ul>\n<li>分类、回归</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>损失函数<ul>\n<li>交叉熵函数</li>\n<li>聚焦损失函数(Lin, 2017b)</li>\n<li>回归损失函数: L1损失函数、平滑L1损失函数、IoU损失函数、GIoU损失函数(Reztofighi, 2019)、CIoU损失函数(Zheng, 2020b)</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>后处理：为每个物体保留一个检测结果，去除冗余结果</p>\n<ul>\n<li>非极大值抑制NMS</li>\n<li>soft-NMS(Bodla, 2017)</li>\n<li>IoUNet(Jiang, 2018)</li>\n<li>定位方差(He, 2018)</li>\n<li>上下文推理(Pato, 2020)</li>\n</ul>\n</li>\n</ol>\n<h3 id=\"基于锚点框方法\"><a href=\"#基于锚点框方法\" class=\"headerlink\" title=\"基于锚点框方法\"></a>基于锚点框方法</h3><p>描述：为空间每个位置设定多个矩形框（框的尺度和长宽比），尽可能的涵盖图像中的物体<br>分类：</p>\n<ol>\n<li>两阶段目标检测<ol>\n<li>提取k个候选框</li>\n<li>对候选框分类和回归</li>\n</ol>\n</li>\n<li>单阶段目标检测<ol>\n<li>直接对锚点框分类和回归</li>\n</ol>\n</li>\n</ol>\n<h3 id=\"基于无锚点框方法\"><a href=\"#基于无锚点框方法\" class=\"headerlink\" title=\"基于无锚点框方法\"></a>基于无锚点框方法</h3><p>分类：</p>\n<ol>\n<li>基于关键点目标检测：多个关键点集成到物体上</li>\n<li>基于内部点目标检测：物体内部点到物体边界的上下左右偏移量</li>\n</ol>\n<h2 id=\"基于双目相机流程及其涵盖的方法\"><a href=\"#基于双目相机流程及其涵盖的方法\" class=\"headerlink\" title=\"基于双目相机流程及其涵盖的方法\"></a>基于双目相机流程及其涵盖的方法</h2><p>方法流程与单目相同</p>\n<h3 id=\"基于直接视锥空间\"><a href=\"#基于直接视锥空间\" class=\"headerlink\" title=\"基于直接视锥空间\"></a>基于直接视锥空间</h3><p>描述：直接使用基础骨干提取的两个单目特征构造双目特征。<br>方法：</p>\n<ol>\n<li>串接特征构造<br> 不改变原单目特征的坐标空间</li>\n<li>平面扫描构造<br> 通过逐视差平面或者深度平面地扫描一对2维特征，所得三维特征即是匹配代价体</li>\n</ol>\n<h3 id=\"基于显式逆投影空间\"><a href=\"#基于显式逆投影空间\" class=\"headerlink\" title=\"基于显式逆投影空间\"></a>基于显式逆投影空间</h3><p>描述：将存在尺度变化和遮挡问题的视锥空间图像逆投影到尺度均匀、不存在重叠遮挡的3维空间，从而缓解视锥投影产生的问题。<br>方法：</p>\n<ol>\n<li>基于原始图像视差的逆投影方法<br> 先利用双目视差估计算法预测每个像素的视差，将像素的视差逆投影到三维空间生成电云，最后利用点云的3维检测方法进行目标检测</li>\n<li>基于特征体的逆投影方法<br> 通过插值和采样将平面扫描得到的匹配代价体变换到3维空间，利用了图像特征提供的颜色和纹理信息。</li>\n<li>基于候选像素视差的逆投影方法<br> 仅聚焦感兴趣目标区域的三维空间，先利用实例分割方案得到目标的前景像素，然后生成仅含前景区域的3维空间。<script type=\"math/tex; mode=display\">逆投影策略\\left\\{\\begin{align*}\n     & 前景共享3维空间\\\\\n     & 每个实例生成相互独立的3维子空间\n \\end{align*}\\right.</script></li>\n</ol>\n<h2 id=\"发展趋势\"><a href=\"#发展趋势\" class=\"headerlink\" title=\"发展趋势\"></a>发展趋势</h2><ol>\n<li>高效的端到端目标检测transform，加快收敛，减少计算资源。</li>\n<li>基于自监督学习的目标检测，目标检测任务存在数量和尺度不确定的物体。</li>\n<li>长尾分布目标检测，现实世界物体类别数量庞大且不同类别的物体数量存在极度不平衡。</li>\n<li>小样本、0样本目标检测能力的提高</li>\n<li>大规模双目目标检测数据集少，需要标注物体的2维和3维信息以及相机标注视差和相机参数，还需完善评价体系和开放测试平台</li>\n<li>弱监督双目目标检测</li>\n</ol>\n<h1 id=\"YOLOv7-Trainable-bag-of-freebies-sets-new-state-of-the-art-for-real-time-object-detectors\"><a href=\"#YOLOv7-Trainable-bag-of-freebies-sets-new-state-of-the-art-for-real-time-object-detectors\" class=\"headerlink\" title=\"YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors\"></a>YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</h1><p>作者：Chien-Yao Wang，2022</p>\n<h2 id=\"解决的问题\"><a href=\"#解决的问题\" class=\"headerlink\" title=\"解决的问题\"></a>解决的问题</h2><p>发现的问题:</p>\n<ol>\n<li>模型再参数化</li>\n<li>用动态标签分配技术后，如何将动态标签分配给模型的不同输出层</li>\n</ol>\n<p>本篇文章解决的问题：</p>\n<ol>\n<li>最高推理精度56.8%AP和最快推理速度160FPS,都达到最高水平，参与对比的模型有：YOLOv5、YOLOX、Scaled-YOLOv4、YOLOR、PPYOLOE、DETR、Deformable DETR、DINO-5scale-R50、ViT-Adapter-B、SWIN-L Cascade-Mask R-CNN、ConvNeXt-XL Cascade-Mask R-CNN</li>\n<li>支持移动GPU以及边缘端和云端GPU</li>\n<li>设计trainable bag-of-freebies方法，既可增强训练代价提高检测准确度又不增加推理代价</li>\n<li>提出planned re-parameterized model</li>\n<li>提出新的标签分配方法，coarse-to-fine lead guided label assigment</li>\n<li>提出extend and compound scaling方法，减少40%的模型参数和50%计算时间</li>\n</ol>\n<p>其他模型的优点和不足：</p>\n<ol>\n<li>YOLOX和YOLOR只改进各种GPU推理速度</li>\n<li>基于MobileNet, ShuffleNet, GhostNet针对CPU设计</li>\n<li>ResNet, DarkNet, DLA, CSPNet针对GPU设计</li>\n<li>YOLO和FCOS具有：1、快而强壮的网络架构，2、高效的特征集成方法，3、鲁棒的损失函数，4、高效的标签分配方法，5、准确的检测方法，6、高效训练方法</li>\n</ol>\n<h2 id=\"当前的不足\"><a href=\"#当前的不足\" class=\"headerlink\" title=\"当前的不足\"></a>当前的不足</h2><h2 id=\"使用的方法\"><a href=\"#使用的方法\" class=\"headerlink\" title=\"使用的方法\"></a>使用的方法</h2><h3 id=\"模型再参数化\"><a href=\"#模型再参数化\" class=\"headerlink\" title=\"模型再参数化\"></a>模型再参数化</h3><p>模型再参数化：融合多个计算模块于一体，可是为组装技术</p>\n<script type=\"math/tex; mode=display\">分类\\left\\{\\begin{align*}\n  & 模块间组合\\\\\n  & 模型间组合\n\\end{align*}\\right.</script><ol>\n<li>模型间组合方法<ol>\n<li>在不同训练集中训练多个相同模型，然后再平均模型的参数</li>\n<li>在不同的迭代次数间进行模型参数均值化</li>\n</ol>\n</li>\n<li>模块间组合方法<br> 在训练期间将一个模块分解成多个分支模块，在推理时将多个分支模块整合成一个完整模块</li>\n</ol>\n<h3 id=\"模型缩放\"><a href=\"#模型缩放\" class=\"headerlink\" title=\"模型缩放\"></a>模型缩放</h3><p>模型放缩可以增大和缩小模型使它适合不同计算能力的设备，满足不同的推理速度。</p>\n<script type=\"math/tex; mode=display\">放缩因子\\left\\{\\begin{align*}\n  & 分辨率resolution（输出图像尺度）\\\\\n  & 深度depth（隐藏层层数）\\\\\n  & 宽度width（通道数） \\\\\n  & 阶段stage（特征金字塔层数）\n\\end{align*}\\right.</script><p>放缩方法：网络架构搜索Network architecture search(NAS)，折中了网络参数大小、计算时间、推理速度和精确性</p>\n<p>放缩因子的影响：</p>\n<ol>\n<li>对基于非连接的网络架构，在进行模型放缩时由于每个隐藏层的入度和出度不被改变因此可以独立分析每个放缩因子对模型参数数量和计算速度的影响</li>\n<li>对基于连接的网络架构，在进行模型隐藏层深度放大或缩小时紧跟在计算模块后的转移/转化模块的入度会减小或增大，不能独立分析单个尺度因子的影响必须一起分析</li>\n<li>文章提出compound scaling method合成尺度方法既可保持原有模型的的性质又可保持最优结构</li>\n</ol>\n<h3 id=\"架构\"><a href=\"#架构\" class=\"headerlink\" title=\"架构\"></a>架构</h3><p>选取架构主要考虑1、模型参数数量，2、计算量，3、计算密度<br>采用Extended-ELAN(E-ELAN)扩展高效层聚合网络架构，该架构使用扩展基数层、清洗基数层、合并基数层增强网络学习能力</p>\n<p>基于早期版本的YOLO框架和YOLOR框架作为基本框架</p>\n<h3 id=\"可训练赠品袋trainable-bag-of-freebies\"><a href=\"#可训练赠品袋trainable-bag-of-freebies\" class=\"headerlink\" title=\"可训练赠品袋trainable bag-of-freebies\"></a>可训练赠品袋trainable bag-of-freebies</h3><ul>\n<li><p>计划再参数化卷积</p>\n<ol>\n<li>如何将再参数化卷积和不同的网络结合？</li>\n<li>提出planned re-parameterized convolution</li>\n<li>提出无identity connection的RePConv构造planned re-parameterized convolution</li>\n<li>用RepConvN网络层替换3堆叠ELAN架构中不同位置处的3x3卷积层</li>\n</ol>\n</li>\n<li><p>以粗为辅以精为主的损失值</p>\n<ol>\n<li>深度监督是在网络的中间层添加额外的辅助头，将带有损失值信息的浅层网络权重作为引导方式</li>\n<li>将负责最后输出的头称为主头，将用于协助训练的头称为辅头</li>\n<li>采用软标签即使用网络预测输出的性质和分布和考虑实际标签，使用一些计算和优化方式生成可靠的标签</li>\n<li>如何分配软标签到主头和辅头？<ol>\n<li>分别计算主头和辅头预测结果，使用各自的分配器结合实际结果制作各自标签，再通过各自标签和头计算损失</li>\n<li>文章提出经分配器用主头和实际结果制作由粗到精的等级标签，再将这些等级标签用在主头和辅头上计算损失值</li>\n</ol>\n</li>\n</ol>\n</li>\n<li><p>批归一化放到conv-bn-activation 拓扑结构中</p>\n</li>\n<li><p>YOLOR的隐性知识以串行和并行方式结合到卷积特征图中</p>\n</li>\n<li><p>EMA模型</p>\n</li>\n</ul>\n<h1 id=\"AN-IMAGE-IS-WORTH-16x16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE\"><a href=\"#AN-IMAGE-IS-WORTH-16x16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE\" class=\"headerlink\" title=\"AN IMAGE IS WORTH 16x16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\"></a>AN IMAGE IS WORTH 16x16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</h1><p>作者：Alexey Dosovitskiy, 2021</p>\n<h2 id=\"解决的问题-1\"><a href=\"#解决的问题-1\" class=\"headerlink\" title=\"解决的问题\"></a>解决的问题</h2><p>根据Transformer计算的效率和可扩展性以及借鉴Transformer在自然语言方面取得的成功将其应用于图像上</p>\n<ol>\n<li>证明在大样本上14M-300M图，Transformer胜过CNN</li>\n<li>可以处理中等分辨率图像</li>\n<li>在更大规模数据集而非ImageNet数据集，探索图像识别</li>\n</ol>\n<p>前人工作优点和不足：</p>\n<ol>\n<li>把CNN框架和自注意力结合(Wang,2018; Carion, 2020)</li>\n<li>用自注意力替换整个卷积网络(Ramachandran, 2019; Wang, 2020a)</li>\n<li>ResNet架构在大尺度图片识别上是效果好的(Mahajan, 2018; Xie, 2020; Kolesnikov, 2020)</li>\n<li>transformer用于机器翻译(Vaswani, 2017)</li>\n<li>将transformer用到图像处理环境中<ul>\n<li>只将自注意力应用于代查询像素的局部临域中，并非全局应用(Parmar,2018)</li>\n<li>局部多头点积自注意力块完全替换卷积(Hu,2019; Ramachandran, 2019; Zhao, 2020)</li>\n<li>稀疏Transformer在全局自注意力中使用放缩近似以适应图片(Child, 2019)</li>\n<li><strong>(Cordonnier, 2020)提出的模型也是ViT但是没有证明大规模预训练模型可以甚至超过CNN模型，使用的2x2块太小只能适应小分辨率图像</strong></li>\n</ul>\n</li>\n<li>(Sun, 2017)研究CNN性能如何随数据集大小变化</li>\n<li>(Kolesnikov,2020; Djolonga,2020)从大规模的数据集上探索CNN的迁移学习</li>\n</ol>\n<h2 id=\"当前的不足-1\"><a href=\"#当前的不足-1\" class=\"headerlink\" title=\"当前的不足\"></a>当前的不足</h2><ol>\n<li>Transformer和CNN相比缺少偏移量无法实现平移等变映射和无法进行局部化，因此在小样本中泛化能力弱</li>\n<li><strong>应用ViT到其它计算机视觉任务，例如目标检测和分割</strong></li>\n<li>持续开发自监督与训练方法</li>\n</ol>\n<h2 id=\"使用的方法-1\"><a href=\"#使用的方法-1\" class=\"headerlink\" title=\"使用的方法\"></a>使用的方法</h2><ul>\n<li>分割图片成若干块，给这些块提供顺序线性嵌入体，并将嵌入体作为Transformer的输入</li>\n<li>选用原始Transformer(Vaswani, 2017)</li>\n<li>框架<br>  <img src=\"./image-20221125100901770.png\" alt=\"image-20221125100901770\"><br>  标准Transformer接收1维符号嵌入序列，将图像$x\\in\\mathbb{R}^{H\\times W\\times C}$分割成有序排列小块$x_p\\in\\mathbb{R}^{N\\times(P^2\\cdot C)}$，输入$z_0=[x_{class}; x_p^1E; x_p^2E; \\cdots; x_p^NE]+E_{pos}$, $x_{class}\\in\\mathbb{R}^{1\\times D}$, $E\\in \\mathbb{R}^{(P^2\\cdot C)\\times D}$, $E_{pos}\\in\\mathbb{R}^{(N+1)\\times D}$</li>\n</ul>\n<h1 id=\"Pyramid-Vision-Transformer-A-Versatile-Backbone-for-Dense-Prediction-without-Convolutions\"><a href=\"#Pyramid-Vision-Transformer-A-Versatile-Backbone-for-Dense-Prediction-without-Convolutions\" class=\"headerlink\" title=\"Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions\"></a>Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions</h1><p>作者：Wenhai Wang, 2021</p>\n<h2 id=\"解决的问题-2\"><a href=\"#解决的问题-2\" class=\"headerlink\" title=\"解决的问题\"></a>解决的问题</h2><ol>\n<li>文章(PVT)解决将transform移植到密集预测任务的问题</li>\n<li>产生高输出分辨率，利用收缩的金字塔减少计算量</li>\n<li>PVT继承了CNN和Transformer的优点，成为应对各种视觉任务的无需卷积的统一骨干架构，可以直接替换CNN骨干架构</li>\n<li>提高下游任务的性能，包括：目标检测，实例分割，语意分割</li>\n<li>克服传统transformer问题方式<ol>\n<li>采用获取精细图像块（每个图像块为4x4像素）作为输入，以学习高分辨率的表示</li>\n<li>采用逐渐缩小的金字塔形式减小transformer在深层网络中的序列长度，以减小计算量</li>\n<li>采用空间减小注意力层spatial-reduction attention(SRA), 这近一步在学习高分辨率特征时减小资源损耗</li>\n</ol>\n</li>\n<li>具有的优点<ol>\n<li>CNN的局部接收视野随网络深度的增加而增加，PVT产生的是全局接收视野，这有利于检测和分割任务</li>\n<li>借助金字塔结构的优势，易于将PVT插入到许多代表密集预测的管道中，例如RetinaNet和Mask R-CNN</li>\n<li>通过结合PVT和其它特殊任务的Transformer解码器可以构建无卷积的管道，例如PVT+DETR用作目标检测</li>\n</ol>\n</li>\n</ol>\n<p>前人的工作及优缺点</p>\n<ol>\n<li><p>Vision Transformer(ViT)被用作分类任务</p>\n</li>\n<li><p>Vision Transformer(ViT)是一种柱状机构具有粗糙的输入图像块，不是很适合像素级别的致密预测任务如目标检测和分割任务，原因有：1、输出的特征图是单一尺度，分辨率低；2、高计算和内存占用成本, </p>\n</li>\n<li><p>CNN在视觉上取得巨大成功(Karen,2015; Kaiming,2017; Saining,2017)</p>\n</li>\n<li><p>将视觉任务建模成具有可学习查询功能的字典查找问题，使用Transformer解码器作为特殊任务的头应用于卷积框架的顶层(Nicolas,2020; Christian,2017; Enze,2021)</p>\n</li>\n<li><p>网络架构</p>\n<ol>\n<li><p>(Yann, 1998)首次引入CNN分辨手写数字，在整个图像空间共享卷积核参数实现平移等变性</p>\n</li>\n<li><p>(Alex, 2012; Karen, 2015)在大尺度图片分类数据集中使用堆叠的卷积块</p>\n</li>\n<li><p>GoogLeNet(Christain,2015)包含了多核路径</p>\n</li>\n<li><p>多路径卷积模块的效率在Inception系列网络(Christian,2016;)、ResNeXt(Saining, 2017)、 DPN(Yunpeng, 2017)、MixNet(Wenhai, 2018)、SKNet(Xiang,2019)网络中被验证</p>\n</li>\n<li><p>(Kaiming,2016)在ResNet网络中引入跳跃式连接卷积模块，这有助于训练更深的网络</p>\n</li>\n<li><p>(Gao,2017)在DenseNet中引入密集连接拓扑结构，把每个卷积模块同它前面的所有卷积模块相连</p>\n</li>\n</ol>\n</li>\n<li><p>密集预测任务</p>\n<ol>\n<li>目的：在特征图上进行像素级别的分类和回归</li>\n<li><script type=\"math/tex; mode=display\">分类\\left\\{\\begin{align*}\n& 目标检测\\left\\{\\begin{array}{l}\n           单阶段\\left\\{\\begin{array}{l}\n                          SSD(Wei,2016) \\\\\n                          RetinaNet(Tsung-Yi\\; Lin,2017) \\\\\n                          FCOS(Zhi\\; Tian,2019) \\\\\n                          GFL(Xiang,2020) \\\\\n                          PolarMask(Enze\\;Xie,2021) \\\\\n                          OneNet(Peize\\; Sun,2020) \\\\\n                       \\end{array}\\right. \\\\\n           多阶段\\left\\{\\begin{array}{l}\n                          Faster R-CNN(Shaoqing\\; Ren,2015) \\\\\n                          Mask R-CNN(Kaiming\\; He,2017) \\\\\n                          Cascade R-CNN(Zhaowei\\; Cai,2018) \\\\\n                          Sparse R-CNN(Peize\\; Sun, 2021) \\\\\n                       \\end{array}\\right. \\\\\n           结合CNN和Transformer\\; decoder\\left\\{\\begin{array}{l}\n                           DETR(Nicolas\\; Carion, 2020)\\\\\n                           deformable DETR(Xizhou\\; Zhu,2021)\\\\\n                        \\end{array}\\right.\n        \\end{array}\\right.\\\\\n& 语意分割 \\left\\{\\begin{array}{l}\n               FCN(Jonathan\\; Long,2015)\\\\\n               deconvolution\\; operation(Hyeonwoo\\; Noh,2015)\\\\\n               U-Net(Olaf\\; Ronneberger,2015)\\\\\n               添加金字塔池化(HengShuang\\; Zhao,2017)\\\\\n               添加FPN头(Alexander\\; Kirillov,2019)\\\\\n               DeepLab(Liang-Chieh\\; Chen,2017)\\\\\n            \\end{array}\\right.\\\\\n& 实例分割\n\\end{align*}\\right.</script></li>\n<li><p>自注意力和变换器</p>\n<ol>\n<li>卷积滤波器权重经过训练后被固定无法动态适应不同的输入，(Xu Jia,2016)使用动态滤波器，(Ashish Vaswani,2017)使用自注意力<ol>\n<li>非局部模块被(Xiaolong Wang,2018)引入解决时间和空间在大尺度上的依赖性，但是代价是计算成本和内存占用成本</li>\n<li>(Zilong Huang,2019)引入十字交叉路径Criss-cross减小注意力机制的复杂度</li>\n<li></li>\n</ol>\n</li>\n</ol>\n</li>\n</ol>\n</li>\n</ol>\n<h2 id=\"当前的不足-2\"><a href=\"#当前的不足-2\" class=\"headerlink\" title=\"当前的不足\"></a>当前的不足</h2><ol>\n<li>有许多特殊模块和运算方法是专门为CNN设计的因此没有在PVT中引入，例如：Squeeze-and-excitation network(SE)模块, Selective kernel network(SK)模块, 膨胀卷积模块, 模型精简模块，Network architecture search(NAS)模块</li>\n<li>基于Transformer 的模型在视觉应用上起步晚可以应用于OCR, 3D和医疗图像分析</li>\n</ol>\n<h2 id=\"使用的方法-2\"><a href=\"#使用的方法-2\" class=\"headerlink\" title=\"使用的方法\"></a>使用的方法</h2><h3 id=\"框架\"><a href=\"#框架\" class=\"headerlink\" title=\"框架\"></a>框架</h3><p>整体框架分成4个阶段，每个阶段都产生不同尺度的特征图，每个阶段都具有相似的结构，包括：1、分块嵌入层，2、若干Transformer编码层</p>\n<ul>\n<li><p>结构（以第一阶段为例）</p>\n<ol>\n<li><p>第一阶段输入图像HxWx3, 并分割成$\\frac{HW}{4^2}$多个图块，每块大小4x4x3</p>\n</li>\n<li><p>将分割块展平进行线性投影变换得到嵌入块形状为$\\frac{HW}{4^2}\\times C_1$</p>\n</li>\n<li><p>将嵌入块和位置嵌入一起传入具有$L_1$层的Transformer编码器中</p>\n</li>\n<li><p>输出特征层$F_1$形状是$\\frac{H}{4}\\times\\frac{W}{4}\\times C_1$</p>\n</li>\n<li><p>以此类推，以上一阶段输出作为下一阶段的输入，选取的块的大小相对于原始图像分别是8x8，16x16，32x32像素，即第i阶段的块大小为$P_i$，得到的特征图为$F_i$:$\\{F_2$,$F_3$,$F_4\\}$,特征图尺寸为$\\frac{H}{8}\\times\\frac{W}{8}\\times C_2$,$\\frac{H}{16}\\times\\frac{W}{16}\\times C_3$,$\\frac{H}{32}\\times\\frac{W}{32}\\times C_4$,</p>\n<p><img src=\"./image-20221127151329769.png\" alt=\"image-20221127151329769\"></p>\n</li>\n</ol>\n</li>\n<li><p>transformer的特征金字塔</p>\n<ul>\n<li>CNN的特征金字塔是使用不同的卷积跨步来实现，PVT是使用逐步缩小策略实现</li>\n</ul>\n</li>\n<li><p>transformer编码器</p>\n<ol>\n<li>Transformer编码器在第i阶段有$L_i$个编码层，每一个编码层又包含：1、注意力层，2、feed-forward层</li>\n<li><p>使用spatial-reduction注意力层(SRA)替换传统多头注意力层(MHA)，为了处理高分辨率特征图(4跨步特征图)<br> <img src=\"./image-20221127153059574.png\" alt=\"image-20221127153059574\"><br> SRA特点：减小Key和Value输入的尺寸：</p>\n<script type=\"math/tex; mode=display\">\\begin{align*}\n         SRA(Q,K,V)&=Concat(head_0,\\cdots,head_{N_i})W^O\\\\\n         head_j&=Attention(QW_j^Q,SR(K)W_j^K,SR(V)W_j^V)\\\\\n         SR(x)&=Norm(Reshape(x,R_i)W^S)\\\\\n         Attention(\\vec{q},\\vec{k},\\vec{v})&=Softmax(\\frac{\\vec{q}\\vec{k}^T}{\\sqrt{\\vec{d}_{head}}})\\vec{v}\\\\\n \\end{align*}</script><p> 符号说明：$Concat(\\cdot)$链接操作；$W_j^Q\\in\\mathbb{R}^{C_i\\times d_{head}}$,$W_j^K\\in\\mathbb{R}^{C_i\\times d_{head}}$,$W_j^V\\in\\mathbb{R}^{C_i\\times d_{head}}$,$W^O\\in\\mathbb{R}^{C_i\\times C_i}$,$W^S\\in\\mathbb{R}^{(R_i^2C_i)\\times C_i}$都是线性投影矩阵；$N_i$注意力的头数；$d_{head}=\\frac{C_i}{N_i}$每个头的大小；$SR(\\cdot)$减小空间尺度操作；$x\\in\\mathbb{R}^{(H_iW_i)\\times C_i}$表示输入序列；$R_i$表示缩减比例；$Reshape(x,R_i)=\\frac{H_iW_i}{R_i^2}\\times(R_i^2C_i)$修改张量形状;$Norm(\\cdot)$层归一化；$Attention(\\cdot,\\cdot,\\cdot)$注意力得分</p>\n</li>\n</ol>\n</li>\n</ul>\n<h1 id=\"Deformable-DETR-Deformable-Transformers-For-End-to-End-Object-Detection\"><a href=\"#Deformable-DETR-Deformable-Transformers-For-End-to-End-Object-Detection\" class=\"headerlink\" title=\"Deformable DETR: Deformable Transformers For End-to-End Object Detection\"></a>Deformable DETR: Deformable Transformers For End-to-End Object Detection</h1><p>作者：Xinzhou Zhu,2020</p>\n<h2 id=\"解决的问题-3\"><a href=\"#解决的问题-3\" class=\"headerlink\" title=\"解决的问题\"></a>解决的问题</h2><ol>\n<li>改善空间分辨率问题和收敛慢问题</li>\n<li>Deformable DETR在检测小目标上优于DETR</li>\n<li>结合了deformable convolution的稀疏空间采样和transformer的相关性建模能力</li>\n</ol>\n<p>前人的工作以及优缺点：</p>\n<ol>\n<li>DETR(Nicolas Carion,2020)用于剔除目标检测中辅助成分的需求（例如，非极大值抑制），缺点是收敛慢、有限的特征空间分辨率，该模型结合了CNN模型和Transformer编解码模型，小目标效果差，优点就是具有元素间的相关性</li>\n<li>目标检测使用了一些辅助成分(Li Liu,2020),例如锚点生成，基于规则的训练目标分配，非极大抑制</li>\n<li>deformable convolution(Jifeng Dai,2017)用于稀疏空间定位，因此高效、收敛快，缺点就是缺少元素间的相关性</li>\n<li>(Ashish Vaswani,2017)在transformer中引入自注意力和交叉注意力机制,缺点是时间成本和内存占用高</li>\n<li>解决时间和内存的方式有三种<ol>\n<li>对关键点使用预定义(冻结参数)的稀疏注意力模式<ul>\n<li>将注意力模式限制在固定局部窗口中使用(Peter J Liu,2018a;Niki Parmar,2018;Rewon Child,2019;Zilong Huang;2019),优点是减少复杂度，缺点是丢失全局信息</li>\n<li>以固定间隔方式设置关键点(Jonathan Ho,2019)，优点：增加接受视野</li>\n<li>允许少许特殊符号可以访问所有关键点(Iz Beltagy,2020),增加全局信息</li>\n</ul>\n</li>\n<li>学习依赖数据的稀疏注意力<ul>\n<li>基于注意力机制的局部敏感哈希映射LSH(Nikita Kitaev,2020),将查询元素和关键字元素映射到不同的区域</li>\n<li>(Aurko Roy,2020)用k-means聚类找到最相关的关键字元素</li>\n<li>(Yi Tay,2020a)对逐块的稀疏注意力学习块交换</li>\n</ul>\n</li>\n<li>研究自注意力机制的低级别性质<ul>\n<li>通过线性投影减少关键点数量(Sinong Wang,2020)</li>\n<li>通过核函数近似计算自注意力(Krzysztof Choromanski,2020)</li>\n</ul>\n</li>\n</ol>\n</li>\n<li>多尺度特征表示<ul>\n<li>FPN(Tsung-Yi Lin,2017a),自上而下生成多尺度特征图</li>\n<li>PANet(Shu Liu,2018b)，自下而上生成多尺度特征图</li>\n<li>从全局自注意力中提取所有尺度上的特征(Tao Kong,2018)</li>\n<li>U-shape模块融合多尺度特征(Qijie Zhao,2019)</li>\n<li>NAS-FPN(Golnaz Ghiasi,2019)、Auto-FPN(Hang Xu,2019)通过神经架构搜索自动进行交叉尺度连接</li>\n<li>BiFPN(Mingxing Tan,2020)</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"当前的不足-3\"><a href=\"#当前的不足-3\" class=\"headerlink\" title=\"当前的不足\"></a>当前的不足</h2><ol>\n<li>简单且高效的可迭代的边界框调优机制进一步改善性能</li>\n<li>可将Deformable DETR应用到2阶段目标识别中，先生成推荐区域，再将推荐区域作为目标query送入解码器</li>\n</ol>\n<h2 id=\"使用的方法-3\"><a href=\"#使用的方法-3\" class=\"headerlink\" title=\"使用的方法\"></a>使用的方法</h2><ol>\n<li>应用在若干的采样位置点处，这些点作为特征图中关键特征点</li>\n<li>使用图像尺度放缩而不是特征金字塔应用于deformable注意力模型</li>\n</ol>\n<h3 id=\"经典多头注意力结构\"><a href=\"#经典多头注意力结构\" class=\"headerlink\" title=\"经典多头注意力结构\"></a>经典多头注意力结构</h3><p>query元素代表了要输出的句子的目标单词，key元素代表输入句子中的单词，多头注意力模块根据测量的query-key对的相似性权重因子汇聚这些key。用$q\\in\\Omega_q$索引具有表达特征$z_q\\in\\mathbb{R}^C的$query元素；用$k\\in\\Omega_k$索引具有表达特征$x_k\\in\\mathbb{R}^C的$key元素,$C$是特征维度，$\\Omega_q$和$\\Omega_k$给出了query和key的元素总数;多头注意力特征计算：</p>\n<script type=\"math/tex; mode=display\">MultiHeadAttn(z_q,x)=\\sum\\limits_{m=1}^MW_m[\\sum\\limits_{k\\in\\Omega_k}A_{mqk}\\cdot W_m^\\prime x_k]</script><p>这里$m$索引各个注意力头总共有M个注意力头，$W_m^\\prime\\in\\mathbb{C_v\\times C}$和$W_m\\in\\mathbb{C\\times C_v}$是第m头待学习的矩阵，$C_v=C/M$,注意力权重$A_{mqk}\\propto \\exp\\{\\frac{z_q^TU_m^TV_mx_k}{\\sqrt{C_v}}\\}$满足归一化$\\sum\\limits_{k\\in\\Omega_k}A_{mqk}=1$, 这里$U_m\\in\\mathbb{R}^{C_v\\times C}$和$V_m\\in\\mathbb{R}^{C_v\\times C}$同样是待学习的矩阵。为了消除不同空间位置的奇异性，表达的查询和关键字特征$z_q$和$x_k$需要和位置嵌入体做结合。</p>\n<h3 id=\"DETR-Transformer编解码架构\"><a href=\"#DETR-Transformer编解码架构\" class=\"headerlink\" title=\"DETR Transformer编解码架构\"></a>DETR Transformer编解码架构</h3><p>采用Hungarian(匈牙利)损失函数借助二分(双边)匹配实现对每一个真实边框都有唯一预测值。用$y$表示ground truth集合，$\\hat{y}=\\{\\hat{y}_i\\}_{i=1}^N$表示有N个预测值的集合，当$N$远大于图像中物体个数时，可以认为$y$集合也是由N个真实结果构成没有目标的结果被符号$\\phi$填充。搜索N个元素$\\sigma\\in\\mathfrak{G}_N$的一个置换$\\hat{\\sigma}=\\mathop{\\arg\\min}\\limits_{\\sigma\\in\\mathfrak{G}_N}\\sum\\limits_i^N\\mathcal{L}_{match}(y_i,\\hat{y}_{\\sigma(i)})$, （1）$\\mathcal{L}_{match}(y_i,\\hat{y}_{\\sigma(i)})$是真值$y_i$和具有索引$\\sigma(i)$的预测值之间的逐对匹配代价函数；（2）第i个真值元素可以看成$y_i=(c_i,b_i)$,$c_i$是目标类别标签，符号$\\phi$用N/A表示，$b_i=\\{b_x,b_y,b_w,b_h\\}\\in[0,1]^4$是向量记录了中心坐标和相对图像大小的高度和宽度；（3）对索引$\\sigma(i)$的预测值，定义类$c_i$的概率为$\\hat{p}_{\\sigma(i)}(c_i)$,预测边框为$\\hat{b}_{\\sigma(i)}$，定义：</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}_{match}(y_i,\\hat{y}_{\\sigma(i)})=-1_{\\{c_i\\neq\\phi\\}}\\hat{p}_{\\sigma(i)}(c_i)+1_{\\{c_i\\neq\\phi\\}}\\mathcal{L}_{box}(b_i,\\hat{b}_{\\sigma(i)})</script><script type=\"math/tex; mode=display\">\\mathcal{L}_{box}(b_i,\\hat{b}_{\\sigma(i)})=\\lambda_{iou}\\mathcal{L}_{iou}(b_i,\\hat{b}_{\\sigma(i)})+\\lambda_{L1}||b_i-\\hat{b}_{\\sigma(i)}||_1</script><p>这是真实框集合中的每一框和预测框集合中的每一个框匹配，损失值最小的预测框为该真实框的最佳匹配框，得到唯一匹配。匈牙利损失函数定义如下：</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}_{Hungarian}(y,\\hat{y})=\\sum\\limits_{i=1}^N\\left[-1_{\\{c_i\\neq\\phi\\}}\\hat{p}_{\\hat{\\sigma}(i)}(c_i)+1_{\\{c_i\\neq\\phi\\}}\\mathcal{L}_{box}(b_i,\\hat{b}_{\\hat{\\sigma}(i)})\\right]</script><p>DETR主要使用Transformer编码解码架构将特征图像映射成一组待查目标对象的特征，feed-forward网络FFN用做回归分支预测边框坐标，线性投影用作分类分支预测目标类别。编码部分输入ResNet的生成的特征图，取query和key元素，它们都是特征图的每一个像素；解码部分输入包含来自编码部分的特征图和N个目标查询，有两个注意力模型：1、cross-attention,2、self-attention。在cross-attention中查询元素是从特征图中提取特征的N个目标查询，关键字元素来自于编码输出的特征图；在self-attention中，查询元素要获取他们的关系因此查询元素和关键字元素都来自于N个目标查询</p>\n<h3 id=\"Deformable-transformer\"><a href=\"#Deformable-transformer\" class=\"headerlink\" title=\"Deformable transformer\"></a>Deformable transformer</h3><ol>\n<li><p>单尺度Deformable attention<br> 通过对每个查询元素只分配少许固定数量的关键字元素，这些关键字元素取自一个参考点附近的点，无需对所有关键字进行匹配，所以无视特征图尺度，已知特征图$x\\in\\mathbb{R}^{C\\times H\\times W}$,$q$是查询元素特征$z_q$索引以及分配的一个2维参考点$p_q$索引</p>\n<script type=\"math/tex; mode=display\">DeformAttn(z_q,p_q,x)=\\sum\\limits_{m=1}^MW_m\\left[\\sum\\limits_{k=1}^KA_{mqk}\\cdot W_m^\\prime x(p_q+\\Delta p_{mqk})\\right]</script><p> 这里m是注意力头的索引，k是采样到的关键字元素索引，K是采样关键字总数，显然$K\\ll HW$。$\\Delta p_{mqk}$和$A_{mqk}$表示对第m注意力头、第k个关键字元素采样点的采样邻域半径和注意力权重，注：标量注意力权重$A_{mqk}\\in [0,1]$要归一化处理, $\\Delta p_{mqk}\\in \\mathbb{R}^2$没有约束范围，当$p_q+\\Delta p_{mqk}$是小数时双线性插值可以使用，$\\Delta p_{mqk}$和$A_{mqk}$是通过对查询元素特征$z_q$线性投影计算得到</p>\n</li>\n<li><p>多尺度Deformable attention<br> 设$\\{x^l\\}_{l=1}^L$是用于输入的多尺度特征图，$x^l\\in\\mathbb{R}^{C\\times H_l\\times W_l}$, $\\hat{p}_q\\in[0,1]^2$是每个查询元素q对应参考点的归一化二维坐标，坐标的归一化操作是对每一个尺度特征图进行, 模型公式为</p>\n<script type=\"math/tex; mode=display\">MSDeformAttn(z_q,\\hat{p}_q,\\{x^l\\}_{l=1}^L)=\\sum\\limits_{m=1}^MW_m\\left[\\sum\\limits_{l=1}^L\\sum\\limits_{k=1}^KA_{mlqk}\\cdot W_m^\\prime x^l(\\phi_l(\\hat{p}_q)+\\Delta p_{mlqk})\\right]</script><p> m是注意力头索引，l是输入特征图的尺度等级索引，k是采样点的索引，$\\Delta p_{mlqk}$和$A_{mlqk}$表示在l层尺度上的特征图、对第m注意力头、第k个关键字元素采样点的采样邻域半径和注意力权重，$\\phi_l(\\hat{p}_q)$将归一化的坐标缩放回尺度为l的特征图中的坐标</p>\n</li>\n<li><p>编码器<br> 编码器的输入输出都是具有相同分辨率的多尺度特征图，编码器的多尺度特征图$\\{x^l\\}_{l=1}^{L-1},L=4$取自ResNet的输出特征图$C_3$到$C5$分辨率分别为$H/2^3$到$H/2^5$,而最低分辨率的特征图$x^4$取自$C_5$特征图进行3x3步长2的卷积后得到的特征图$C_6$,所有的输入特征图的通道数都是256：<br> <img src=\"./image-20221130151059959.png\" alt=\"image-20221130151059959\"><br> query和key元素都是多尺度特征图的像素，每个query像素的参考点就是其自身，为了识别每个query像素在哪个尺度图上，除了添加位置嵌入体外，需要添加尺度嵌入体$e_l$到特征表达中，区别是位置嵌入体是固定编码，尺度嵌入体$\\{e_l\\}_{l=1}^L$需要连同网络一起训练获取。</p>\n</li>\n<li><p>解码器<br> 包含cross-attention和self-attention,query元素在两类注意力机制中都是目标query，目标query在cross-attention中取自特征图，而key元素是编码器的输出特征图；在self-attention中，key元素是目标query。每个目标query参考点的二维归一化坐标需要经过线性投影和激活函数从目标query的嵌入体中给出<br> <img src=\"./image-20221130160631440.png\" alt=\"image-20221130160631440\"><br> 模型提取的图像特征都是在参考点周围的点，所以我们预测的边框都是相对于参考点的偏移量，参考点初始值都是边框的中心点</p>\n</li>\n</ol>\n<h1 id=\"End-to-End-Object-Detection-with-Transformers\"><a href=\"#End-to-End-Object-Detection-with-Transformers\" class=\"headerlink\" title=\"End-to-End Object Detection with Transformers\"></a>End-to-End Object Detection with Transformers</h1><p>作者：Nicolas Carion,2020</p>\n<h2 id=\"解决的问题-4\"><a href=\"#解决的问题-4\" class=\"headerlink\" title=\"解决的问题\"></a>解决的问题</h2><ol>\n<li>提出目标检测新方法DEtection TRansformer(DETR)：将目标检测看作单向集合预测问题</li>\n<li>不再使用辅助设计成分，例如非极大值抑制抑制，锚点，是完全端到端的理念，流水化检测过程直接预测具有相对输入图像的绝对边框的检测集合</li>\n<li>结合了二分匹配损失和具有并行解码(非自动回归)的transformer，损失函数对预测目标交换具有不变性，因此可以并行执行</li>\n<li>对大目标具有更好的性能，可能是由于transformer的非局部计算</li>\n</ol>\n<p>前人的工作及其优缺点：</p>\n<ol>\n<li>以间接方式解决预测边框和种类任务，通过对大量推荐边框(Ren,S.,2015; Cai,Z.,2019)、锚点(Lin,T.Y.,2017)和窗口中心(Zhou,X.,2019;Tian,Z.,2019)定义回归和分类问题,通过后处理步骤削减近邻重复预测</li>\n<li>(Stewart,R.J.,2015;Hosang,J.H.,2017;Bodla,N.,2017,Rezatofighi,S.H.,2018)要么添加了其他形式的先验内容，要么在有挑战的基准样本上没有证明有强大竞争力</li>\n<li>(Stewart,R.J.,2015;Romera-Paraedes,B.,2015;Park,E.,2015)关注具有RNN自动回归解码</li>\n<li>集合预测<br> 基础的集合预测任务是多标签分类(Rezatofighi,S.H.,2017;Pineda,L.,2019)而目标检测要考虑元素间暗含的结构即识别近邻边框，因此不能用one-vs-rest方式处理问题。使用后处理方式例如非极大抑制处理近邻重叠问题；利用全局的对所有预测结果建立相互影响的推理方案规避这种冗余问题。集合预测可以使用致密的全连接网络(Erhan,D.,2014)、自动回归序列模型(recurrent neural network:Vinyals,O.,2016)。损失函数可以基于Hungarian算法(Kuhn,H.W.,1955)保证交换不变性和每个目标元素都有唯一匹配</li>\n<li>transformer和并行解码<br> Transformer是一个基于注意力机制的搭建模块用于机器翻译(Vaswani,A.,2017)；注意力机制是一个神经网络层，可以从整个输入序列中汇集信息(Bahdanau,D.,2015)；Transformer用在自动回归模型，根据之前的句子到句子模型(Sutskever,I.,2014)生成一个接一个的输出符号,这种方法缺点是推理成本(正比于输出长度)和很难批处理，所以需要开发并行序列生成，可以应用在语音(Oord,A.v.d.,2017)、机器翻译(Gu,J.,2018;Ghazvininejad,M.,2019)、单词表达学习(Devlin,J.,2019)、语音识别(Chan,W.,2020)。将transformer和并行解码结合权衡了计算成本和集合预测需要的全局计算能力</li>\n<li>目标检测<br> 目标检测根据一些初始猜测做预测，两阶段目标检测是根据推荐框(Ren,S.,2015;Cai,Z.,2019)做预测，单阶段目标检测是根据锚点(Lin,T.Y.,2017)或可能的目标中心网格(Zhou,X.,2019;Tian,Z.,2019)做预测,这类目标检测严重依赖初始猜测设置方式(Zhang,S.2019)。<ul>\n<li>基于集合损失<br>  早期的深度学习模型只用卷积和全连接层建模不同预测的关系并用辅助后处理改善性能；最近有些检测器使用真值和预测值之间的非唯一分配规则并结合NMS(Ren,S.,2015;Lin,T.Y.,2017;Zhou,X.,2019);可学习NMS方法(Hosang,J.H.,2017;Bodla,N.,2017)和关系网络(Hu,H.,2018)利用注意力显式建模不同预测间的关系，直接使用集合损失不在需要后处理步骤，但是也需要上下文特征例如推荐框坐标。</li>\n<li>循环探测<br>  端到端方法(Stewart,R.J.,2015;Romera-Paredes,B.,2015;Park,E.,2015;Ren,M.,2017)使用二分匹配损失以及基于CNN的编解码框架直接产生一组边界框，这些方法只在小样本数据中评估，基于的是自动回归模型没有利用具有并行解码的transformer。</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"当前的问题\"><a href=\"#当前的问题\" class=\"headerlink\" title=\"当前的问题\"></a>当前的问题</h2><ol>\n<li>对小目标性能不佳，可以用FPN进行改善</li>\n<li>长时间的训练</li>\n</ol>\n<h2 id=\"使用的方法-4\"><a href=\"#使用的方法-4\" class=\"headerlink\" title=\"使用的方法\"></a>使用的方法</h2><p>基于一组全局损失，通过二分匹配给出唯一预测，使用transformer编解码架构，需要一组固定数量的学习好的目标query, DETR会推理目标和全局图像的关系然后输出最终的预测集合<br><img src=\"./image-20221201110746341.png\" alt=\"image-20221201110746341\"></p>\n<p><img src=\"./image-20221202090850052.png\" alt=\"image-20221202090850052\"></p>\n<h3 id=\"目标检测集合预测损失\"><a href=\"#目标检测集合预测损失\" class=\"headerlink\" title=\"目标检测集合预测损失\"></a>目标检测集合预测损失</h3><p>DETR推理N个固定大小的预测结果集合，N明显多于图像中典型的目标数量，给出预测目标（类别，位置，大小）同真值比较后的好坏。用$y$表示ground truth目标集合，$\\hat{y}=\\{\\hat{y}_i\\}_{i=1}^N$表示有N个预测值的集合，当$N$远大于图像中物体个数时，可以认为$y$集合也是由N个真实结果构成没有目标的结果被符号$\\phi$填充。两个集合之间进行二分匹配，搜索N个元素$\\sigma\\in\\mathfrak{G}_N$的一个置换以得到最小代价$\\hat{\\sigma}=\\mathop{\\arg\\min}\\limits_{\\sigma\\in\\mathfrak{G}_N}\\sum\\limits_i^N\\mathcal{L}_{match}(y_i,\\hat{y}_{\\sigma(i)})$, （1）$\\mathcal{L}_{match}(y_i,\\hat{y}_{\\sigma(i)})$是真值$y_i$和具有索引$\\sigma(i)$的预测值之间的逐对匹配代价函数；（2）第i个真值元素可以看成$y_i=(c_i,b_i)$,$c_i$是目标类别标签，符号$\\phi$用N/A表示，$b_i=\\{b_x,b_y,b_w,b_h\\}\\in[0,1]^4$是向量记录了中心坐标和相对图像大小的高度和宽度；（3）对索引$\\sigma(i)$的预测值，定义类$c_i$的概率为$\\hat{p}_{\\sigma(i)}(c_i)$,预测边框为$\\hat{b}_{\\sigma(i)}$，定义：</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}_{match}(y_i,\\hat{y}_{\\sigma(i)})=-1_{\\{c_i\\neq\\phi\\}}\\hat{p}_{\\sigma(i)}(c_i)+1_{\\{c_i\\neq\\phi\\}}\\mathcal{L}_{box}(b_i,\\hat{b}_{\\sigma(i)})</script><script type=\"math/tex; mode=display\">\\mathcal{L}_{box}(b_i,\\hat{b}_{\\sigma(i)})=\\lambda_{iou}\\mathcal{L}_{iou}(b_i,\\hat{b}_{\\sigma(i)})+\\lambda_{L1}||b_i-\\hat{b}_{\\sigma(i)}||_1</script><p>这是真实框集合中的每一框和预测框集合中的每一个框匹配，损失值最小的预测框为该真实框的最佳匹配框，得到唯一匹配。作用类似于用于匹配的启发式分配规则（匹配推荐框、锚点）区别只是一一匹配。根据二分匹配损失计算匈牙利损失函数(类别的负对数似然函数损失和边框损失线性组合)，定义如下：</p>\n<script type=\"math/tex; mode=display\">\\mathcal{L}_{Hungarian}(y,\\hat{y})=\\sum\\limits_{i=1}^N\\left[-\\log\\,\\hat{p}_{\\hat{\\sigma}(i)}(c_i)+1_{\\{c_i\\neq\\phi\\}}\\mathcal{L}_{box}(b_i,\\hat{b}_{\\hat{\\sigma}(i)})\\right]</script><p>$\\hat{\\sigma}$是二分分配损失计算得到的最优预测值索引，注：$c_i=\\phi$的样本通常比目标类别多很多，属于不平衡样本集，所以对于无目标类别将对数几率的权重减少到$1/10$。别的目标检测器对边界框的预测是基于和初始猜测边框的偏差，这里是直接生成边框预测，$\\ell_1$损失的缺点就是有损失值的相对尺度问题，即大目标的大边框和小目标的小边框都可能产生相似的$\\ell_1$损失，因此无法通过该损失区分大小尺寸，需要结合具有尺度不变性的交并比损失，注两个损失值还需要在批中样本上做归一化处理。</p>\n<h3 id=\"DETR框架\"><a href=\"#DETR框架\" class=\"headerlink\" title=\"DETR框架\"></a>DETR框架</h3><p>架构三部分组成：1、CNN骨架（提取出致密特征表达），2、transformer编解码，3、feed forward network,FFN(生成最终的预测)。</p>\n<h4 id=\"CNN\"><a href=\"#CNN\" class=\"headerlink\" title=\"CNN\"></a>CNN</h4><p>初始图片$x_{img}\\in\\mathbb{R}^{3\\times H_0\\times W_0}$, CNN输出低分辨率的激活图$f\\in\\mathbb{R}^{C\\times H\\times W}$,$C=2048$,$H,W=\\frac{H_0}{32},\\frac{W_0}{32}$</p>\n<h4 id=\"transformer编码器\"><a href=\"#transformer编码器\" class=\"headerlink\" title=\"transformer编码器\"></a>transformer编码器</h4><ol>\n<li>用1x1卷积核减少高层激活图$f$的通道维度从$C$到$d$，新的特征图为$z_0\\in\\mathbb{R}^{d\\times H\\times W}$</li>\n<li>编码器需要输入特征序列，将$z_0$的空间维度塌缩到一个维度$z_0\\in\\mathbb{R}^{d\\times HW}$</li>\n<li>编码器包含：1、多头注意力模块，2、feed forward网络层，编码器具有输入序列的交换不变性，因此需要向序列中添加固定位置嵌入体</li>\n</ol>\n<h4 id=\"transformer解码器\"><a href=\"#transformer解码器\" class=\"headerlink\" title=\"transformer解码器\"></a>transformer解码器</h4><ul>\n<li>使用标准的多头自注意力和编解码注意力机制的transformer架构，但是区别是解码器每层并行解码N个目标，原来的模型是使用自动回归模型一次预测输出序列中的一个元素。</li>\n<li>由于解码器也是对输入序列具有交换不变性，因此也需要让N个输入嵌入是不同的以生成不同结果，这N个输入嵌入体就是位置编码也称作N个目标查询，将他们添加到每一个注意力层的输入部分。</li>\n<li>N个目标查询被解码器转换成输出嵌入体</li>\n</ul>\n<h4 id=\"FFN网络层\"><a href=\"#FFN网络层\" class=\"headerlink\" title=\"FFN网络层\"></a>FFN网络层</h4><ul>\n<li>具有ReLU激活函数的3层感知网络、线性投影层</li>\n<li>FFN将N个输出嵌入体独立分解成N个边框坐标和类别标签，即最终预测结果</li>\n<li>边框坐标是根据输入图像做归一化</li>\n<li>类别标签使用softmax函数给出</li>\n</ul>\n<h4 id=\"补充多头注意力层\"><a href=\"#补充多头注意力层\" class=\"headerlink\" title=\"补充多头注意力层\"></a>补充多头注意力层</h4><p><strong>多头注意力</strong><br>有M个注意力头，每个头的维度是d，多头注意力是一个函数函数的输入输出如下，注：$d^\\prime=\\frac{d}{M}$，在花括号中列出了矩阵/张量的大小</p>\n<script type=\"math/tex; mode=display\">\\operatorname{mh-attn}: \\underbrace{X_q}\\limits_{d\\times N_q},\\underbrace{X_{kv}}\\limits_{d\\times N_{kv}},\\underbrace{T}\\limits_{M\\times 3\\times d^\\prime\\times d},\\underbrace{L}\\limits_{d\\times d}\\;\\mapsto\\;\\underbrace{\\tilde{X}_q}\\limits_{d\\times N_q}</script><p>展开为每个单头计算结果在特征阶次上的连接</p>\n<script type=\"math/tex; mode=display\">\\operatorname{mh-attn}(X,X,T,L)=L\\left[attn(X_q,X_{kv},T_1);\\cdots;attn(X_q,X_{kv},T_M)\\right]</script><p>$X_q$是长度为$N_q$的查询序列，$X_{kv}$是长度为$N_{kv}$的键-值序列，$T$是权重张量用于计算所谓的查询、键值、得分嵌入体，$L$是投影矩阵。多头自注意力是$X_q=X_{kv}$的特殊情况</p>\n<script type=\"math/tex; mode=display\">\\operatorname{mh-s-attn}(X,T,L)=\\operatorname{mh-attn}(X,X,T,L)</script><p>一般在得到$\\tilde{X}_q$后还要在进行残差连接、dropout和层归一化处理</p>\n<script type=\"math/tex; mode=display\">\\hat{X}_q=layernorm(X_q+dropout(\\tilde{X}_q))</script><p><strong>单头注意力</strong><br>单头注意力的权重张量$T^\\prime\\in\\mathbb{R}^{3\\times d^\\prime\\times d}$，在计算单头注意力时还要引入位置编码嵌入体$P_q\\in\\mathbb{R}^{d\\times N_q}$和$P_{kv}\\in\\mathbb{R}^{d\\times N_{kv}}$。<br>首先计算添加位置编码后的查询、键值、得分嵌入体，计算复杂度$\\mathcal{O}(d^\\prime N_q\\cdot d +d^\\prime N_{kv} \\cdot d)$，$Q_i,K_i:\\mathcal{O}(d^\\prime d)$</p>\n<script type=\"math/tex; mode=display\">\\left[Q;K;V\\right]=\\left[T_1^\\prime(X_q+P_q);T_2^\\prime(X_{kv}+P_{kv});T_3^\\prime X_{kv}\\right]</script><p>$T^\\prime$就是$T_1^\\prime,T_2^\\prime,T_3^\\prime$在第一阶次上的连接</p>\n<p>再计算注意力权重$\\alpha$,通过query嵌入体和键值嵌入体点积运算和softmax运算<br>因此可以得到查询序列中的每个查询嵌入体和键值序列中的每个键值嵌入体之间的权重（相似性），计算复杂度$\\alpha_{i,j}:\\mathcal{O}(d^\\prime)$：</p>\n<script type=\"math/tex; mode=display\">\\alpha_{i,j}=\\frac{e^{\\frac{1}{\\sqrt{d^\\prime}}Q_i^TK_j}}{Z_i}\\quad\\quad Z_i=\\sum\\limits_{j=1}^{N_{kv}}e^{\\frac{1}{\\sqrt{d^\\prime}}Q_i^TK_j}</script><p>最后计算带注意力权重$\\alpha$的第i个嵌入体在特征维度上的加权得分分布,计算复杂度$\\mathcal{O}(N_{kv}\\cdot d^\\prime)$</p>\n<script type=\"math/tex; mode=display\">attn(X_{q_i},X_{kv},T^\\prime)=\\sum\\limits_{j=1}^{N_{kv}}\\alpha_{i,j}V_j</script><p>在查询序列阶次上连接,计算复杂度$\\mathcal{O}(N_qN_{kv}\\cdot d^\\prime)$</p>\n<script type=\"math/tex; mode=display\">attn(X_q,X_{kv},T^\\prime)=[attn(X_{q_1},X_{kv},T^\\prime);\\cdots;attn(X_{q_{N_q}},X_{kv},T^\\prime)]</script><p><img src=\"./image-20221203145853415.png\" alt=\"image-20221203145853415\"></p>\n"}],"PostAsset":[{"_id":"source/_posts/c-知识汇总/内存.jpg","slug":"内存.jpg","post":"cl9s8r0va0001203k88x70g64","modified":0,"renderable":0},{"_id":"source/_posts/c-知识汇总/内存.psd","slug":"内存.psd","post":"cl9s8r0va0001203k88x70g64","modified":0,"renderable":0},{"_id":"source/_posts/c-知识汇总/内存副本.jpg","slug":"内存副本.jpg","post":"cl9s8r0va0001203k88x70g64","modified":0,"renderable":0},{"_id":"source/_posts/robotic-operating-system/image-20221020164052499.png","slug":"image-20221020164052499.png","post":"cl9s8r0vk0008203k01xr50hi","modified":0,"renderable":0},{"_id":"source/_posts/slam/image-20221107155224157.png","slug":"image-20221107155224157.png","post":"cla22wp820001qt3k8e0ademl","modified":0,"renderable":0},{"_id":"source/_posts/slam/image-20221107161443842.png","slug":"image-20221107161443842.png","post":"cla22wp820001qt3k8e0ademl","modified":0,"renderable":0},{"_id":"source/_posts/slam/image-20221107162453662.png","slug":"image-20221107162453662.png","post":"cla22wp820001qt3k8e0ademl","modified":0,"renderable":0},{"_id":"source/_posts/slam/image-20221107162722048.png","slug":"image-20221107162722048.png","post":"cla22wp820001qt3k8e0ademl","modified":0,"renderable":0},{"_id":"source/_posts/slam/image-20221107163948504.png","slug":"image-20221107163948504.png","post":"cla22wp820001qt3k8e0ademl","modified":0,"renderable":0},{"_id":"source/_posts/slam/image-20221107164511662.png","slug":"image-20221107164511662.png","post":"cla22wp820001qt3k8e0ademl","modified":0,"renderable":0},{"_id":"source/_posts/slam/image-20221107165342106.png","slug":"image-20221107165342106.png","post":"cla22wp820001qt3k8e0ademl","modified":0,"renderable":0},{"_id":"source/_posts/slam/image-20221108112428467.png","slug":"image-20221108112428467.png","post":"cla22wp820001qt3k8e0ademl","modified":0,"renderable":0},{"_id":"source/_posts/slam/image-20221108141628283.png","slug":"image-20221108141628283.png","post":"cla22wp820001qt3k8e0ademl","modified":0,"renderable":0},{"_id":"source/_posts/c-知识汇总/基类派生类.ai","slug":"基类派生类.ai","post":"cl9s8r0va0001203k88x70g64","modified":0,"renderable":0},{"_id":"source/_posts/c-知识汇总/基类派生类.png","slug":"基类派生类.png","post":"cl9s8r0va0001203k88x70g64","modified":0,"renderable":0},{"_id":"source/_posts/g2o/20210103152500545.png","slug":"20210103152500545.png","post":"claugokrd0007ccfy627y2ju3","modified":0,"renderable":0},{"_id":"source/_posts/目标识别/image-20221125100901770.png","slug":"image-20221125100901770.png","post":"claugokre0008ccfyfe7b9of7","modified":0,"renderable":0},{"_id":"source/_posts/目标识别/image-20221127151329769.png","slug":"image-20221127151329769.png","post":"claugokre0008ccfyfe7b9of7","modified":0,"renderable":0},{"_id":"source/_posts/目标识别/image-20221127153059574.png","slug":"image-20221127153059574.png","post":"claugokre0008ccfyfe7b9of7","modified":0,"renderable":0},{"_id":"source/_posts/目标识别/image-20221130151059959.png","slug":"image-20221130151059959.png","post":"claugokre0008ccfyfe7b9of7","modified":0,"renderable":0},{"_id":"source/_posts/目标识别/image-20221130160631440.png","slug":"image-20221130160631440.png","post":"claugokre0008ccfyfe7b9of7","modified":0,"renderable":0},{"_id":"source/_posts/目标识别/image-20221201110746341.png","slug":"image-20221201110746341.png","post":"claugokre0008ccfyfe7b9of7","modified":1,"renderable":0},{"_id":"source/_posts/目标识别/image-20221202090850052.png","slug":"image-20221202090850052.png","post":"claugokre0008ccfyfe7b9of7","modified":1,"renderable":0},{"_id":"source/_posts/目标识别/image-20221203145853415.png","slug":"image-20221203145853415.png","post":"claugokre0008ccfyfe7b9of7","modified":1,"renderable":0}],"PostCategory":[{"post_id":"cl9s8r0vk0008203k01xr50hi","category_id":"cl9s8r0vm000b203kdq677r54","_id":"cl9s8r0vo000l203kbqlodnot"},{"post_id":"cl9s8r0vk0008203k01xr50hi","category_id":"cl9s8r0vo000h203k7s5gfwog","_id":"cl9s8r0vp000m203kbrtxbhzo"},{"post_id":"cl9s8r0va0001203k88x70g64","category_id":"cl9s8r0vf0004203k3cwx1coh","_id":"cl9s8r0vp000n203k4drpfnbe"},{"post_id":"cl9s8r0va0001203k88x70g64","category_id":"cl9s8r0vn000d203kf6qw03ei","_id":"cl9s8r0vp000o203ker869hau"},{"post_id":"cl9s8r0va0001203k88x70g64","category_id":"cl9s8r0vo000i203k8sa7auph","_id":"cl9s8r0vp000p203k4ae11d8s"},{"post_id":"cl9s8r0vq000q203k2je24i63","category_id":"cl9s8r0vq000r203k2yeg0o14","_id":"cl9s8r0vs000x203kestygzlz"},{"post_id":"cl9s8r0vq000q203k2je24i63","category_id":"cl9s8r0vr000u203kh2t9a0ff","_id":"cl9s8r0vs000y203k4gug45ph"},{"post_id":"cla22wp800000qt3k2a12avyd","category_id":"cla22wp830002qt3k5phfec7a","_id":"cla22wp880008qt3k65jv9xhp"},{"post_id":"cla22wp820001qt3k8e0ademl","category_id":"cla22wp870004qt3k9awob7qa","_id":"cla22wp89000aqt3kbfup0u2h"},{"post_id":"claugokr40000ccfyaarqamt8","category_id":"cl9s8r0vf0004203k3cwx1coh","_id":"claugokrb0004ccfy534jbmb0"},{"post_id":"claugokr40000ccfyaarqamt8","category_id":"cl9s8r0vn000d203kf6qw03ei","_id":"claugokrb0005ccfyasl02v62"},{"post_id":"claugokr40000ccfyaarqamt8","category_id":"claugokra0002ccfycjb4dx9h","_id":"claugokrb0006ccfyds0hgnk2"},{"post_id":"claugokrd0007ccfy627y2ju3","category_id":"cl9s8r0vf0004203k3cwx1coh","_id":"claugokrf000cccfyepj8ebda"},{"post_id":"claugokrd0007ccfy627y2ju3","category_id":"cl9s8r0vn000d203kf6qw03ei","_id":"claugokrf000dccfy9fby9pd8"},{"post_id":"claugokre0008ccfyfe7b9of7","category_id":"cl9s8r0vq000r203k2yeg0o14","_id":"claugokrg000fccfyh2kzfj4s"},{"post_id":"claugokre0008ccfyfe7b9of7","category_id":"claugokrf000bccfy4j171pfg","_id":"claugokrg000gccfy2ems9c0x"}],"PostTag":[{"post_id":"cl9s8r0va0001203k88x70g64","tag_id":"cl9s8r0vh0005203kbwdkd6mg","_id":"cl9s8r0vn000e203kc7sr4237"},{"post_id":"cl9s8r0va0001203k88x70g64","tag_id":"cl9s8r0vm000a203k9yh80eu6","_id":"cl9s8r0vn000f203k3bmn4sxb"},{"post_id":"cl9s8r0vk0008203k01xr50hi","tag_id":"cl9s8r0vm000c203k8dbwdj6r","_id":"cl9s8r0vo000j203k2moq4epj"},{"post_id":"cl9s8r0vk0008203k01xr50hi","tag_id":"cl9s8r0vn000g203k0iy55pa1","_id":"cl9s8r0vo000k203k3vuv0osg"},{"post_id":"cl9s8r0vq000q203k2je24i63","tag_id":"cl9s8r0vr000s203k8rs4925x","_id":"cl9s8r0vs000v203k2adw5fna"},{"post_id":"cl9s8r0vq000q203k2je24i63","tag_id":"cl9s8r0vr000t203kapyma8dg","_id":"cl9s8r0vs000w203k0lqy0juz"},{"post_id":"cla22wp800000qt3k2a12avyd","tag_id":"cla22wp860003qt3kh2ws87hg","_id":"cla22wp880006qt3kfeo6egsp"},{"post_id":"cla22wp800000qt3k2a12avyd","tag_id":"cl9s8r0vh0005203kbwdkd6mg","_id":"cla22wp880007qt3k9kgt09wr"},{"post_id":"cla22wp820001qt3k8e0ademl","tag_id":"cla22wp870005qt3kh3e219qf","_id":"cla22wp890009qt3kfs7922dt"},{"post_id":"claugokr40000ccfyaarqamt8","tag_id":"claugokr60001ccfy0iqo0yz0","_id":"claugokrb0003ccfy8sqfc05c"},{"post_id":"claugokrd0007ccfy627y2ju3","tag_id":"cla22wp870005qt3kh3e219qf","_id":"claugokre0009ccfy9j0gdzis"},{"post_id":"claugokre0008ccfyfe7b9of7","tag_id":"claugokrf000accfy1zisckmv","_id":"claugokrg000eccfy72gj18fq"}],"Tag":[{"name":"博客","_id":"cl9s8r0vh0005203kbwdkd6mg"},{"name":"c++","_id":"cl9s8r0vm000a203k9yh80eu6"},{"name":"机器人","_id":"cl9s8r0vm000c203k8dbwdj6r"},{"name":"系统","_id":"cl9s8r0vn000g203k0iy55pa1"},{"name":"神经网络","_id":"cl9s8r0vr000s203k8rs4925x"},{"name":"机器学习","_id":"cl9s8r0vr000t203kapyma8dg"},{"name":"数学","_id":"cla22wp860003qt3kh2ws87hg"},{"name":"slam14讲","_id":"cla22wp870005qt3kh3e219qf"},{"name":"cmake","_id":"claugokr60001ccfy0iqo0yz0"},{"name":"目标识别","_id":"claugokrf000accfy1zisckmv"}]}}