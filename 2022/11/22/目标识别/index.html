<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>目标识别 | 和其光, 同其尘</title><meta name="keywords" content="目标识别"><meta name="author" content="刘传玺"><meta name="copyright" content="刘传玺"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="[TOC] 基于深度学习的视觉目标检测技术综述作者：曹家乐，2022 发展历程 基于手工设计特征的方法 支持向量机 AdaBoost Haar特征(Viola, 2004) 方向梯度直方图histograms of oriented gradients(Dalal, 2005)   深度学习 区域卷积神经网络region-based convolutional neural network, R-">
<meta property="og:type" content="article">
<meta property="og:title" content="目标识别">
<meta property="og:url" content="http://chauchy-liu.github.io/2022/11/22/%E7%9B%AE%E6%A0%87%E8%AF%86%E5%88%AB/index.html">
<meta property="og:site_name" content="和其光, 同其尘">
<meta property="og:description" content="[TOC] 基于深度学习的视觉目标检测技术综述作者：曹家乐，2022 发展历程 基于手工设计特征的方法 支持向量机 AdaBoost Haar特征(Viola, 2004) 方向梯度直方图histograms of oriented gradients(Dalal, 2005)   深度学习 区域卷积神经网络region-based convolutional neural network, R-">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/cover5.JPG">
<meta property="article:published_time" content="2022-11-22T01:13:23.000Z">
<meta property="article:modified_time" content="2022-12-01T02:02:00.875Z">
<meta property="article:author" content="刘传玺">
<meta property="article:tag" content="目标识别">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/cover5.JPG"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://chauchy-liu.github.io/2022/11/22/%E7%9B%AE%E6%A0%87%E8%AF%86%E5%88%AB/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="manifest" href="/img/pwa/manifest.json"/><link rel="apple-touch-icon" sizes="180x180" href="/img/pwa/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/img/pwa/32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/img/pwa/16.png"/><link rel="mask-icon" href="/img/pwa/safari-pinned-tab.svg" color="#5bbad5"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'mediumZoom',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#2d3035","position":"bottom-left"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '目标识别',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2022-12-01 10:02:00'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/touxiang.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">11</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">10</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">11</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/cover5.JPG')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">和其光, 同其尘</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">目标识别</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-11-22T01:13:23.000Z" title="发表于 2022-11-22 09:13:23">2022-11-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-01T02:02:00.875Z" title="更新于 2022-12-01 10:02:00">2022-12-01</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%96%87%E7%8C%AE/">文献</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%96%87%E7%8C%AE/%E7%9B%AE%E6%A0%87%E8%AF%86%E5%88%AB/">目标识别</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>26分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="目标识别"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>[TOC]</p>
<h1 id="基于深度学习的视觉目标检测技术综述"><a href="#基于深度学习的视觉目标检测技术综述" class="headerlink" title="基于深度学习的视觉目标检测技术综述"></a>基于深度学习的视觉目标检测技术综述</h1><p>作者：曹家乐，2022</p>
<h2 id="发展历程"><a href="#发展历程" class="headerlink" title="发展历程"></a>发展历程</h2><ol>
<li>基于手工设计特征的方法<ol>
<li>支持向量机</li>
<li>AdaBoost</li>
<li>Haar特征(Viola, 2004)</li>
<li>方向梯度直方图histograms of oriented gradients(Dalal, 2005)</li>
</ol>
</li>
<li>深度学习<ol>
<li>区域卷积神经网络region-based convolutional neural network, R-CNN (Girshick, 2014)</li>
<li>单次检测single shot detector, SSD (Liu, 2016)</li>
<li>yolo (Redmon, 2016)</li>
<li>detection transformer, DETR (Carion, 2020)</li>
</ol>
</li>
</ol>
<p>深度网络模型：</p>
<ol>
<li>AlexNet(Krizhevsky,2012)</li>
<li>GoogLeNet(Szegedy,2015)</li>
<li>VGGNet(Simonyan,2015)</li>
<li>ResNet(He,2016)</li>
<li>DenseNet(Huang,2017)</li>
<li>Mobilenet(Howard,2018)</li>
<li>ShuffleNet(Zhang,2018)</li>
<li>SENet(Hu, 2018)</li>
<li>EfficientNet(Tan,2019)</li>
<li>ViT(Dosovitskiy,2021)</li>
<li>Swin(Liu,2022)</li>
</ol>
<p>目标检测方法</p>
<ol>
<li>DetectorNet(Szegedy,2014)</li>
<li>R-CNN(Girshick,2014)</li>
<li>OvearFeat(Sermanet,2014)</li>
<li>SPPNet(He,2015)</li>
<li>Fast R-CNN(Girshick,2016; Ren,2016)</li>
<li>YOLO(Redmon,2016)</li>
<li>SSD(Liu,2016)</li>
<li>R-FCN(Dai,2017)</li>
<li>FPN(Lin,2017)</li>
<li>YOLOv2(Redmon,2017)</li>
<li>Mask RCNN(He,2018)</li>
<li>DCN(Dai,2018)</li>
<li>RetinaNet(Lin,2018)</li>
<li>Cascade RCNN(Cai,2018)</li>
<li>YOLOv3(Redmon,2019)</li>
<li>CornerNet(Law,2019)</li>
<li>FCOS(Tian,2020)</li>
<li>CenterNet(Zhou,2020)</li>
<li>EfficientDet(Tan,2020)</li>
<li>ATSS(Zhang,2020)</li>
<li>MoCo(He,2020)</li>
<li>YOLOv4(Bochkovskiy,2021)</li>
<li>Deformable DETR(Zhu,2021)</li>
<li>DETR(Carion,2021)</li>
<li>YOLOv5(Jocher,2021)</li>
<li>UP-DETR(Dai,2021)</li>
<li>Pix2seq(Chen,2022)</li>
</ol>
<h2 id="检测设备"><a href="#检测设备" class="headerlink" title="检测设备"></a>检测设备</h2><ol>
<li>单目相机</li>
<li>双目相机 （提供三维信息）</li>
</ol>
<h2 id="基于单目相机流程及其涵盖的方法"><a href="#基于单目相机流程及其涵盖的方法" class="headerlink" title="基于单目相机流程及其涵盖的方法"></a>基于单目相机流程及其涵盖的方法</h2><ol>
<li><p>数据预处理</p>
<ul>
<li>翻转</li>
<li>放缩</li>
<li>均值归一化</li>
<li>色调变化</li>
<li>剪切、擦除、分区(DeVries, 2017; Zhong, 2020b; Singh, 2017; Chen, 2020a)</li>
<li>混合(Mixup: Zhang, 2018; CutMix: Yun, 2019; Fang, 2019; Mosaic: Bochkovskiy, 2020; Montage: Zhou, 2020; dynamic scale training: Chen, 2020b)</li>
</ul>
</li>
<li><p>检测网络</p>
<ul>
<li>基础骨架<ul>
<li>AlexNet(Krizhevsky, 2012)</li>
<li>VGGNet(Simonyan, 2014) </li>
<li>ResNet(He, 2016)</li>
<li>DenseNet(Huang, 2017)</li>
<li>Transformer(Vaswani, 2017), ViT(Dosovitskiy, 2021; Beal, 2020), Swin(Liu, 2021c), PVT(Wang, 2021c)</li>
</ul>
</li>
<li>特征融合<ul>
<li>特征金字塔(Lin, 2017a) </li>
</ul>
</li>
<li>预测网络(分类回归任务)<ul>
<li>两阶段目标检测：全连接</li>
<li>单阶段目标检测：全卷积</li>
</ul>
</li>
</ul>
</li>
<li><p>标签分配与损失计算</p>
<ul>
<li><p>标签分配准则</p>
<ul>
<li>交并比准则<ul>
<li>基于锚点框与真实框的交并比</li>
</ul>
</li>
<li>距离准则<ul>
<li>基于无锚点框 ，点到物体中心的距离</li>
</ul>
</li>
<li>似然估计准则<ul>
<li>分类、回归</li>
</ul>
</li>
<li>二分匹配准则<ul>
<li>分类、回归</li>
</ul>
</li>
</ul>
<ul>
<li>损失函数<ul>
<li>交叉熵函数</li>
<li>聚焦损失函数(Lin, 2017b)</li>
<li>回归损失函数: L1损失函数、平滑L1损失函数、IoU损失函数、GIoU损失函数(Reztofighi, 2019)、CIoU损失函数(Zheng, 2020b)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>后处理：为每个物体保留一个检测结果，去除冗余结果</p>
<ul>
<li>非极大值抑制NMS</li>
<li>soft-NMS(Bodla, 2017)</li>
<li>IoUNet(Jiang, 2018)</li>
<li>定位方差(He, 2018)</li>
<li>上下文推理(Pato, 2020)</li>
</ul>
</li>
</ol>
<h3 id="基于锚点框方法"><a href="#基于锚点框方法" class="headerlink" title="基于锚点框方法"></a>基于锚点框方法</h3><p>描述：为空间每个位置设定多个矩形框（框的尺度和长宽比），尽可能的涵盖图像中的物体<br>分类：</p>
<ol>
<li>两阶段目标检测<ol>
<li>提取k个候选框</li>
<li>对候选框分类和回归</li>
</ol>
</li>
<li>单阶段目标检测<ol>
<li>直接对锚点框分类和回归</li>
</ol>
</li>
</ol>
<h3 id="基于无锚点框方法"><a href="#基于无锚点框方法" class="headerlink" title="基于无锚点框方法"></a>基于无锚点框方法</h3><p>分类：</p>
<ol>
<li>基于关键点目标检测：多个关键点集成到物体上</li>
<li>基于内部点目标检测：物体内部点到物体边界的上下左右偏移量</li>
</ol>
<h2 id="基于双目相机流程及其涵盖的方法"><a href="#基于双目相机流程及其涵盖的方法" class="headerlink" title="基于双目相机流程及其涵盖的方法"></a>基于双目相机流程及其涵盖的方法</h2><p>方法流程与单目相同</p>
<h3 id="基于直接视锥空间"><a href="#基于直接视锥空间" class="headerlink" title="基于直接视锥空间"></a>基于直接视锥空间</h3><p>描述：直接使用基础骨干提取的两个单目特征构造双目特征。<br>方法：</p>
<ol>
<li>串接特征构造<br> 不改变原单目特征的坐标空间</li>
<li>平面扫描构造<br> 通过逐视差平面或者深度平面地扫描一对2维特征，所得三维特征即是匹配代价体</li>
</ol>
<h3 id="基于显式逆投影空间"><a href="#基于显式逆投影空间" class="headerlink" title="基于显式逆投影空间"></a>基于显式逆投影空间</h3><p>描述：将存在尺度变化和遮挡问题的视锥空间图像逆投影到尺度均匀、不存在重叠遮挡的3维空间，从而缓解视锥投影产生的问题。<br>方法：</p>
<ol>
<li>基于原始图像视差的逆投影方法<br> 先利用双目视差估计算法预测每个像素的视差，将像素的视差逆投影到三维空间生成电云，最后利用点云的3维检测方法进行目标检测</li>
<li>基于特征体的逆投影方法<br> 通过插值和采样将平面扫描得到的匹配代价体变换到3维空间，利用了图像特征提供的颜色和纹理信息。</li>
<li>基于候选像素视差的逆投影方法<br> 仅聚焦感兴趣目标区域的三维空间，先利用实例分割方案得到目标的前景像素，然后生成仅含前景区域的3维空间。<script type="math/tex; mode=display">逆投影策略\left\{\begin{align*}
     & 前景共享3维空间\\
     & 每个实例生成相互独立的3维子空间
 \end{align*}\right.</script></li>
</ol>
<h2 id="发展趋势"><a href="#发展趋势" class="headerlink" title="发展趋势"></a>发展趋势</h2><ol>
<li>高效的端到端目标检测transform，加快收敛，减少计算资源。</li>
<li>基于自监督学习的目标检测，目标检测任务存在数量和尺度不确定的物体。</li>
<li>长尾分布目标检测，现实世界物体类别数量庞大且不同类别的物体数量存在极度不平衡。</li>
<li>小样本、0样本目标检测能力的提高</li>
<li>大规模双目目标检测数据集少，需要标注物体的2维和3维信息以及相机标注视差和相机参数，还需完善评价体系和开放测试平台</li>
<li>弱监督双目目标检测</li>
</ol>
<h1 id="YOLOv7-Trainable-bag-of-freebies-sets-new-state-of-the-art-for-real-time-object-detectors"><a href="#YOLOv7-Trainable-bag-of-freebies-sets-new-state-of-the-art-for-real-time-object-detectors" class="headerlink" title="YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors"></a>YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</h1><p>作者：Chien-Yao Wang，2022</p>
<h2 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h2><p>发现的问题:</p>
<ol>
<li>模型再参数化</li>
<li>用动态标签分配技术后，如何将动态标签分配给模型的不同输出层</li>
</ol>
<p>本篇文章解决的问题：</p>
<ol>
<li>最高推理精度56.8%AP和最快推理速度160FPS,都达到最高水平，参与对比的模型有：YOLOv5、YOLOX、Scaled-YOLOv4、YOLOR、PPYOLOE、DETR、Deformable DETR、DINO-5scale-R50、ViT-Adapter-B、SWIN-L Cascade-Mask R-CNN、ConvNeXt-XL Cascade-Mask R-CNN</li>
<li>支持移动GPU以及边缘端和云端GPU</li>
<li>设计trainable bag-of-freebies方法，既可增强训练代价提高检测准确度又不增加推理代价</li>
<li>提出planned re-parameterized model</li>
<li>提出新的标签分配方法，coarse-to-fine lead guided label assigment</li>
<li>提出extend and compound scaling方法，减少40%的模型参数和50%计算时间</li>
</ol>
<p>其他模型的优点和不足：</p>
<ol>
<li>YOLOX和YOLOR只改进各种GPU推理速度</li>
<li>基于MobileNet, ShuffleNet, GhostNet针对CPU设计</li>
<li>ResNet, DarkNet, DLA, CSPNet针对GPU设计</li>
<li>YOLO和FCOS具有：1、快而强壮的网络架构，2、高效的特征集成方法，3、鲁棒的损失函数，4、高效的标签分配方法，5、准确的检测方法，6、高效训练方法</li>
</ol>
<h2 id="当前的不足"><a href="#当前的不足" class="headerlink" title="当前的不足"></a>当前的不足</h2><h2 id="使用的方法"><a href="#使用的方法" class="headerlink" title="使用的方法"></a>使用的方法</h2><h3 id="模型再参数化"><a href="#模型再参数化" class="headerlink" title="模型再参数化"></a>模型再参数化</h3><p>模型再参数化：融合多个计算模块于一体，可是为组装技术</p>
<script type="math/tex; mode=display">分类\left\{\begin{align*}
  & 模块间组合\\
  & 模型间组合
\end{align*}\right.</script><ol>
<li>模型间组合方法<ol>
<li>在不同训练集中训练多个相同模型，然后再平均模型的参数</li>
<li>在不同的迭代次数间进行模型参数均值化</li>
</ol>
</li>
<li>模块间组合方法<br> 在训练期间将一个模块分解成多个分支模块，在推理时将多个分支模块整合成一个完整模块</li>
</ol>
<h3 id="模型缩放"><a href="#模型缩放" class="headerlink" title="模型缩放"></a>模型缩放</h3><p>模型放缩可以增大和缩小模型使它适合不同计算能力的设备，满足不同的推理速度。</p>
<script type="math/tex; mode=display">放缩因子\left\{\begin{align*}
  & 分辨率resolution（输出图像尺度）\\
  & 深度depth（隐藏层层数）\\
  & 宽度width（通道数） \\
  & 阶段stage（特征金字塔层数）
\end{align*}\right.</script><p>放缩方法：网络架构搜索Network architecture search(NAS)，折中了网络参数大小、计算时间、推理速度和精确性</p>
<p>放缩因子的影响：</p>
<ol>
<li>对基于非连接的网络架构，在进行模型放缩时由于每个隐藏层的入度和出度不被改变因此可以独立分析每个放缩因子对模型参数数量和计算速度的影响</li>
<li>对基于连接的网络架构，在进行模型隐藏层深度放大或缩小时紧跟在计算模块后的转移/转化模块的入度会减小或增大，不能独立分析单个尺度因子的影响必须一起分析</li>
<li>文章提出compound scaling method合成尺度方法既可保持原有模型的的性质又可保持最优结构</li>
</ol>
<h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p>选取架构主要考虑1、模型参数数量，2、计算量，3、计算密度<br>采用Extended-ELAN(E-ELAN)扩展高效层聚合网络架构，该架构使用扩展基数层、清洗基数层、合并基数层增强网络学习能力</p>
<p>基于早期版本的YOLO框架和YOLOR框架作为基本框架</p>
<h3 id="可训练赠品袋trainable-bag-of-freebies"><a href="#可训练赠品袋trainable-bag-of-freebies" class="headerlink" title="可训练赠品袋trainable bag-of-freebies"></a>可训练赠品袋trainable bag-of-freebies</h3><ul>
<li><p>计划再参数化卷积</p>
<ol>
<li>如何将再参数化卷积和不同的网络结合？</li>
<li>提出planned re-parameterized convolution</li>
<li>提出无identity connection的RePConv构造planned re-parameterized convolution</li>
<li>用RepConvN网络层替换3堆叠ELAN架构中不同位置处的3x3卷积层</li>
</ol>
</li>
<li><p>以粗为辅以精为主的损失值</p>
<ol>
<li>深度监督是在网络的中间层添加额外的辅助头，将带有损失值信息的浅层网络权重作为引导方式</li>
<li>将负责最后输出的头称为主头，将用于协助训练的头称为辅头</li>
<li>采用软标签即使用网络预测输出的性质和分布和考虑实际标签，使用一些计算和优化方式生成可靠的标签</li>
<li>如何分配软标签到主头和辅头？<ol>
<li>分别计算主头和辅头预测结果，使用各自的分配器结合实际结果制作各自标签，再通过各自标签和头计算损失</li>
<li>文章提出经分配器用主头和实际结果制作由粗到精的等级标签，再将这些等级标签用在主头和辅头上计算损失值</li>
</ol>
</li>
</ol>
</li>
<li><p>批归一化放到conv-bn-activation 拓扑结构中</p>
</li>
<li><p>YOLOR的隐性知识以串行和并行方式结合到卷积特征图中</p>
</li>
<li><p>EMA模型</p>
</li>
</ul>
<h1 id="AN-IMAGE-IS-WORTH-16x16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE"><a href="#AN-IMAGE-IS-WORTH-16x16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE" class="headerlink" title="AN IMAGE IS WORTH 16x16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"></a>AN IMAGE IS WORTH 16x16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</h1><p>作者：Alexey Dosovitskiy, 2021</p>
<h2 id="解决的问题-1"><a href="#解决的问题-1" class="headerlink" title="解决的问题"></a>解决的问题</h2><p>根据Transformer计算的效率和可扩展性以及借鉴Transformer在自然语言方面取得的成功将其应用于图像上</p>
<ol>
<li>证明在大样本上14M-300M图，Transformer胜过CNN</li>
<li>可以处理中等分辨率图像</li>
<li>在更大规模数据集而非ImageNet数据集，探索图像识别</li>
</ol>
<p>前人工作优点和不足：</p>
<ol>
<li>把CNN框架和自注意力结合(Wang,2018; Carion, 2020)</li>
<li>用自注意力替换整个卷积网络(Ramachandran, 2019; Wang, 2020a)</li>
<li>ResNet架构在大尺度图片识别上是效果好的(Mahajan, 2018; Xie, 2020; Kolesnikov, 2020)</li>
<li>transformer用于机器翻译(Vaswani, 2017)</li>
<li>将transformer用到图像处理环境中<ul>
<li>只将自注意力应用于代查询像素的局部临域中，并非全局应用(Parmar,2018)</li>
<li>局部多头点积自注意力块完全替换卷积(Hu,2019; Ramachandran, 2019; Zhao, 2020)</li>
<li>稀疏Transformer在全局自注意力中使用放缩近似以适应图片(Child, 2019)</li>
<li><strong>(Cordonnier, 2020)提出的模型也是ViT但是没有证明大规模预训练模型可以甚至超过CNN模型，使用的2x2块太小只能适应小分辨率图像</strong></li>
</ul>
</li>
<li>(Sun, 2017)研究CNN性能如何随数据集大小变化</li>
<li>(Kolesnikov,2020; Djolonga,2020)从大规模的数据集上探索CNN的迁移学习</li>
</ol>
<h2 id="当前的不足-1"><a href="#当前的不足-1" class="headerlink" title="当前的不足"></a>当前的不足</h2><ol>
<li>Transformer和CNN相比缺少偏移量无法实现平移等变映射和无法进行局部化，因此在小样本中泛化能力弱</li>
<li><strong>应用ViT到其它计算机视觉任务，例如目标检测和分割</strong></li>
<li>持续开发自监督与训练方法</li>
</ol>
<h2 id="使用的方法-1"><a href="#使用的方法-1" class="headerlink" title="使用的方法"></a>使用的方法</h2><ul>
<li>分割图片成若干块，给这些块提供顺序线性嵌入体，并将嵌入体作为Transformer的输入</li>
<li>选用原始Transformer(Vaswani, 2017)</li>
<li>框架<br>  <img src="./image-20221125100901770.png" alt="image-20221125100901770"><br>  标准Transformer接收1维符号嵌入序列，将图像$x\in\mathbb{R}^{H\times W\times C}$分割成有序排列小块$x_p\in\mathbb{R}^{N\times(P^2\cdot C)}$，输入$z_0=[x_{class}; x_p^1E; x_p^2E; \cdots; x_p^NE]+E_{pos}$, $x_{class}\in\mathbb{R}^{1\times D}$, $E\in \mathbb{R}^{(P^2\cdot C)\times D}$, $E_{pos}\in\mathbb{R}^{(N+1)\times D}$</li>
</ul>
<h1 id="Pyramid-Vision-Transformer-A-Versatile-Backbone-for-Dense-Prediction-without-Convolutions"><a href="#Pyramid-Vision-Transformer-A-Versatile-Backbone-for-Dense-Prediction-without-Convolutions" class="headerlink" title="Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"></a>Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions</h1><p>作者：Wenhai Wang, 2021</p>
<h2 id="解决的问题-2"><a href="#解决的问题-2" class="headerlink" title="解决的问题"></a>解决的问题</h2><ol>
<li>文章(PVT)解决将transform移植到密集预测任务的问题</li>
<li>产生高输出分辨率，利用收缩的金字塔减少计算量</li>
<li>PVT继承了CNN和Transformer的优点，成为应对各种视觉任务的无需卷积的统一骨干架构，可以直接替换CNN骨干架构</li>
<li>提高下游任务的性能，包括：目标检测，实例分割，语意分割</li>
<li>克服传统transformer问题方式<ol>
<li>采用获取精细图像块（每个图像块为4x4像素）作为输入，以学习高分辨率的表示</li>
<li>采用逐渐缩小的金字塔形式减小transformer在深层网络中的序列长度，以减小计算量</li>
<li>采用空间减小注意力层spatial-reduction attention(SRA), 这近一步在学习高分辨率特征时减小资源损耗</li>
</ol>
</li>
<li>具有的优点<ol>
<li>CNN的局部接收视野随网络深度的增加而增加，PVT产生的是全局接收视野，这有利于检测和分割任务</li>
<li>借助金字塔结构的优势，易于将PVT插入到许多代表密集预测的管道中，例如RetinaNet和Mask R-CNN</li>
<li>通过结合PVT和其它特殊任务的Transformer解码器可以构建无卷积的管道，例如PVT+DETR用作目标检测</li>
</ol>
</li>
</ol>
<p>前人的工作及优缺点</p>
<ol>
<li><p>Vision Transformer(ViT)被用作分类任务</p>
</li>
<li><p>Vision Transformer(ViT)是一种柱状机构具有粗糙的输入图像块，不是很适合像素级别的致密预测任务如目标检测和分割任务，原因有：1、输出的特征图是单一尺度，分辨率低；2、高计算和内存占用成本, </p>
</li>
<li><p>CNN在视觉上取得巨大成功(Karen,2015; Kaiming,2017; Saining,2017)</p>
</li>
<li><p>将视觉任务建模成具有可学习查询功能的字典查找问题，使用Transformer解码器作为特殊任务的头应用于卷积框架的顶层(Nicolas,2020; Christian,2017; Enze,2021)</p>
</li>
<li><p>网络架构</p>
<ol>
<li><p>(Yann, 1998)首次引入CNN分辨手写数字，在整个图像空间共享卷积核参数实现平移等变性</p>
</li>
<li><p>(Alex, 2012; Karen, 2015)在大尺度图片分类数据集中使用堆叠的卷积块</p>
</li>
<li><p>GoogLeNet(Christain,2015)包含了多核路径</p>
</li>
<li><p>多路径卷积模块的效率在Inception系列网络(Christian,2016;)、ResNeXt(Saining, 2017)、 DPN(Yunpeng, 2017)、MixNet(Wenhai, 2018)、SKNet(Xiang,2019)网络中被验证</p>
</li>
<li><p>(Kaiming,2016)在ResNet网络中引入跳跃式连接卷积模块，这有助于训练更深的网络</p>
</li>
<li><p>(Gao,2017)在DenseNet中引入密集连接拓扑结构，把每个卷积模块同它前面的所有卷积模块相连</p>
</li>
</ol>
</li>
<li><p>密集预测任务</p>
<ol>
<li>目的：在特征图上进行像素级别的分类和回归</li>
<li><script type="math/tex; mode=display">分类\left\{\begin{align*}
& 目标检测\left\{\begin{array}{l}
           单阶段\left\{\begin{array}{l}
                          SSD(Wei,2016) \\
                          RetinaNet(Tsung-Yi\; Lin,2017) \\
                          FCOS(Zhi\; Tian,2019) \\
                          GFL(Xiang,2020) \\
                          PolarMask(Enze\;Xie,2021) \\
                          OneNet(Peize\; Sun,2020) \\
                       \end{array}\right. \\
           多阶段\left\{\begin{array}{l}
                          Faster R-CNN(Shaoqing\; Ren,2015) \\
                          Mask R-CNN(Kaiming\; He,2017) \\
                          Cascade R-CNN(Zhaowei\; Cai,2018) \\
                          Sparse R-CNN(Peize\; Sun, 2021) \\
                       \end{array}\right. \\
           结合CNN和Transformer\; decoder\left\{\begin{array}{l}
                           DETR(Nicolas\; Carion, 2020)\\
                           deformable DETR(Xizhou\; Zhu,2021)\\
                        \end{array}\right.
        \end{array}\right.\\
& 语意分割 \left\{\begin{array}{l}
               FCN(Jonathan\; Long,2015)\\
               deconvolution\; operation(Hyeonwoo\; Noh,2015)\\
               U-Net(Olaf\; Ronneberger,2015)\\
               添加金字塔池化(HengShuang\; Zhao,2017)\\
               添加FPN头(Alexander\; Kirillov,2019)\\
               DeepLab(Liang-Chieh\; Chen,2017)\\
            \end{array}\right.\\
& 实例分割
\end{align*}\right.</script></li>
<li><p>自注意力和变换器</p>
<ol>
<li>卷积滤波器权重经过训练后被固定无法动态适应不同的输入，(Xu Jia,2016)使用动态滤波器，(Ashish Vaswani,2017)使用自注意力<ol>
<li>非局部模块被(Xiaolong Wang,2018)引入解决时间和空间在大尺度上的依赖性，但是代价是计算成本和内存占用成本</li>
<li>(Zilong Huang,2019)引入十字交叉路径Criss-cross减小注意力机制的复杂度</li>
<li></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="当前的不足-2"><a href="#当前的不足-2" class="headerlink" title="当前的不足"></a>当前的不足</h2><ol>
<li>有许多特殊模块和运算方法是专门为CNN设计的因此没有在PVT中引入，例如：Squeeze-and-excitation network(SE)模块, Selective kernel network(SK)模块, 膨胀卷积模块, 模型精简模块，Network architecture search(NAS)模块</li>
<li>基于Transformer 的模型在视觉应用上起步晚可以应用于OCR, 3D和医疗图像分析</li>
</ol>
<h2 id="使用的方法-2"><a href="#使用的方法-2" class="headerlink" title="使用的方法"></a>使用的方法</h2><h3 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h3><p>整体框架分成4个阶段，每个阶段都产生不同尺度的特征图，每个阶段都具有相似的结构，包括：1、分块嵌入层，2、若干Transformer编码层</p>
<ul>
<li><p>结构（以第一阶段为例）</p>
<ol>
<li><p>第一阶段输入图像HxWx3, 并分割成$\frac{HW}{4^2}$多个图块，每块大小4x4x3</p>
</li>
<li><p>将分割块展平进行线性投影变换得到嵌入块形状为$\frac{HW}{4^2}\times C_1$</p>
</li>
<li><p>将嵌入块和位置嵌入一起传入具有$L_1$层的Transformer编码器中</p>
</li>
<li><p>输出特征层$F_1$形状是$\frac{H}{4}\times\frac{W}{4}\times C_1$</p>
</li>
<li><p>以此类推，以上一阶段输出作为下一阶段的输入，选取的块的大小相对于原始图像分别是8x8，16x16，32x32像素，即第i阶段的块大小为$P_i$，得到的特征图为$F_i$:$\{F_2$,$F_3$,$F_4\}$,特征图尺寸为$\frac{H}{8}\times\frac{W}{8}\times C_2$,$\frac{H}{16}\times\frac{W}{16}\times C_3$,$\frac{H}{32}\times\frac{W}{32}\times C_4$,</p>
<p><img src="./image-20221127151329769.png" alt="image-20221127151329769"></p>
</li>
</ol>
</li>
<li><p>transformer的特征金字塔</p>
<ul>
<li>CNN的特征金字塔是使用不同的卷积跨步来实现，PVT是使用逐步缩小策略实现</li>
</ul>
</li>
<li><p>transformer编码器</p>
<ol>
<li>Transformer编码器在第i阶段有$L_i$个编码层，每一个编码层又包含：1、注意力层，2、feed-forward层</li>
<li><p>使用spatial-reduction注意力层(SRA)替换传统多头注意力层(MHA)，为了处理高分辨率特征图(4跨步特征图)<br> <img src="./image-20221127153059574.png" alt="image-20221127153059574"><br> SRA特点：减小Key和Value输入的尺寸：</p>
<script type="math/tex; mode=display">\begin{align*}
         SRA(Q,K,V)&=Concat(head_0,\cdots,head_{N_i})W^O\\
         head_j&=Attention(QW_j^Q,SR(K)W_j^K,SR(V)W_j^V)\\
         SR(x)&=Norm(Reshape(x,R_i)W^S)\\
         Attention(\vec{q},\vec{k},\vec{v})&=Softmax(\frac{\vec{q}\vec{k}^T}{\sqrt{\vec{d}_{head}}})\vec{v}\\
 \end{align*}</script><p> 符号说明：$Concat(\cdot)$链接操作；$W_j^Q\in\mathbb{R}^{C_i\times d_{head}}$,$W_j^K\in\mathbb{R}^{C_i\times d_{head}}$,$W_j^V\in\mathbb{R}^{C_i\times d_{head}}$,$W^O\in\mathbb{R}^{C_i\times C_i}$,$W^S\in\mathbb{R}^{(R_i^2C_i)\times C_i}$都是线性投影矩阵；$N_i$注意力的头数；$d_{head}=\frac{C_i}{N_i}$每个头的大小；$SR(\cdot)$减小空间尺度操作；$x\in\mathbb{R}^{(H_iW_i)\times C_i}$表示输入序列；$R_i$表示缩减比例；$Reshape(x,R_i)=\frac{H_iW_i}{R_i^2}\times(R_i^2C_i)$修改张量形状;$Norm(\cdot)$层归一化；$Attention(\cdot,\cdot,\cdot)$注意力得分</p>
</li>
</ol>
</li>
</ul>
<h1 id="Deformable-DETR-Deformable-Transformers-For-End-to-End-Object-Detection"><a href="#Deformable-DETR-Deformable-Transformers-For-End-to-End-Object-Detection" class="headerlink" title="Deformable DETR: Deformable Transformers For End-to-End Object Detection"></a>Deformable DETR: Deformable Transformers For End-to-End Object Detection</h1><p>作者：Xinzhou Zhu,2020</p>
<h2 id="解决的问题-3"><a href="#解决的问题-3" class="headerlink" title="解决的问题"></a>解决的问题</h2><ol>
<li>改善空间分辨率问题和收敛慢问题</li>
<li>Deformable DETR在检测小目标上优于DETR</li>
<li>结合了deformable convolution的稀疏空间采样和transformer的相关性建模能力</li>
</ol>
<p>前人的工作以及优缺点：</p>
<ol>
<li>DETR(Nicolas Carion,2020)用于剔除目标检测中辅助成分的需求（例如，非极大值抑制），缺点是收敛慢、有限的特征空间分辨率，该模型结合了CNN模型和Transformer编解码模型，小目标效果差，优点就是具有元素间的相关性</li>
<li>目标检测使用了一些辅助成分(Li Liu,2020),例如锚点生成，基于规则的训练目标分配，非极大抑制</li>
<li>deformable convolution(Jifeng Dai,2017)用于稀疏空间定位，因此高效、收敛快，缺点就是缺少元素间的相关性</li>
<li>(Ashish Vaswani,2017)在transformer中引入自注意力和交叉注意力机制,缺点是时间成本和内存占用高</li>
<li>解决时间和内存的方式有三种<ol>
<li>对关键点使用预定义(冻结参数)的稀疏注意力模式<ul>
<li>将注意力模式限制在固定局部窗口中使用(Peter J Liu,2018a;Niki Parmar,2018;Rewon Child,2019;Zilong Huang;2019),优点是减少复杂度，缺点是丢失全局信息</li>
<li>以固定间隔方式设置关键点(Jonathan Ho,2019)，优点：增加接受视野</li>
<li>允许少许特殊符号可以访问所有关键点(Iz Beltagy,2020),增加全局信息</li>
</ul>
</li>
<li>学习依赖数据的稀疏注意力<ul>
<li>基于注意力机制的局部敏感哈希映射LSH(Nikita Kitaev,2020),将查询元素和关键字元素映射到不同的区域</li>
<li>(Aurko Roy,2020)用k-means聚类找到最相关的关键字元素</li>
<li>(Yi Tay,2020a)对逐块的稀疏注意力学习块交换</li>
</ul>
</li>
<li>研究自注意力机制的低级别性质<ul>
<li>通过线性投影减少关键点数量(Sinong Wang,2020)</li>
<li>通过核函数近似计算自注意力(Krzysztof Choromanski,2020)</li>
</ul>
</li>
</ol>
</li>
<li>多尺度特征表示<ul>
<li>FPN(Tsung-Yi Lin,2017a),自上而下生成多尺度特征图</li>
<li>PANet(Shu Liu,2018b)，自下而上生成多尺度特征图</li>
<li>从全局自注意力中提取所有尺度上的特征(Tao Kong,2018)</li>
<li>U-shape模块融合多尺度特征(Qijie Zhao,2019)</li>
<li>NAS-FPN(Golnaz Ghiasi,2019)、Auto-FPN(Hang Xu,2019)通过神经架构搜索自动进行交叉尺度连接</li>
<li>BiFPN(Mingxing Tan,2020)</li>
</ul>
</li>
</ol>
<h2 id="当前的不足-3"><a href="#当前的不足-3" class="headerlink" title="当前的不足"></a>当前的不足</h2><ol>
<li>简单且高效的可迭代的边界框调优机制进一步改善性能</li>
<li>可将Deformable DETR应用到2阶段目标识别中，先生成推荐区域，再将推荐区域作为目标query送入解码器</li>
</ol>
<h2 id="使用的方法-3"><a href="#使用的方法-3" class="headerlink" title="使用的方法"></a>使用的方法</h2><ol>
<li>应用在若干的采样位置点处，这些点作为特征图中关键特征点</li>
<li>使用图像尺度放缩而不是特征金字塔应用于deformable注意力模型</li>
</ol>
<h3 id="经典多头注意力结构"><a href="#经典多头注意力结构" class="headerlink" title="经典多头注意力结构"></a>经典多头注意力结构</h3><p>query元素代表了要输出的句子的目标单词，key元素代表输入句子中的单词，多头注意力模块根据测量的query-key对的相似性权重因子汇聚这些key。用$q\in\Omega_q$索引具有表达特征$z_q\in\mathbb{R}^C的$query元素；用$k\in\Omega_k$索引具有表达特征$x_k\in\mathbb{R}^C的$key元素,$C$是特征维度，$\Omega_q$和$\Omega_k$给出了query和key的元素总数;多头注意力特征计算：</p>
<script type="math/tex; mode=display">MultiHeadAttn(z_q,x)=\sum\limits_{m=1}^MW_m[\sum\limits_{k\in\Omega_k}A_{mqk}\cdot W_m^\prime x_k]</script><p>这里$m$索引各个注意力头总共有M个注意力头，$W_m^\prime\in\mathbb{C_v\times C}$和$W_m\in\mathbb{C\times C_v}$是第m头待学习的矩阵，$C_v=C/M$,注意力权重$A_{mqk}\propto \exp\{\frac{z_q^TU_m^TV_mx_k}{\sqrt{C_v}}\}$满足归一化$\sum\limits_{k\in\Omega_k}A_{mqk}=1$, 这里$U_m\in\mathbb{R}^{C_v\times C}$和$V_m\in\mathbb{R}^{C_v\times C}$同样是待学习的矩阵。为了消除不同空间位置的奇异性，表达的查询和关键字特征$z_q$和$x_k$需要和位置嵌入体做结合。</p>
<h3 id="DETR-Transformer编解码架构"><a href="#DETR-Transformer编解码架构" class="headerlink" title="DETR Transformer编解码架构"></a>DETR Transformer编解码架构</h3><p>采用Hungarian(匈牙利)损失函数借助二分(双边)匹配实现对每一个真实边框都有唯一预测值。用$y$表示ground truth集合，$\hat{y}=\{\hat{y}_i\}_{i=1}^N$表示有N个预测值的集合，当$N$远大于图像中物体个数时，可以认为$y$集合是由N个被符号$\phi$填充的集合。最低代价搜索N个元素$\sigma\in\mathfrak{G}_N$的一个置换$\hat{\sigma}=\mathop{\arg\min}\limits_{\sigma\in\mathfrak{G}_N}\sum\limits_i^N\mathcal{L}_{match}(y_i,\hat{y}_{\sigma(i)})$, （1）$\mathcal{L}_{match}(y_i,\hat{y}_{\sigma(i)})$是真值$y_i$和具有索引$\sigma(i)$的预测值之间的逐对匹配代价函数；（2）第i个真值元素可以看成$y_i=(c_i,b_i)$,$c_i$是目标类别标签，符号$\phi$用N/A表示，$b_i=\{b_x,b_y,b_w,b_h\}\in[0,1]^4$是向量记录了中心坐标和相对图像大小的高度和宽度；（3）对索引$\sigma(i)$的预测值，定义类$c_i$的概率为$\hat{p}_{\sigma(i)}(c_i)$,预测边框为$\hat{b}_{\sigma(i)}$，定义：</p>
<script type="math/tex; mode=display">\mathcal{L}_{match}(y_i,\hat{y}_{\sigma(i)})=-1_{\{c_i\neq\phi\}}\hat{p}_{\sigma(i)}(c_i)+1_{\{c_i\neq\phi\}}\mathcal{L}_{box}(b_i,\hat{b}_{\sigma(i)})</script><script type="math/tex; mode=display">\mathcal{L}_{box}(b_i,\hat{b}_{\sigma(i)})=\lambda_{iou}\mathcal{L}_{iou}(b_i,\hat{b}_{\sigma(i)})+\lambda_{L1}||b_i-\hat{b}_{\sigma(i)}||_1</script><p>这是真实框集合中的每一框和预测框集合中的每一个框匹配，损失值最小的预测框为该真实框的最佳匹配框，得到唯一匹配。匈牙利损失函数定义如下：</p>
<script type="math/tex; mode=display">\mathcal{L}_{Hungarian}(y,\hat{y})=\sum\limits_{i=1}^N\left[-1_{\{c_i\neq\phi\}}\hat{p}_{\hat{\sigma}(i)}(c_i)+1_{\{c_i\neq\phi\}}\mathcal{L}_{box}(b_i,\hat{b}_{\hat{\sigma}(i)})\right]</script><p>DETR主要使用Transformer编码解码架构将特征图像映射成一组待查目标对象的特征，feed-forward网络FFN用做回归分支预测边框坐标，线性投影用作分类分支预测目标类别。编码部分输入ResNet的生成的特征图，取query和key元素，它们都是特征图的每一个像素；解码部分输入包含来自编码部分的特征图和N个目标查询，有两个注意力模型：1、cross-attention,2、self-attention。在cross-attention中查询元素是从特征图中提取特征的N个目标查询，关键字元素来自于编码输出的特征图；在self-attention中，查询元素要获取他们的关系因此查询元素和关键字元素都来自于N个目标查询</p>
<h3 id="Deformable-transformer"><a href="#Deformable-transformer" class="headerlink" title="Deformable transformer"></a>Deformable transformer</h3><ol>
<li><p>单尺度Deformable attention<br> 通过对每个查询元素只分配少许固定数量的关键字元素，这些关键字元素取自一个参考点附近的点，无需对所有关键字进行匹配，所以无视特征图尺度，已知特征图$x\in\mathbb{R}^{C\times H\times W}$,$q$是查询元素特征$z_q$索引以及分配的一个2维参考点$p_q$索引</p>
<script type="math/tex; mode=display">DeformAttn(z_q,p_q,x)=\sum\limits_{m=1}^MW_m\left[\sum\limits_{k=1}^KA_{mqk}\cdot W_m^\prime x(p_q+\Delta p_{mqk})\right]</script><p> 这里m是注意力头的索引，k是采样到的关键字元素索引，K是采样关键字总数，显然$K\ll HW$。$\Delta p_{mqk}$和$A_{mqk}$表示对第m注意力头、第k个关键字元素采样点的采样邻域半径和注意力权重，注：标量注意力权重$A_{mqk}\in [0,1]$要归一化处理, $\Delta p_{mqk}\in \mathbb{R}^2$没有约束范围，当$p_q+\Delta p_{mqk}$是小数时双线性插值可以使用，$\Delta p_{mqk}$和$A_{mqk}$是通过对查询元素特征$z_q$线性投影计算得到</p>
</li>
<li><p>多尺度Deformable attention<br> 设$\{x^l\}_{l=1}^L$是用于输入的多尺度特征图，$x^l\in\mathbb{R}^{C\times H_l\times W_l}$, $\hat{p}_q\in[0,1]^2$是每个查询元素q对应参考点的归一化二维坐标，坐标的归一化操作是对每一个尺度特征图进行, 模型公式为</p>
<script type="math/tex; mode=display">MSDeformAttn(z_q,\hat{p}_q,\{x^l\}_{l=1}^L)=\sum\limits_{m=1}^MW_m\left[\sum\limits_{l=1}^L\sum\limits_{k=1}^KA_{mlqk}\cdot W_m^\prime x^l(\phi_l(\hat{p}_q)+\Delta p_{mlqk})\right]</script><p> m是注意力头索引，l是输入特征图的尺度等级索引，k是采样点的索引，$\Delta p_{mlqk}$和$A_{mlqk}$表示在l层尺度上的特征图、对第m注意力头、第k个关键字元素采样点的采样邻域半径和注意力权重，$\phi_l(\hat{p}_q)$将归一化的坐标缩放回尺度为l的特征图中的坐标</p>
</li>
<li><p>编码器<br> 编码器的输入输出都是具有相同分辨率的多尺度特征图，编码器的多尺度特征图$\{x^l\}_{l=1}^{L-1},L=4$取自ResNet的输出特征图$C_3$到$C5$分辨率分别为$H/2^3$到$H/2^5$,而最低分辨率的特征图$x^4$取自$C_5$特征图进行3x3步长2的卷积后得到的特征图$C_6$,所有的输入特征图的通道数都是256：<br> <img src="./image-20221130151059959.png" alt="image-20221130151059959"><br> query和key元素都是多尺度特征图的像素，每个query像素的参考点就是其自身，为了识别每个query像素在哪个尺度图上，除了添加位置嵌入体外，需要添加尺度嵌入体$e_l$到特征表达中，区别是位置嵌入体是固定编码，尺度嵌入体$\{e_l\}_{l=1}^L$需要连同网络一起训练获取。</p>
</li>
<li><p>解码器<br> 包含cross-attention和self-attention,query元素在两类注意力机制中都是目标query，目标query在cross-attention中取自特征图，而key元素是编码器的输出特征图；在self-attention中，key元素是目标query。每个目标query参考点的二维归一化坐标需要经过线性投影和激活函数从目标query的嵌入体中给出<br> <img src="./image-20221130160631440.png" alt="image-20221130160631440"><br> 模型提取的图像特征都是在参考点周围的点，所以我们预测的边框都是相对于参考点的偏移量，参考点初始值都是边框的中心点</p>
</li>
</ol>
<h1 id="End-to-End-Object-Detection-with-Transformers"><a href="#End-to-End-Object-Detection-with-Transformers" class="headerlink" title="End-to-End Object Detection with Transformers"></a>End-to-End Object Detection with Transformers</h1><p>作者：Nicolas Carion,2020</p>
<h2 id="解决的问题-4"><a href="#解决的问题-4" class="headerlink" title="解决的问题"></a>解决的问题</h2><ol>
<li>提出目标检测新方法DEtection TRansformer(DETR)：将目标检测看作单向集合预测问题</li>
<li>不再使用辅助设计成分，例如非极大值抑制抑制，锚点</li>
</ol>
<p>前人的工作及其优缺点：</p>
<h2 id="当前的问题"><a href="#当前的问题" class="headerlink" title="当前的问题"></a>当前的问题</h2><h2 id="使用的方法-4"><a href="#使用的方法-4" class="headerlink" title="使用的方法"></a>使用的方法</h2><p>基于一组全局损失，通过二分匹配给出唯一预测，使用transformer编解码架构，需要一组固定数量的学习好的目标query, DETR会推理目标和全局图像的关系然后输出最终的预测集合</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://chauchy-Liu.github.io">刘传玺</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://chauchy-liu.github.io/2022/11/22/%E7%9B%AE%E6%A0%87%E8%AF%86%E5%88%AB/">http://chauchy-liu.github.io/2022/11/22/%E7%9B%AE%E6%A0%87%E8%AF%86%E5%88%AB/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://chauchy-Liu.github.io" target="_blank">和其光, 同其尘</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%9B%AE%E6%A0%87%E8%AF%86%E5%88%AB/">目标识别</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/cover5.JPG" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2022/11/15/cmake/"><img class="next-cover" src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/cover5.JPG" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">cmake</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div id="comment-switch"><span class="first-comment">Valine</span><span class="switch-btn"></span><span class="second-comment">Disqus</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="disqus_thread"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">刘传玺</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">11</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">10</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">11</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/chauchy-liu/"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="mailto:liuchuanxi_211@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">学习笔记</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%A7%86%E8%A7%89%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text">基于深度学习的视觉目标检测技术综述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B"><span class="toc-number">1.1.</span> <span class="toc-text">发展历程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A3%80%E6%B5%8B%E8%AE%BE%E5%A4%87"><span class="toc-number">1.2.</span> <span class="toc-text">检测设备</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E5%8D%95%E7%9B%AE%E7%9B%B8%E6%9C%BA%E6%B5%81%E7%A8%8B%E5%8F%8A%E5%85%B6%E6%B6%B5%E7%9B%96%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">1.3.</span> <span class="toc-text">基于单目相机流程及其涵盖的方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E9%94%9A%E7%82%B9%E6%A1%86%E6%96%B9%E6%B3%95"><span class="toc-number">1.3.1.</span> <span class="toc-text">基于锚点框方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%97%A0%E9%94%9A%E7%82%B9%E6%A1%86%E6%96%B9%E6%B3%95"><span class="toc-number">1.3.2.</span> <span class="toc-text">基于无锚点框方法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E5%8F%8C%E7%9B%AE%E7%9B%B8%E6%9C%BA%E6%B5%81%E7%A8%8B%E5%8F%8A%E5%85%B6%E6%B6%B5%E7%9B%96%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">1.4.</span> <span class="toc-text">基于双目相机流程及其涵盖的方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E7%9B%B4%E6%8E%A5%E8%A7%86%E9%94%A5%E7%A9%BA%E9%97%B4"><span class="toc-number">1.4.1.</span> <span class="toc-text">基于直接视锥空间</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%98%BE%E5%BC%8F%E9%80%86%E6%8A%95%E5%BD%B1%E7%A9%BA%E9%97%B4"><span class="toc-number">1.4.2.</span> <span class="toc-text">基于显式逆投影空间</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%91%E5%B1%95%E8%B6%8B%E5%8A%BF"><span class="toc-number">1.5.</span> <span class="toc-text">发展趋势</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#YOLOv7-Trainable-bag-of-freebies-sets-new-state-of-the-art-for-real-time-object-detectors"><span class="toc-number">2.</span> <span class="toc-text">YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">2.1.</span> <span class="toc-text">解决的问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BD%93%E5%89%8D%E7%9A%84%E4%B8%8D%E8%B6%B3"><span class="toc-number">2.2.</span> <span class="toc-text">当前的不足</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">2.3.</span> <span class="toc-text">使用的方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%86%8D%E5%8F%82%E6%95%B0%E5%8C%96"><span class="toc-number">2.3.1.</span> <span class="toc-text">模型再参数化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%BC%A9%E6%94%BE"><span class="toc-number">2.3.2.</span> <span class="toc-text">模型缩放</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%B6%E6%9E%84"><span class="toc-number">2.3.3.</span> <span class="toc-text">架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%AF%E8%AE%AD%E7%BB%83%E8%B5%A0%E5%93%81%E8%A2%8Btrainable-bag-of-freebies"><span class="toc-number">2.3.4.</span> <span class="toc-text">可训练赠品袋trainable bag-of-freebies</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#AN-IMAGE-IS-WORTH-16x16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE"><span class="toc-number">3.</span> <span class="toc-text">AN IMAGE IS WORTH 16x16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98-1"><span class="toc-number">3.1.</span> <span class="toc-text">解决的问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BD%93%E5%89%8D%E7%9A%84%E4%B8%8D%E8%B6%B3-1"><span class="toc-number">3.2.</span> <span class="toc-text">当前的不足</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E7%9A%84%E6%96%B9%E6%B3%95-1"><span class="toc-number">3.3.</span> <span class="toc-text">使用的方法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Pyramid-Vision-Transformer-A-Versatile-Backbone-for-Dense-Prediction-without-Convolutions"><span class="toc-number">4.</span> <span class="toc-text">Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98-2"><span class="toc-number">4.1.</span> <span class="toc-text">解决的问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BD%93%E5%89%8D%E7%9A%84%E4%B8%8D%E8%B6%B3-2"><span class="toc-number">4.2.</span> <span class="toc-text">当前的不足</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E7%9A%84%E6%96%B9%E6%B3%95-2"><span class="toc-number">4.3.</span> <span class="toc-text">使用的方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A1%86%E6%9E%B6"><span class="toc-number">4.3.1.</span> <span class="toc-text">框架</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Deformable-DETR-Deformable-Transformers-For-End-to-End-Object-Detection"><span class="toc-number">5.</span> <span class="toc-text">Deformable DETR: Deformable Transformers For End-to-End Object Detection</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98-3"><span class="toc-number">5.1.</span> <span class="toc-text">解决的问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BD%93%E5%89%8D%E7%9A%84%E4%B8%8D%E8%B6%B3-3"><span class="toc-number">5.2.</span> <span class="toc-text">当前的不足</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E7%9A%84%E6%96%B9%E6%B3%95-3"><span class="toc-number">5.3.</span> <span class="toc-text">使用的方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%8F%E5%85%B8%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%BB%93%E6%9E%84"><span class="toc-number">5.3.1.</span> <span class="toc-text">经典多头注意力结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DETR-Transformer%E7%BC%96%E8%A7%A3%E7%A0%81%E6%9E%B6%E6%9E%84"><span class="toc-number">5.3.2.</span> <span class="toc-text">DETR Transformer编解码架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Deformable-transformer"><span class="toc-number">5.3.3.</span> <span class="toc-text">Deformable transformer</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#End-to-End-Object-Detection-with-Transformers"><span class="toc-number">6.</span> <span class="toc-text">End-to-End Object Detection with Transformers</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98-4"><span class="toc-number">6.1.</span> <span class="toc-text">解决的问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BD%93%E5%89%8D%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">6.2.</span> <span class="toc-text">当前的问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E7%9A%84%E6%96%B9%E6%B3%95-4"><span class="toc-number">6.3.</span> <span class="toc-text">使用的方法</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/11/22/%E7%9B%AE%E6%A0%87%E8%AF%86%E5%88%AB/" title="目标识别"><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/cover5.JPG" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="目标识别"/></a><div class="content"><a class="title" href="/2022/11/22/%E7%9B%AE%E6%A0%87%E8%AF%86%E5%88%AB/" title="目标识别">目标识别</a><time datetime="2022-11-22T01:13:23.000Z" title="发表于 2022-11-22 09:13:23">2022-11-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/11/15/cmake/" title="cmake"><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/cover5.JPG" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="cmake"/></a><div class="content"><a class="title" href="/2022/11/15/cmake/" title="cmake">cmake</a><time datetime="2022-11-15T05:43:56.000Z" title="发表于 2022-11-15 13:43:56">2022-11-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/11/11/g2o/" title="g2o"><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/cover4.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="g2o"/></a><div class="content"><a class="title" href="/2022/11/11/g2o/" title="g2o">g2o</a><time datetime="2022-11-11T03:05:21.000Z" title="发表于 2022-11-11 11:05:21">2022-11-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/11/04/%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/" title="数学知识"><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/cover3.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数学知识"/></a><div class="content"><a class="title" href="/2022/11/04/%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/" title="数学知识">数学知识</a><time datetime="2022-11-04T02:35:57.000Z" title="发表于 2022-11-04 10:35:57">2022-11-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/11/04/slam/" title="slam"><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/cover8.JPG" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="slam"/></a><div class="content"><a class="title" href="/2022/11/04/slam/" title="slam">slam</a><time datetime="2022-11-04T01:21:41.000Z" title="发表于 2022-11-04 09:21:41">2022-11-04</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/cover5.JPG')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By 刘传玺</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '',
      appKey: '',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script><script>function loadDisqus () {
  var disqus_config = function () {
    this.page.url = 'http://chauchy-liu.github.io/2022/11/22/%E7%9B%AE%E6%A0%87%E8%AF%86%E5%88%AB/'
    this.page.identifier = '/2022/11/22/%E7%9B%AE%E6%A0%87%E8%AF%86%E5%88%AB/'
    this.page.title = '目标识别'
  };

  window.disqusReset = () => {
    DISQUS.reset({
      reload: true,
      config: disqus_config
    })
  }

  if (window.DISQUS) disqusReset()
  else {
    (function() { 
      var d = document, s = d.createElement('script');
      s.src = 'https://.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  }

  document.getElementById('darkmode').addEventListener('click', () => {
    setTimeout(() => window.disqusReset(), 200)
  })
}

if ('Valine' === 'Disqus' || !true) {
  if (true) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
  else loadDisqus()
} else {
  function loadOtherComment () {
    loadDisqus()
  }
}
</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginModelPath":"assets/","model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"left","width":100,"height":200,"vOffset":-80,"hOffset":0},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>